[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Zion’s Blog",
    "section": "",
    "text": "I put this together during Winter Quarter of 2024 for PIC 16B. Here I’m going to be placing all of my homework assignments, but I might also keep this for later use."
  },
  {
    "objectID": "posts/16B Final Project/index.html",
    "href": "posts/16B Final Project/index.html",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "",
    "text": "https://torwar02.github.io/pic16b/posts/16B%20Final%20Project/\n\n\n\nhttps://github.com/torwar02/trails"
  },
  {
    "objectID": "posts/16B Final Project/index.html#blog-post",
    "href": "posts/16B Final Project/index.html#blog-post",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "",
    "text": "https://torwar02.github.io/pic16b/posts/16B%20Final%20Project/"
  },
  {
    "objectID": "posts/16B Final Project/index.html#github-repository",
    "href": "posts/16B Final Project/index.html#github-repository",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "",
    "text": "https://github.com/torwar02/trails"
  },
  {
    "objectID": "posts/16B Final Project/index.html#brief-summary",
    "href": "posts/16B Final Project/index.html#brief-summary",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Brief Summary",
    "text": "Brief Summary\nThere are websites were you can get nitty-gritty information about trails. There are websites where you can read about people’s experiences at parks. Why not combine the two to see where you want to go on your next adventure? That’s why the pipeline for this project looks something like this:\n\n\n\nImage1\n\n\nTrailForks contains information about both parks and individual trails. Most of the information is numerical: how long a trail is, how many trails a park has, the difficulty, things like that. Because there’s a lot of infromation here, we’ve stored it in a SQL database. TripAdvisor contains reviews and photos, and scraped information will be directly put into the recommendation system.\nFirst, I have taken part of Tyler’s writeup here to show how he took data from TripAdvisor and then used it to create a recommendation system. Then, we will look at the TrailForks scraping and SQL datbabase aspect before looking at how we can combine the information from TrailForks with the recommendation. Finally, we will see how the information is stored on the website, as well as a look at an image matting tool that’s also on it.\n\nimport matplotlib.pyplot as plt\nimport pandas as pd"
  },
  {
    "objectID": "posts/16B Final Project/index.html#load-data",
    "href": "posts/16B Final Project/index.html#load-data",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Load Data",
    "text": "Load Data\nNow let us load the data we scraped by TripAdvisor as well as a Excel file containing coordinate points of our national parks so that we can create a geograpical plot later\n\ndf = pd.read_csv('https://raw.githubusercontent.com/torwar02/trails/main/trails/national_parks.csv')\n\n\ndf2 = pd.read_excel('https://raw.githubusercontent.com/torwar02/trails/main/trails/coords.xlsx')\n\n\ndf.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nLatitude\nLongitude\nPark\nState(s)\nPark Established\nArea\nVisitors (2018)\n\n\n\n\n0\n44.35\n-68.21\nAcadia\nMaine\nFebruary 26, 1919\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\n-14.25\n-170.68\nAmerican Samoa\nAmerican Samoa\nOctober 31, 1988\n8,256.67 acres (33.4 km2)\n28626\n\n\n2\n38.68\n-109.57\nArches\nUtah\nNovember 12, 1971\n76,678.98 acres (310.3 km2)\n1663557\n\n\n3\n43.75\n-102.50\nBadlands\nSouth Dakota\nNovember 10, 1978\n242,755.94 acres (982.4 km2)\n1008942\n\n\n4\n29.25\n-103.25\nBig Bend\nTexas\nJune 12, 1944\n801,163.21 acres (3,242.2 km2)\n440091\n\n\n\n\n\n\n\nTo merge the two files together, we utilize regex. Get string preceding ‘National Park’ in df such that we can merge with df2 on National Park name\n\nimport re\npattern = r'(.*?)(?:\\s+National Park)?$'\nresult = re.findall(pattern, df['national_park'].iloc[0])\npark = []\nfor row in df['national_park']:\n    test_park = re.findall(pattern, row)\n    park.append(test_park[0])\ndf['park'] = park\nnational_parks = pd.merge(df, df2, left_on='park', right_on='Park')\nnational_parks = national_parks.drop(columns = ['park', 'Park', 'State(s)', 'Park Established'])\nnational_parks.head()\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n0\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nTurned back on 3/20/21 due to ice\n4.0 of 5 bubbles\nI have hiked to the fire tower a few times. It...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n1\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nSpectacular\n5.0 of 5 bubbles\nThis trail was recommended in my Acadia travel...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n2\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat Trail\n5.0 of 5 bubbles\nBeech Mountain Trail is one of my favorites in...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n3\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nBest trail in Acadia\n5.0 of 5 bubbles\nWe stumbled onto this trail and were very happ...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575\n\n\n4\nAcadia National Park\nMaine (ME)\nBeech Mountain Trail\nHiking Trails\n4.5\nGreat trail for family\n5.0 of 5 bubbles\nMy family has kids ranging from age 10 to 3. W...\n44.35\n-68.21\n49,075.26 acres (198.6 km2)\n3537575"
  },
  {
    "objectID": "posts/16B Final Project/index.html#word-embedding-and-comment-similarity-score",
    "href": "posts/16B Final Project/index.html#word-embedding-and-comment-similarity-score",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Word Embedding and Comment Similarity Score",
    "text": "Word Embedding and Comment Similarity Score\nFirst let us go over what Word Embedding is. Word embedding in NLP is an important technique that is used for representing words for text analysis in the form of real-valued vectors. In this approach, words and documents are represented in the form of numeric vectors allowing similar words to have similar vector representations. The extracted features are fed into a machine learning model so as to work with text data and preserve the semantic and syntactic information. This information once received in its converted form is used by NLP algorithms that easily digest these learned representations and process textual information.\n\nComment Similarity Function\nNow let us create a function called comment_similarity which takes in our national_parks.csv file we just created via the park_data parameter, a comment_index parameter, and an all_comments parameter which is our word embedding vector representation of all comments in our csv file.\n\nall_docs = [nlp(row) for row in national_parks['comment_text']] #getting vector representation of all comments in our csv file\n\n\ndef comment_similarity(parks_data, comment_index, all_comments):\n    example_comment = parks_data.loc[comment_index, 'comment_text']\n    reference_comment = nlp(example_comment) #vectorize our reference sentence\n    simularity_score = []\n    row_id = []\n    for i in range(len(all_comments)):\n        sim_score = all_comments[i].similarity(reference_comment)\n        simularity_score.append(sim_score)\n        row_id.append(i)\n    simularity_docs = pd.DataFrame(list(zip(row_id, simularity_score)), columns = ['Comment_ID', 'sims'])\n    simularity_docs_sorted = simularity_docs.sort_values(by = 'sims', ascending = False)\n    most_similar_comments = simularity_docs_sorted['Comment_ID'][1:2]\n    new_reviews = national_parks.iloc[most_similar_comments.values]\n    return(new_reviews)\n\nNow let us show what our returned dataframe looks like\n\nshowcase = comment_similarity(national_parks, 0, all_docs)\nshowcase\n\n/var/folders/8d/g7_l6rp52r74q3dwsx2j4p000000gn/T/ipykernel_81743/385968650.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n  sim_score = all_comments[i].similarity(reference_comment)\n\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n2129\nMesa Verde National Park\nColorado (CO)\nMesa Verde National Park\nNational Parks\n4.5\nVery unusual and very enjoyable.\n5.0 of 5 bubbles\nThis review should be most helpful to visitors...\n37.18\n-108.49\n52,485.17 acres (212.4 km2)\n563420\n\n\n\n\n\n\n\nAs we can see, we return a dataframe of the most similar review to the review with the index 999. To see how similar this similar review is to our inputted review let us output both comments.\nFirst the original comment\n\nexample_comment = national_parks.loc[0, 'comment_text']\nexample_comment\n\n\"I have hiked to the fire tower a few times. Its a great hike, and not too strenuous elevation gains.  If the NO rangers are up there ( in the summer) they used to allow you to go up the tower. We had to turn back on 3/20 because of hard pack solid ice. We had our Katoohla micro spikes on, and solid hiking poles, and knew they simply  wouldn't be enough if the ice was on the steeper sections.  We walked into the trailhead because the access road gate is still closed. After deciding to cross the lot and hike Beech Cliff Loop, which was much more clear of ice, and has excellent views of Echo Lake and the ocean out toward  Southwest Harbor. We returned to BH to hear of the recovery of a young couple from Rutland Massachusetts  who had fallen 100 feet to their death on Dorr Mountain Gorge Trail. The tragedy attributed to ice on the trails. Anyone not experienced with full crampon travel, and ice climbing training should never attempt to hike or climb on solid ice. The danger is severe.. \"\n\n\nNow the similar comment.\n\nshowcase['comment_text'].iloc[0]\n\n\"This review should be most helpful to visitors who wish to do minimal hiking/walking, who enjoy spending most of the time driving, or who do not plan to pay for any of the popular tours of the ruins. We had a wonderful experience even without the tours. We arrived at the park the day they started shutting things down for COVID, so the tours were not available. The park was extremely enjoyable though, even without the tours. In fact we almost had the entire park to ourselves. The road up into the mesa had great views of the surrounding areas. Be sure to stop at some of the overlooks. My favorite was the Park Point Overlook...make sure to hike up to the fire lookout.  You can see everything from there. Since we were here in March, we only did the drives on Chapin Mesa. We stopped at the main parking lot and walked out to see the Spruce Tree House--very cool. We then very slowly and very leisurely drove around both the Mesa Top Loop and the Cliff Palace Loop, stopping at each and every pullout. We obtained a guide brochure at the first Pit House on the Mesa Loop which was very helpful along the way. I wasn't sure I was going to enjoy seeing a bunch of ruins, but it was marvelous and beautiful at every stop. My favorite stops on the Mesa Loop were the First Pit House, the Square Tower House, the Sun Point View, and the Sun Temple. At the Sun Point View, you can see about a dozen different ruins up and down a couple of canyons...amazing. The park has great signage at each stop indicating the names of each ruin as well as the time period they are from. My favorite stop on the Cliff Palace Loop was of course the Cliff Palace. Make sure to park and walk out to the overlook on the north side of the ruin. The Balcony House is also on the Cliff Palace Loop, but it is not visible from the road. It is apparently only visible if you hike out the Soda Canyon Overlook Trail (which we did not do), or take a guided tour (not available to us since the park was closing). By the time we finished these two loops, my experience seemed very complete and very satisfying. Finally, as we started our journey back out of the park, we stopped briefly at Cedar Tree Tower for one final experience with ruins. Very nice. I am so glad we finally made it to this national park (my 28th). This is one very amazing place! I'd love to come back some time and take the tours, but I definitely can say that taking in the park as we did was very fulfilling and felt very complete.  If you wish to replicate this trip,  it took us 4-5 hours.\"\n\n\nAs we can see the comments are very similar! They both talk about the dangers of the trail and how they both saw people fall.\n\n\nTotal Trail Simularity\nNow let us create a function called total_similarity which takes in the same parameters as our last function except takes in the trail name instead of comment_index. We do so because we want to get all 10 comments per trail. Our total_similarity function calls comment_similarity to get the most similar comment per each individual comment of the 10 trails. As a result, we get 10 total similar trails returned to us.\n\ndef total_similarity(trail, parks_data, all_comments):\n    trail_subset = parks_data[parks_data['trail'] == trail].index\n    total_df = []\n    for number in trail_subset:\n        total_df.append(comment_similarity(national_parks, number, all_docs))\n    df = pd.concat(total_df)\n    return(df)\n\n\noutput = total_similarity(\"Landscape Arch\", national_parks, all_docs)\noutput\n\n/var/folders/8d/g7_l6rp52r74q3dwsx2j4p000000gn/T/ipykernel_81743/385968650.py:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n  sim_score = all_comments[i].similarity(reference_comment)\n\n\n\n\n\n\n\n\n\nnational_park\nstate\ntrail\nactivity\noverall_rating\ncomment_title\ncomment_ratings\ncomment_text\nLatitude\nLongitude\nArea\nVisitors (2018)\n\n\n\n\n1492\nGrand Canyon National Park\nArizona (AZ)\nGrand Canyon Desert View Watchtower\nObservation Decks & Towers\n4.5\nGreat views and shop - be aware of COVID limit...\n5.0 of 5 bubbles\nThis area is either your first stop on the way...\n36.06\n-112.14\n1,201,647.03 acres (4,862.9 km2)\n6380495\n\n\n655\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nA challenging hike with Native American Art re...\n5.0 of 5 bubbles\nHorseshoe Canyon is difficult to access, possi...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n655\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nA challenging hike with Native American Art re...\n5.0 of 5 bubbles\nHorseshoe Canyon is difficult to access, possi...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n727\nCanyonlands National Park\nUtah (UT)\nGrand View Point Overlook\nLookouts\n5.0\nfreaking amazing!\n5.0 of 5 bubbles\nDon't miss this.\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n1369\nGlacier National Park\nMontana (MT)\nVirginia Falls\nWaterfalls\n5.0\nA Must See !!\n5.0 of 5 bubbles\nWe parked by St Mary’s Fall’s parking. It took...\n48.80\n-114.00\n1,013,125.99 acres (4,100.0 km2)\n2965309\n\n\n3027\nYosemite National Park\nCalifornia (CA)\nSentinel Dome\nGeologic Formations\n5.0\nTHE view at Yosemite!\n5.0 of 5 bubbles\nThis is the place to come for views if you onl...\n37.83\n-119.50\n761,747.50 acres (3,082.7 km2)\n4009436\n\n\n1063\nChannel Islands National Park\nCalifornia (CA)\nThe Sea Caves\nCaverns & Caves\n5.0\nA unique and awesome experience!\n5.0 of 5 bubbles\nMy wife and I camped 2 nights at Scorpion Harb...\n34.01\n-119.42\n249,561.00 acres (1,009.9 km2)\n366250\n\n\n3291\nZion National Park\nUtah (UT)\nKolob Canyons\nCanyons\n4.5\nChoose Kolob to avoid the Zion crowds\n5.0 of 5 bubbles\nThis is a great alternative to the very crowde...\n37.30\n-113.05\n147,237.02 acres (595.8 km2)\n4320033\n\n\n3291\nZion National Park\nUtah (UT)\nKolob Canyons\nCanyons\n4.5\nChoose Kolob to avoid the Zion crowds\n5.0 of 5 bubbles\nThis is a great alternative to the very crowde...\n37.30\n-113.05\n147,237.02 acres (595.8 km2)\n4320033\n\n\n650\nCanyonlands National Park\nUtah (UT)\nHorseshoe Canyon\nCanyons\n5.0\nWHOA! READ PLEASE. Things you NEED to know a...\n5.0 of 5 bubbles\nThere are some older reviews. Some are VERY M...\n38.20\n-109.93\n337,597.83 acres (1,366.2 km2)\n739449\n\n\n\n\n\n\n\nAs we can see we get 10 similar trails to our desired trail Landscape Arch\n\n\nPlotly Function\nNow let us construct a geographical plot function called plotting_parks to get the location of these trails on a map. This is so that the user can better visualize where in the United States they may have to travel to. The function also analyzes other metrics from national_parks.csv such as visitors in 2018, type of activity, trail name, and overall TripAdvisor rating. This function calls total_similarity in order to get the dataframe with the most similar reviews!\n\nfrom plotly import express as px\nimport plotly.io as pio\nimport inspect\npio.renderers.default=\"iframe\"\n\n\ndef plotting_parks(trail, parks_data, all_comments, **kwargs):\n    output = total_similarity(trail, parks_data, all_comments)\n    fig = px.scatter_mapbox(output, lon = \"Longitude\", lat = \"Latitude\", color = \"overall_rating\",\n                        color_continuous_midpoint = 2.5, hover_name = \"national_park\", height = 600,\n                        hover_data = [\"Visitors (2018)\", \"activity\", \"trail\", \"overall_rating\"],\n                        title = \"Recommended National Park Trails\",\n                        size_max=50,\n                        **kwargs,\n                        )\n    return fig\n\n\ncolor_map = px.colors.diverging.RdGy_r # produce a color map\nfig = plotting_parks(\"Landscape Arch\", national_parks, all_docs, mapbox_style=\"carto-positron\",\n                                   color_continuous_scale = color_map)\n\n/var/folders/8d/g7_l6rp52r74q3dwsx2j4p000000gn/T/ipykernel_81743/385968650.py:7: UserWarning:\n\n[W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n\n\n\n\nfig.show()\n\n\n\n\nGreat, as we can see, we get a geo plot of the most similar National Park trails in the United States to Landscape Arch!"
  },
  {
    "objectID": "posts/16B Final Project/index.html#how-do-you-get-started-with-selenium",
    "href": "posts/16B Final Project/index.html#how-do-you-get-started-with-selenium",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "How do you get started with Selenium?",
    "text": "How do you get started with Selenium?\nSelenium is able to evade certain anti-bot measures by actually using an instance of a web browser (called a webdriver) that runs on your system while scraping. In fact, once you get the scraper to work, you can actually watch it run in real time! Unfortunately, that makes it a lot slower than scrapy, for instance, because your computer actually has to manually open every page. I used a Google Chrome webdriver. Below is (part of) the head of the scraper.py function which scrapes data from individual trails from TrailForks.\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nimport pandas as pd\nfrom selenium import webdriver\n\n\nchrome_options = webdriver.ChromeOptions()\noptions = webdriver.ChromeOptions()\noptions.add_experimental_option(\n        \"prefs\", {\n            # block image loading\n            \"profile.managed_default_content_settings.images\": 2,\n            \"profile.managed_default_content_settings.javascript\": 2\n        }\n    )\ndriver = webdriver.Chrome(\n        service=service,\n        options=options\n    )\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--headless')\nOf note are the experimental options under prefs which block both images and javascript content from loading on a website. When I first made the scraper, I did not have these enabled, and a result, sometimes pages would take between 3 and 5 seconds to load (way too long!). Similarly, the --headless argument also make the pages load faster by disabling certain Google Chrome functionalities: https://www.selenium.dev/blog/2023/headless-is-going-away/"
  },
  {
    "objectID": "posts/16B Final Project/index.html#how-do-you-scrape-using-selenium",
    "href": "posts/16B Final Project/index.html#how-do-you-scrape-using-selenium",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "How do you scrape using Selenium?",
    "text": "How do you scrape using Selenium?\nAt its core, Selenium isn’t that different from scrapy in that you can have a scraper download HTML code which you can then filter through in Python as more familiar objects. Both scraper.py and scraper_parks.py (which filters through park-related information on TrailForks) have the same general principles: 1. Look at what state a user has inputted 2. Get links to all of the park/trail pages for that state 3. Get corresponding information from each page, using helper functions if need be 4. Add data to SQL database (see next section for that!)"
  },
  {
    "objectID": "posts/16B Final Project/index.html#set-up",
    "href": "posts/16B Final Project/index.html#set-up",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Set-up",
    "text": "Set-up\nThe settings for Selenium are the same as in the last case. The pre-scraping part is almost identical as well:\nstart_url = f\"https://www.trailforks.com/region/{state_name}/ridingareas/?activitytype=6\"\n    url_list = [start_url]\n\n    for page_num in range(database_info.state_dictionary[state_name]):\n        url_list.append(start_url + f\"&page={page_num+2}\")\n\n    for page_num in range(state_page_dict[state_name]):\n        href_list = []\n        driver.get(url_list[page_num])\n        green_links = driver.find_elements(\"xpath\",\"//tr//a[contains(@class, 'green')]\")\n\n        for green in green_links:\n            href = green.get_attribute(\"href\") #Grab URLs--otherwise this doesn't work\n            href_list.append(href+\"/?activitytype=6\")\nOnce again, I pre-scraped the number of pages required per state, though because there are fewer parks than trails, it wasn’t as big of a load on my computer."
  },
  {
    "objectID": "posts/16B Final Project/index.html#names-and-coordinates",
    "href": "posts/16B Final Project/index.html#names-and-coordinates",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Names and coordinates",
    "text": "Names and coordinates\nHere’s the first part of where we actually scrape:\n\n for url in href_list:\n            no_name_found = False\n            info_dict = {\"Name\":[\"NA\"], \"Location\":[\"NA\"], \"Coords\":[\"NA\"]}\n            stats_dict =  {\"Trails (view details)\":[\"NA\"],\"Total Distance\":[\"NA\"], \"State Ranking\":[\"NA\"],}\n            trail_difficulty_count = {\"Access Road/Trail\":0,\"White\":0,\"Green\":0,\"Blue\":0,\"Black\":0,\"Double Black Diamond\":0, \"Proline\":0}\n            print(url)\n            driver.get(url)\n            area_name_raw = driver.find_element(\"xpath\", \"//span[contains(@class, 'translate')][1]\")\n            info_dict[\"Name\"] = area_name_raw.text\n            try:\n                city_name_raw = driver.find_element(By.CLASS_NAME, \"small.grey2.mobile_hide\")\n                info_dict[\"Location\"] = city_name_raw.text\n            except:\n                no_name_found = True\n                \nWe create our three dictionaries that we want for each URL. The print(url) function is present as a debugging tool since, unfortunately, this scraper crashed multiple times due to unfixed bugs (which I eventually patched out, mostly due to elements not being present).\nWe get the name of the trail by finding a span with class translate (not sure why it’s stored like that, it’s actually within an h1 within a ul called page_title_container). Then, we try to look for the name of the city that it’s in by grabbing a small piece of text that’s next to the park’s name. Sometimes, this isn’t present, which is why we have a bool called no_name_found in case it’s not. There’s a way around this, though, which we’ll show later…\n\nRanking, Distance, and Trail numbers:\n    stats_items = [\"State Ranking\", \"Total Distance\", \"Trails (view details)\"]\n            dict_category = driver.find_elements(\"xpath\", \"//dl//dt\")\n            dict_information = driver.find_elements(\"xpath\", \"//dl//dd\")\n           \n            for idx, terms in enumerate(dict_category):\n                if terms.text in stats_items:\n                    stats_dict[terms.text] = [dict_information[idx].text]\n                    \n            try:\n                difficulty_ul = driver.find_element(By.CLASS_NAME, 'stats.flex.nostyle.inline.clearfix')\n\n                for li in difficulty_ul.find_elements(By.TAG_NAME, 'li'):\n                    difficulty_span = li.find_element(By.XPATH, './/span[contains(@class, \"stat-label clickable\")]/span')\n                    difficulty_name = difficulty_span.get_attribute('title')\n                    if difficulty_name in trail_difficulty_count.keys():\n                        num_trails_span = li.find_element(By.CLASS_NAME, 'stat-num')\n                        num_trails = int(num_trails_span.text)\n                        trail_difficulty_count[difficulty_name] = num_trails\nThe code here is somewhat dense thanks to the fact that all of this information is stored in a dictionary-like object called a dl which, in turn, has something like a key in a dl and something like a value in a dd. Essentially, we update the ranking and trail distances by inspecting these.\nIt’s a little bit harder to get the number of trails per difficulty. Basically, there’s an unordered list with a long class name ('stats.flex.nostyle.inline.clearfix' that sorts the number of trails by difficulty. Each li has the number of trails stored within it, but it also has a graphic that represents the difficulty (it’s a small picture), and it’s the graphic that actually hides the name of the difficulty, which is why we have to extract difficulty_name from a span of class stat-label clickable. Then, we simply grab the actual text that displays how many trails of a given difficulty there are, convert it to an integer, and then add it to our dictionary.\n\n\nCoordinates\nOne of the unfortunate parts of the parks list is that the coordinates of each park are not present! To get around this, we tell the scraper to go to the first trail in each park and grab its coordinates (remember scraper.py?) and then store it.\n  try:\n                green_link = driver.find_element(\"xpath\",\"//tr//a[contains(@class, 'green')]\")\n                park_link = green_link.get_attribute(\"href\")\n                driver.get(park_link)\n            except:\n                pass\n                \n            try:\n                coord_raw = driver.find_element(\"xpath\", \"//div[contains(@class, 'margin-bottom-15 grey')]/span[contains(@class, 'grey2')][2]\") #Get coords\n                info_dict['Coords'] = [coord_raw.text]\n                if no_name_found:\n                    city_name_raw = driver.find_element(By.CLASS_NAME, \"weather_date bold green\")\n                    info_dict[\"Location\"] = city_name_raw.text\n            except:\n                info_dict['Coords'] = [\"NA\"]\nIt’s here where we also resolve the issue of when we can’t find a city’s name. Basically, on each trail’s page, there’s a short infobox containing weather information for the nearest city which is guaranteed to appear, so we can get an approximate location name precisely by grabbing the city name from this box.\nHere’s another image of what the scraper in action looks like:\n\n\n\nImageScraperParks\n\n\nOnce we’re done with that, it’s off to the database again!"
  },
  {
    "objectID": "posts/16B Final Project/index.html#database_info.py",
    "href": "posts/16B Final Project/index.html#database_info.py",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "database_info.py",
    "text": "database_info.py\nEverything relevant to managing the datbase is stored in a different python file called database_info.py. Here I can show you the structure of both databases:\n\nMaking the databases\ndef make_db(state):\n    conn = sqlite3.connect(\"trails.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    name VARCHAR(255),\n    coords VARCHAR(255),\n    Distance VARCHAR(255),\n    'Avg time' VARCHAR(255),\n    Climb VARCHAR(255),\n    Descent VARCHAR(255),\n    Activities VARCHAR(255),\n    'Riding Area' VARCHAR(255),\n    'Difficulty Rating' VARCHAR(255),\n    'Dogs Allowed' VARCHAR(255),\n    'Local Popularity' VARCHAR(255),\n    'Altitude start' VARCHAR(255),\n    'Altitude end' VARCHAR(255),\n    Grade VARCHAR(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\n    \ndef make_db_parks(state):\n    conn = sqlite3.connect(\"trails_new.db\")\n    cmd = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {state_name_code_name_dict[state]}(\n    Name VARCHAR(255),\n    Location VARCHAR(255),\n    Coords VARCHAR(255),\n    'Trails (view details)' SMALLINT(255),\n    'Total Distance' VARCHAR(255),\n    'State Ranking' VARCHAR(255),\n    'Access Road/Trail' SMALLINT(255),\n    White SMALLINT(255),\n    Green SMALLINT(255),\n    Blue SMALLINT(255),\n    Black SMALLINT(255),\n    'Double Black Diamond' SMALLINT(255),\n    Proline SMALLINT(255)\n    );\n    \"\"\"\n    cursor = conn.cursor()\n    cursor.execute(cmd)\n    cursor.close()\n    conn.close()\nThese two functions were run in order to actually create the datbaase for the first time. They contain the variables as mentioned previously, mostly in the form of text.\n\n\nAdding information\nIf you recall from the scraping functions, there was a function call that would add information from each park to the SQL database. Here’s the source code for those functions:\ndef get_db():\n    conn = sqlite3.connect(\"trails.db\")\n    return conn\n    \ndef add_trails(df,state):\n    conn = get_db()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\n    \ndef get_db_new():\n    conn = sqlite3.connect(\"trails_new.db\")\n    return conn\n    \ndef add_trails_new(df,state):\n    conn = get_db_new()\n    df.to_sql(state, conn, if_exists = \"append\", index = False)\nThe functions get_db and get_db_new (most things relating to scraper_parks are labeled new since we did this second) establish connections to their respective databases. add_trails and add_trails_new, therefore, are actually responsible for adding entries to each database. Note that they take a df as one input (which contains the scraped info) and a state name, which sends the information to the correct table.\n\n\nMiscellaneous Tables\nThere are several dictionaries and lists that we generated in order to make the functions easier to run:\nstates = [\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"idaho-3166\", \"Illinois\", \"Indiana\", \"Iowa\", \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"new-hampshire\", \"new-jersey\", \"new-mexico\", \"new-york\", \"north-carolina\", \"north-dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"rhode-island\", \"south-carolina\", \"south-dakota\", \"Tennessee\", \"Texas\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"west-virginia\", \"Wisconsin\", \"Wyoming\"]\n\nstate_name_code_name_dict = {\n    'Alabama': 'Alabama',\n    'Alaska': 'Alaska',\n    'Arizona': 'Arizona',\n    'Arkansas': 'Arkansas',\n    'California': 'California',\n    'Colorado': 'Colorado',\n    'Connecticut': 'Connecticut',\n    'Delaware': 'Delaware',\n    'Florida': 'Florida',\n    'Georgia': 'Georgia',\n    'Hawaii': 'Hawaii',\n    'idaho-3166': 'Idaho',\n    'Illinois': 'Illinois',\n    'Indiana': 'Indiana',\n    'Iowa': 'Iowa',\n    'Kansas': 'Kansas',\n    'Kentucky': 'Kentucky',\n    'Louisiana': 'Louisiana',\n    'Maine': 'Maine',\n    'Maryland': 'Maryland',\n    'Massachusetts': 'Massachusetts',\n    'Michigan': 'Michigan',\n    'Minnesota': 'Minnesota',\n    'Mississippi': 'Mississippi',\n    'Missouri': 'Missouri',\n    'Montana': 'Montana',\n    'Nebraska': 'Nebraska',\n    'Nevada': 'Nevada',\n    'new-hampshire': 'NewHampshire',\n    'new-jersey': 'NewJersey',\n    'new-mexico': 'NewMexico',\n    'new-york': 'NewYork',\n    'north-carolina': 'NorthCarolina',\n    'north-dakota': 'NorthDakota',\n    'Ohio': 'Ohio',\n    'Oklahoma': 'Oklahoma',\n    'Oregon': 'Oregon',\n    'Pennsylvania': 'Pennsylvania',\n    'rhode-island': 'RhodeIsland',\n    'south-carolina': 'SouthCarolina',\n    'south-dakota': 'SouthDakota',\n    'Tennessee': 'Tennessee',\n    'Texas': 'Texas',\n    'Utah': 'Utah',\n    'Vermont': 'Vermont',\n    'Virginia': 'Virginia',\n    'Washington': 'Washington',\n    'west-virginia': 'WestVirginia',\n    'Wisconsin': 'Wisconsin',\n    'Wyoming': 'Wyoming'\n}\n\n\nstate_dictionary = {'Alabama': 11, 'Alaska': 11, 'Arizona': 49, 'Arkansas': 16, 'California': 152, 'Colorado': 69, 'Connecticut': 56, 'Delaware': 4, 'Florida': 18, 'Georgia': 17, 'Hawaii': 5, 'idaho-3166': 31, 'Illinois': 51, 'Indiana': 10, 'Iowa': 8, 'Kansas': 3, 'Kentucky': 9, 'Louisiana': 2, 'Maine': 27, 'Maryland': 16, 'Massachusetts': 146, 'Michigan': 55, 'Minnesota': 36, 'Mississippi': 3, 'Missouri': 11, 'Montana': 41, 'Nebraska': 3, 'Nevada': 16, 'new-hampshire': 41, 'new-jersey': 40, 'new-mexico': 25, 'new-york': 60, 'north-carolina': 26, 'north-dakota': 7, 'Ohio': 29, 'Oklahoma': 4, 'Oregon': 38, 'Pennsylvania': 54, 'rhode-island': 9, 'south-carolina': 6, 'south-dakota': 7, 'Tennessee': 16, 'Texas': 50, 'Utah': 62, 'Vermont': 25, 'Virginia': 27, 'Washington': 92, 'west-virginia': 18, 'Wisconsin': 25, 'Wyoming': 19}\n\nstate_parks_dictionary = {'Alabama': 1, 'Alaska': 1, 'Arizona': 3, 'Arkansas': 2, 'California': 8, 'Colorado': 4, 'Connecticut': 7, 'Delaware': 1, 'Florida': 2, 'Georgia': 2, 'Hawaii': 1, 'idaho-3166': 2, 'Illinois': 10, 'Indiana': 1, 'Iowa': 1, 'Kansas': 1, 'Kentucky': 1, 'Louisiana': 1, 'Maine': 3, 'Maryland': 1, 'Massachusetts': 7, 'Michigan': 5, 'Minnesota': 3, 'Mississippi': 1, 'Missouri': 2, 'Montana': 2, 'Nebraska': 1, 'Nevada': 1, 'new-hampshire': 3, 'new-jersey': 3, 'new-mexico': 2, 'new-york': 5, 'north-carolina': 3, 'north-dakota': 1, 'Ohio': 4, 'Oklahoma': 1, 'Oregon': 3, 'Pennsylvania': 3, 'rhode-island': 1, 'south-carolina': 1, 'south-dakota': 1, 'Tennessee': 2, 'Texas': 4, 'Utah': 3, 'Vermont': 2, 'Virginia': 2, 'Washington': 6, 'west-virginia': 2, 'Wisconsin': 3, 'Wyoming': 1}\nstate_dictionary and state_parks_dictionary store the number of pages required for each state. states simply contains the names of all the states in alphabetical order, and state_name_code_dict helps sort between the name of a state and the way in which it is displayed on TrailForks URLs."
  },
  {
    "objectID": "posts/16B Final Project/index.html#connecting-national-parks-to-individual-trailpark-info",
    "href": "posts/16B Final Project/index.html#connecting-national-parks-to-individual-trailpark-info",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Connecting National Parks to Individual Trail/Park Info",
    "text": "Connecting National Parks to Individual Trail/Park Info\nNow we need to make sure to connect the data that we’ve collected here with the actual table generated by the recommender to give the user more information. Let’s take a look at our output from the similarity score model:\n\noutput\n\nBecause we have two different SQL databases, one for nation-wide park data (trails_new.db) and one with state-wide trail data (trails.db), let’s split this into two different frames.\n\ncalifornia_df = output[output['state'] == 'California (CA)']\nnon_california_df = output[output['state'] != 'California (CA)']\n\nNow we’ll get our databases in our notebook:\n\n!wget https://raw.githubusercontent.com/torwar02/trails/main/trails/trails.db -O trails.db\n!wget https://raw.githubusercontent.com/torwar02/trails/main/trails/trails_new.db -O trails_new.db\n\nThere’s a bit of an issue, though. Let’s look at our table names:\n\nimport sqlite3\ndb_path = 'trails_new.db'\n\nconn = sqlite3.connect(db_path) #Establish connection with DB\ncur = conn.cursor()\n\ncur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") #This specifically grabs all table names from our datbaase.\ntables = cur.fetchall()\ntable_names = [table[0] for table in tables] #Places them into a list\nprint(\"List of tables in the database:\", table_names)\nconn.close()\n\nOur tables aren’t completely in alphabetical order (I was testing around with Maine first, for instance). And some of them aren’t two words, like south-dakota, for instance. But if we compare this to what we have in output:\n\nset(output['state'])\n\nHere we have nice, capitalized state names with two-letter abbreviations. So, then, how are we going to fix this? We’re going to create a dictionary that essentially works as a mapping that takes what we have in output and matches it to what exists in table_names based on some matching criteria:\n\n# Extract unique states and sort them\nunique_states_in_output = sorted(set(output['state']), key=str.lower)\ntable_names = sorted(table_names, key=str.lower)\n\n\n\ndef compare_letters(state_name, table_name):\n    clean_state_name = ''.join(filter(str.isalpha, state_name)).lower() #Eliminate non-alphabetical characters, condense together\n    clean_table_name = ''.join(filter(str.isalpha, table_name)).lower()\n    return sorted(clean_state_name) == sorted(clean_table_name) #Gives a boolean value.\n\nstate_name_to_table_name = {} #Create new dictionary\nfor state_with_abbreviation in unique_states_in_output:\n    state_name = state_with_abbreviation.split(' (')[0]  # Get rid of the parentheses in the abbreviation (like 'South Dakota (SD)')\n    match = next((table for table in table_names if compare_letters(state_name, table)), None) #Generator based on whether or not names are the same\n    if match:\n        state_name_to_table_name[state_with_abbreviation] = match #Update dict if match found\n\nprint(state_name_to_table_name)\n\nNow that’s what we’re looking for! We do a few important things here:\nFirstly, we make sure to get both the states that we have in output and the tables in table_names in alphabetical order. The reason why we do key=str.lower is because some of the table names are written in uppercase while others are in lowercase. This makes it case-insensitive.\nThen we create a helper function called compare_letters which takes two state names (one from output, one from the database) and compares them to see if they have the same letters. We do this by filtering out non-alphabetical characters, spaces, and making everything lowercase and just checking if they have the same letters. The function will just return True or False depending on whether or not they match.\nWe actually use state_name_to_table_name in the for loop below this. We go through each of the states in output. Then, we extract just the part of the state name that comes before the two-letter abbreviation, and then we create a generator that individualls calls compare_letters on each of the names. If it returns True, then we have a match, which then causes the dictionary to be updated. Otherwise, nothing happens and we simply move onto the next entry (that’s why the second argument of next is none).\n##Logic for linking databases\nOur goal is to now go through each recommendation, and match up either the park or trail information corresponding to it (assuming that it’s present in the database). One issue that can arise with this, however, is that the name of the park in output might be different from that of the database. To mitigate this, we’re going to instead compare the coordinates of what’s in output to the rows inside of trails_new.db and trails.db. The idea is that if two parks are close enough to each other in terms of their coordinates, then they should represent the same thing. So, we’re going to make two functions that do similar (but different) things. One will be called fetch_park_info_based_on_coords which will look at parks (i.e., outside of California), and the other will be called fetch_trail_info_based_on_coords\n\ndef fetch_park_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor() #Connect to database\n\n    for table_name in state_name_to_table_name.values(): #This is what we made earlier\n        cursor.execute(f\"SELECT * FROM \\\"{table_name}\\\"\") #Grab everything from the table\n        rows = cursor.fetchall()\n\n        for row in rows: #For each row\n            coords_text = row[2]  # Coords are in the third column\n            try:\n                coords = eval(coords_text) #Kept as a tuple, essentially\n                lat_diff = abs(coords[0] - latitude)\n                long_diff = abs(coords[1] - longitude)\n\n                if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                    return row[3:]  # Don't need name and coords\n            except:\n                continue\n\n    conn.close()\n    return None\n\ndef fetch_trail_info_based_on_coords(db_name, latitude, longitude, margin_lat, margin_long):\n    conn = sqlite3.connect(db_name)\n    cursor = conn.cursor()\n    table_name = 'California'  #Only getting CA trails\n\n    cursor.execute(f\"SELECT * FROM {table_name}\") #Grab everything\n    rows = cursor.fetchall()\n\n    for row in rows:\n        coords_text = row[1]  # Coords are in column 2\n        try:\n            coords = eval(coords_text)\n            lat_diff = abs(coords[0] - latitude)\n            long_diff = abs(coords[1] - longitude)\n\n            if lat_diff &lt; margin_lat and long_diff &lt; margin_long:\n                return row[2:]\n        except:\n            continue  # Skip rows with invalid 'Coords'\n\n    conn.close()\n    return None\n\nOkay, so, it will make a lot more sense if we actually inspect the structure of our database again. Click the link below to see screenshots of two .csv files: the first of parks in Wyoming, and the second is of trails in California:\nhttps://imgur.com/a/6fCixEt\nWith that out of the way, let’s dive into the code. We go through the mapping dictionary that we made previously and we grab all of the possible parks from each one. Then, we look at the third column (i.e., row[2], which represents the third entry in the row) which corresponds to the coordinates (see screenshot), and we record the absolute difference in the coordinates between a given latitude and longitude (we’ll be taking those from output–they’re individual columns rather than a tuple). If both of them are within a specified margin of error, then we’ve found our match. Note that we’re only going to return everything starting from the third column: the first 2 are just the name and coordinates of the trail.\nFor fetch_trail_info_based_on_coords, we have a very similar set-up except for the fact that the coordinates are in the second column, and we’re interested in returning everything after the first two.\nNow, let’s move on so we can see how we actually use these functions!"
  },
  {
    "objectID": "posts/16B Final Project/index.html#putting-it-all-together",
    "href": "posts/16B Final Project/index.html#putting-it-all-together",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Putting it all together",
    "text": "Putting it all together\nThe first thing we’re going to do is to specify the names of the new columns that we want to put into california_df and non_california_df. I’ve just grabbed these from the database:\n\nnew_columns = [\n    'Trails (view details)', 'Total Distance', 'State Ranking',\n    'Access Road/Trail', 'White', 'Green', 'Blue', 'Black',\n    'Double Black Diamond', 'Proline'\n]\nnew_trail_columns = [\n    'Distance', 'Avg time', 'Climb', 'Descent', 'Activities',\n    'Riding Area', 'Difficulty Rating', 'Dogs Allowed',\n    'Local Popularity', 'Altitude start', 'Altitude end', 'Grade'\n]\n\nNow, all we need to do is iterate through the rows of non_california_df to match up the entires!\n\nmargin_lat = 0.1  # Decently generous\nmargin_long = 0.1\nfor index, row in non_california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']): #Some parks have NA coordinates\n        continue\n    park_info = fetch_park_info_based_on_coords('trails_new.db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n    #Remember, this grabs almost all of the columns if a match is found\n    if park_info:\n        non_california_df.loc[index, new_columns] = park_info #We can mass-add new columns\n\nIn the above code, we use the fetch_park_info_based_on_coords function to essentially create a new data frame that contains the information that we want once we match the coordinates. Then, we insert all of these as new columns, taking advantage of the .loc() method from pandas. Now let’s do the same thing for the California df:\n\n\nfor index, row in california_df.iterrows():\n    if pd.isna(row['Latitude']) or pd.isna(row['Longitude']):\n        continue\n\n    park_info = fetch_trail_info_based_on_coords('trails.db', row['Latitude'], row['Longitude'], margin_lat, margin_long)\n\n    if park_info and len(park_info) == len(new_trail_columns):\n        california_df.loc[index, new_trail_columns] = park_info\n    else:\n        pass\n\nOkay, let’s take a look at our results!\n\nnon_california_df\n\nSuccess! It looks like we unfortunately have a few NA values. Unfortunately, it’s hard to guarantee precision in the coordinates. We only had one trail for California:\n\ncalifornia_df\n\nWait, really? I thought we would’ve had this for sure in our database…\nOn closer inspection, we actually do, but hte coordinates on TrailForks versus what we got from the National Park data is a bit off. On the TrailForks page for Zabriskie Point, the coordinates are (36.420820, -116.810120), which is just outside the margin of error.\nNow, let’s move onto Jamie’s section to look at aspects of the website! We’ll start with the image matting tool before putting it all together."
  },
  {
    "objectID": "posts/16B Final Project/index.html#image-matting-jamie",
    "href": "posts/16B Final Project/index.html#image-matting-jamie",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Image Matting – Jamie",
    "text": "Image Matting – Jamie"
  },
  {
    "objectID": "posts/16B Final Project/index.html#introduction-of-image-matting-and-modnet",
    "href": "posts/16B Final Project/index.html#introduction-of-image-matting-and-modnet",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "1. Introduction of Image matting and MODNet",
    "text": "1. Introduction of Image matting and MODNet\n\nMODNet - Portrait Image Matting\nBefore web development, let’s take a look at a fun model about image matting - MODNet. With image matting, we could merge our selfies with pictures of any us national parks in the file background, or you can upload your own choice.\nImage matting, also known as foreground/background separation, is a computer vision technique that aims to accurately extract the foreground object or region of interest from an image, while preserving the fine details and transparency information around the object boundaries. This process generates an alpha matte, which represents the opacity values for each pixel, allowing for seamless composition of the foreground onto a new background.\nThe MODNet (Modulator-Decoupled Network) model is a deep learning architecture specifically designed for image matting tasks. It was introduced in a research paper by Zhanghan Ke, Jingyu Zhang, Kaihao Zhang, Qiong Yan, and Kaiqi Huang in 2022. MODNet stands out from other image matting models due to its unique approach and several key features:\n\nDecoupled Modulation: MODNet decouples the modulation process of the foreground and background features, allowing the model to better capture the intricate relationships between the foreground and background regions. This decoupling helps to improve the accuracy of the alpha matte predictions, especially around complex object boundaries.\nEffective Feature Fusion: MODNet incorporates an effective feature fusion mechanism that combines multi-level features from different stages of the network. This fusion strategy helps to capture both low-level details and high-level semantic information, leading to more accurate and coherent alpha matte predictions.\nLightweight Architecture: Despite its impressive performance, MoDNet has a relatively lightweight architecture compared to other state-of-the-art image matting models. This makes it more efficient and suitable for deployment on resource-constrained devices or in real-time applications.\nImproved Generalization: MODNet demonstrates strong generalization capabilities, meaning it can produce accurate alpha mattes even for objects or scenes that are significantly different from the training data. This is a crucial advantage over many traditional image matting methods that often struggle with generalization.\n\n\n\n\nimage.png\n\n\nThe key innovation of MODNet lies in its decoupled modulation approach, which allows the model to effectively disentangle the foreground and background features, leading to superior performance in capturing intricate object boundaries and transparency information. This architectural design, combined with effective feature fusion and a lightweight structure, has made MoDNet a notable advancement in the field of image matting.\nThere are several other state-of-the-art models for image matting tasks, in addition to the MODNet architecture. Here are some notable ones:\n\nGCA Matting: Proposed in 2020, the Guided Contextual Attention (GCA) model utilizes a two-stream encoder-decoder architecture with a contextual attention module. This module helps the model better capture long-range dependencies and global context information, leading to improved performance on complex scenes.\nAlphaMatting: Introduced in 2021, AlphaMatting is a transformer-based model that leverages the self-attention mechanism to effectively capture long-range dependencies in images. It achieves impressive results, particularly in handling highly complicated backgrounds and foreground objects with intricate structures.\nSHM Matting: The Spatially-Hierarchical Matting (SHM) model, proposed in 2022, employs a hierarchical architecture that processes the input image at multiple spatial scales. This approach helps the model capture both fine-grained details and global structures, leading to improved accuracy, especially around object boundaries.\nBGMatting: Introduced in 2022, BGMatting (Background Matting) is a two-stage model that first predicts a coarse alpha matte and then refines it using a background estimation module. This unique approach helps the model better handle challenging cases with complex backgrounds or semi-transparent objects.\nHDMatt: The High-Definition Matting (HDMatt) model, introduced in 2022, is designed to produce high-resolution alpha mattes by leveraging a progressive upsampling strategy. It achieves impressive results, particularly for high-resolution images, while maintaining a relatively lightweight architecture.\n\nThese models represent some of the latest advancements in the field of image matting, each with its own unique architectural design and strengths. The choice of model often depends on factors such as the complexity of the scenes, the required level of detail, and the computational resources available."
  },
  {
    "objectID": "posts/16B Final Project/index.html#preparation",
    "href": "posts/16B Final Project/index.html#preparation",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "2. Preparation",
    "text": "2. Preparation\n\nIn the top menu of this session, select Runtime -&gt; Change runtime type, and set Hardware Accelerator to GPU.\n\n\nClone the repository, and download the pre-trained model:\n\nFirst we import the os module, which provides functions for interacting with the operating system."
  },
  {
    "objectID": "posts/16B Final Project/index.html#note-this-was-originally-done-in-a-google-colab-notebook.-there-are-certain-functionalities-commands-like-downloading-from-google-drive-navigating-through-colab-that-are-not-possible-to-exactly-replicate-here.-as-a-result-i-have-placed-images-of-the-colab-notebook-instead.",
    "href": "posts/16B Final Project/index.html#note-this-was-originally-done-in-a-google-colab-notebook.-there-are-certain-functionalities-commands-like-downloading-from-google-drive-navigating-through-colab-that-are-not-possible-to-exactly-replicate-here.-as-a-result-i-have-placed-images-of-the-colab-notebook-instead.",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "Note: This was originally done in a Google Colab notebook. There are certain functionalities (commands like downloading from Google Drive, navigating through Colab) that are not possible to exactly replicate here. As a result, I have placed images of the Colab Notebook instead.",
    "text": "Note: This was originally done in a Google Colab notebook. There are certain functionalities (commands like downloading from Google Drive, navigating through Colab) that are not possible to exactly replicate here. As a result, I have placed images of the Colab Notebook instead.\n\n\n\nColabImage1"
  },
  {
    "objectID": "posts/16B Final Project/index.html#upload-images",
    "href": "posts/16B Final Project/index.html#upload-images",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "3. Upload Images",
    "text": "3. Upload Images"
  },
  {
    "objectID": "posts/16B Final Project/index.html#inference",
    "href": "posts/16B Final Project/index.html#inference",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "4. Inference",
    "text": "4. Inference\n\n\n\nColabImage4"
  },
  {
    "objectID": "posts/16B Final Project/index.html#visualization",
    "href": "posts/16B Final Project/index.html#visualization",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "5. Visualization",
    "text": "5. Visualization\n \ncombined_display, takes an image and its corresponding matte (alpha channel) as inputs and returns two images: one for combined display and the other for the middle image (foreground).\nHere’s what each part of the function does:\n\nCalculate Display Resolution:\n\nIt calculates the display resolution for the output image.\nw and h store the width and height of the input image, respectively.\nrw is set to 800, indicating the desired width for the output image.\nrh is calculated to maintain the aspect ratio of the input image.\n\nObtain Predicted Foreground:\n\nConvert the input image and matte to NumPy arrays (image and matte).\nCheck if the input image is grayscale or has an alpha channel. If so, convert it to a 3-channel image.\nRepeat the matte across channels and normalize it.\nCalculate the predicted foreground by applying the matte to the input image.\n\nCombine Image, Foreground, and Matte:\n\nConcatenate the input image, predicted foreground, and matte along the horizontal axis.\nConvert the combined array back to an image (Image.fromarray) and resize it to the calculated resolution.\n\nExtract Middle Image:\n\nConvert the predicted foreground array to an image (Image.fromarray) to extract the middle image.\n\nReturn Output:\n\nReturn the combined display image and the middle image.\n\n\nHere’s the explanation of the return values: - combined: The combined image showing the original image, predicted foreground, and matte (alpha channel) concatenated horizontally. - middle_image: The image representing the predicted foreground, extracted from the combined image.\n \nAs you can see, the first line of images corresponds to original image, image we get, and matte(alpha)\nThe second line is the image we merged with background.\nLet’s break down what each part does:\n\nIterating Through Image Files:\nimage_names = os.listdir(input_folder)\nfor image_name in image_names:\n\nThis loop iterates through each file name in the input_folder, which contains the input images.\n\n\n\nObtaining Matte File Name:\n    matte_name = image_name.split('.')[0] + '.png'\n\nIt extracts the file name of the matte corresponding to the current image by splitting the image file name at the ‘.’ character and appending ‘.png’ to it.\n\n\n\nOpening Image and Matte:\n    image = Image.open(os.path.join(input_folder, image_name))\n    matte = Image.open(os.path.join(output_folder, matte_name))\n\nIt opens the input image and matte files using Image.open() from the PIL library, specifying their respective paths.\n\n\n\nVisualizing Combined Image and Middle Image:\n    combined, middle_image = combined_display(image, matte)\n\n    # Display combined image\n    display(combined)\n\n    # Display middle image\n    display(middle_image)\n\nIt calls the combined_display() function to create the combined image and extract the middle image (predicted foreground).\nThen, it displays both the combined image and the middle image using display().\n\n\n\nMerging Middle Image with Background:\n    merged = Image.composite(middle_image, background_image, matte)\n\nIt composites the middle image with a background image using the alpha channel provided by the matte.\n\n\n\nPrinting Image Name:\n    print(image_name, '\\n')\n\nIt prints the name of the current image file.\n\n\n\nDisplaying Merged Image:\n    display(merged)\n\nIt displays the merged image, which combines the middle image with a background using the provided matte."
  },
  {
    "objectID": "posts/16B Final Project/index.html#implementation-in-web",
    "href": "posts/16B Final Project/index.html#implementation-in-web",
    "title": "Get on the Trail! PIC16B Final Project Overview",
    "section": "6. Implementation in Web",
    "text": "6. Implementation in Web\nWith all the functions we made, we want to realize it in our web. Just like this:\n\n\n\nColabImage9\n\n\nHow are we going to achieve this?"
  },
  {
    "objectID": "posts/Homework 4/index.html",
    "href": "posts/Homework 4/index.html",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "",
    "text": "Aside from PIC 16A-level Python knowledge, this post expects you to be decently familiar with linear algebra and multivariable calculus. The formulas I use are taken from Professor Ko’s materials.\nMany physical processes are modeled using differential equations, but because such equations involving limiting processes, actually visualizing them in their purest form is not possible. As such, we have to use some sort of approximation method in order to actually do so. Linear methods are the simplest, as you have probably seen in a calculus class.\nWhen we attempt to approximate functions of multiple variables, however, it is often impractical to write out linear approximations using linear combinations of terms. It is, however, much easier to condense this information into a matrix and use matrix-vector multiplication in order to discretize the solution of a differential equation based on some initial values and a step-size (essentially, similar to a linear recurrence relation).\nIn our case, we want to approximate a solution to the two-dimensional heat equation, which models the amount of heat present in two-dimensional space as it diffuses over time. The differential equation is as follows:\n\\(\\frac{\\partial f(x,t)}{\\partial t} = \\frac{\\partial^2f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}\\)\nThat is, the rate at which the amount of heat present at a specific point in two-dimensional changes over time is related to the acceleration of this heat over a spatial dimension. It should not be surprising, therefore, that a discrete, linear approximation of a solution thus depends on having access to values of \\(f\\) at points neighboring a given point \\((x,y)\\):\n\\(u_{i,j}^{k+1} \\approx u_{i,j}^{k} + \\epsilon\\left(u_{i-1,j}^{k}+u_{i+1,j}^{k}+u_{i,j-1}^{k}+u_{i,j+1}^{k} - 4u_{i,j}^{k}\\right)\\)\nThat is, at time \\(k+1\\), the amount of heat present at \\((i,j)\\) is approximately equal to the amount of heat present at the previous time plus a step size \\(\\epsilon\\) (think about this as a very small change in time) multiplied by the amount of heat present in the four neighboring points minus four times the current amount of heat–that is, it depends on the amount of heat coming in from the point’s surroundings and also the amount of heat going out into the four neighboring directions.\nOne final thing is what’s known as the “boundary condition” which allows for heat to escape from the system. If \\(f\\) takes values over an \\(N\\times N\\) grid, then we denote \\(u_{-1,j}^k = u_{N,j}^k = u_{i,-1}^k = u_{i,N}^k = 0\\) for all \\(k\\) (assuming we index from zero, which would mean that the indices \\(-1\\) and \\(N\\) are not found on our original grid).\n\n\nPer Professor Ko’s requests, we will be using the following specifications for our model:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nN = 101\nepsilon = 0.2\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nu_test = u0.copy()\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nThat is, a 101 by 101 grid, a step size of 0.2, and no heat at all except for one unit in the very middle. Based on this, we’re going to be investigating four different ways we can model the approximation presented earlier."
  },
  {
    "objectID": "posts/Homework 4/index.html#base-assumptions",
    "href": "posts/Homework 4/index.html#base-assumptions",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "",
    "text": "Per Professor Ko’s requests, we will be using the following specifications for our model:\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\nN = 101\nepsilon = 0.2\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nu_test = u0.copy()\nplt.imshow(u0)\n\n\n\n\n\n\n\n\nThat is, a 101 by 101 grid, a step size of 0.2, and no heat at all except for one unit in the very middle. Based on this, we’re going to be investigating four different ways we can model the approximation presented earlier."
  },
  {
    "objectID": "posts/Homework 4/index.html#approximation-matrix",
    "href": "posts/Homework 4/index.html#approximation-matrix",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "Approximation Matrix",
    "text": "Approximation Matrix\nIn particular, the matrix that we will be multiplying our heat vector by is of size \\(N^2*N^2\\) and will look something like this:\n\nWhere each \\(B\\) is a matrix that looks like the following:\n\nProfessor Ko showed us a way to generate the matrix which encodes the linear transformation for our approximation. Here, I’ve encoded it in a function called get_A which, along with all the other functions for this post, is in heat_equation.py.\n\nimport inspect\nimport heat_equation\nprint(inspect.getsource(heat_equation.get_A))\n\ndef get_A(N):\n    \"\"\"Prepares a linear approximation for advancing a two-dimensional heat\n    simulation in the form of a matrix\n    Args:\n        N: An integer denoting the side length of the square grid for the simulation\n        \n    Returns:\n        A: An NxN matrix (numpy array) that encodes the approximation transformation\n    \"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\n\n\nA = get_A(N)\n\nprint(A.shape,'\\n')\nprint(A)\n\n(10201, 10201) \n\n[[-4.  1.  0. ...  0.  0.  0.]\n [ 1. -4.  1. ...  0.  0.  0.]\n [ 0.  1. -4. ...  0.  0.  0.]\n ...\n [ 0.  0.  0. ... -4.  1.  0.]\n [ 0.  0.  0. ...  1. -4.  1.]\n [ 0.  0.  0. ...  0.  1. -4.]]\n\n\nAs we can see, our matrix has \\(101\\cdot101 = 10201\\) rows/columns and the (block) diagonal shape as presented above. Now, let us implement our simulation using matrix multiplication. This first function, advance_time_matvecmul, was made by professor Ko.\n\nprint(inspect.getsource(heat_equation.advance_time_matvecmul))\n\ndef advance_time_matvecmul(A,u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: An N x N numpy array used to operate on the heat grid.\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at instant k+1\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\n\nRecall that @ is the matrix multiplication operator. Thus, the function takes the current grid u, and adds epsilon times the linear combination as shown above but in the form of matrix-vector multiplication, namely, our special matrix A multiplied by the grid u flattened into a one-dimensional array. This was why we needed A to be an extremely large matrix, by the way–if we were to put all of the entries in u into a single vector, it would indeed be \\(101\\cdot101 = 10201\\) entries long.\nLet’s, then, try visualizing the spread of the heat over time! We’ll make a function that helps visualize this for us. As specified by Professor Ko, we want to run the simulation for 2700 iterations and plot the system for every 300 iterations in a 3 by 3 grid. We’ll also make sure our function times how long it takes for our simulation to run (I use the default_timer from the timeit library).\n\nfrom timeit import default_timer #To time\ndef visualize_plot(func,u0,A=None,direct_manip = True):\n    \"\"\"Plots the state of a heat diffusion simulation in a 3x3 grid\n    every 300 iterations up to iteration 2700.\n    Args:\n        func: The function used to conduct the simulation\n        u0: The matrix/array that contains the initial state of the system.\n        A: The matrix used to carry out the linear approximation for heat diffusion.\n           Note that this does not need to be specified for functions that directly\n           manipulate ``u0``\n        direct_manip: A boolean set by default to True which controls whether or not\n                      our function uses a matrix multiplication or direct manipulation\n                      technique\n    \"\"\"\n    fig, axs = plt.subplots(3, 3, figsize=(12, 12)) #Makes 3 by 3 grid\n    start = default_timer()\n    heat_matrix = u0.copy() #Copy of the original `u0`\n    for i in range(1,2701):\n        if direct_manip:\n            heat_matrix = func(heat_matrix,epsilon)\n        else:\n            heat_matrix = func(A,heat_matrix,epsilon) #Progresses the simulation by 1 step for\n        if i % 300 == 0:\n            axs[(i-300)//900, ((i-300)%900)//300].imshow(heat_matrix) #Places graph onto grid according to\n            #iteration number.\n    end = default_timer() #Finish timer\n    print(f\"Time to execute: {end-start} seconds\") #Print necessary time\n\nAs you can see our function above takes in a simulation function of some sort (there will be others!) and loops through 2700 different iterations and graphs every 300 on a grid of matplotlib subplots. As for the coordinates of these plots, you can figure this out with some basic modular arithmetic (both are shifted by 300 to account for indexing from 0. The rows involve integer division by 900 while the columns are the remainder when divided by 300 within each group of 900).\nNote that the input A corresponds to the matrix that we use to advance the simulation. The argument direct_manip will become relevant later in this post, but we essentially need it so that we know what sort of function we’re dealing with.\nWe also use the default_timer() function from the timeit package to see how long it takes for our code to run. Let’s give it a try.\n\nu0 = u_test.copy()\nvisualize_plot(heat_equations.advance_time_matvecmul,u0,A,direct_manip = False)\n\nTime to execute: 40.459757999982685 seconds\n\n\n\n\n\n\n\n\n\nWell, this looks really cool! Unfortunately, it also takes a long time to run…40 seconds for something like this is way too long and amounts to nearly 14 milliseconds per iteration. Why does this take so long? Well, recall that we have to multiply a matrix by a vector. This involves conducting \\(10201^2\\) multiplications per iteration, which requires a large amount of computing power.\nWhat can we do to fix this? Well, recall from when we first defined A that most of the elements are zero– in fact, if we recall, there are 7 non-zero entries in each 3-by-3 block matrix, which makes for about \\(\\frac{10201}{3}\\cdot7\\) non-zero entries out of the massive \\(10201^2\\) total, which is only .023%, more or less! This means that over 99% of our computations are wasted on multiplying numbers by zero (and we know what that evaluates to)."
  },
  {
    "objectID": "posts/Homework 4/index.html#quick-introduction-to-jnp",
    "href": "posts/Homework 4/index.html#quick-introduction-to-jnp",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "Quick introduction to jnp",
    "text": "Quick introduction to jnp\njax is actually a machine learning package, but what makes it special is that it has been created in such a way as to optimize computationally intensive operations. As such, it has its own way of processing arrays which means that, by default, jax functions are not compatible with numpy arrays. However, there is a module within jax called jnp that essentially allows us to use jax arrays with almost the same functionalities as numpy arrays. This is why we will need to import both sparse from jax.experimental as well as jax.numpy in order to make our second function–advance_time_spmatvec–work. Also, jax arrays get converted into numpy arrays when interacting with matplotlib plots, so we’ll have no issue there as well.\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nimport jax\nprint(inspect.getsource(heat_equation.advance_time_spmatvec))\n\ndef advance_time_spmatvec(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication of sparsified BCOO matrices\n    Args:\n        A: an N x N sparsified array used to operate on the heat grid.\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at instant k+1\n    \"\"\"\n    N = u.shape[0]\n    u= u + epsilon * (A @ jnp.ravel(u)).reshape((N, N))\n    return u\n\n\n\nA few things to note:\nA ‘normal’ array is referred to as a dense array, which we have to convert into a sparse one using the fromdense() function from sparse. In heat_equation, I implemented a function, get_sparse_A that allows us to create a sparse matrix that is numerically identical to A as in the previous case:\n\nprint(inspect.getsource(heat_equation.get_sparse_A))\n\ndef get_sparse_A(N):\n    \"\"\"Prepares a linear approximation for advancing a two-dimensional heat\n    simulation in the form of a sparse `jnp`array.\n    Args:\n        N: An integer denoting the side length of the square grid for the simulation\n        \n    Returns:\n        A: An NxN matrix (BCOO sparsified `jnp` array)\n        that encodes the approximation transformation\n    \"\"\"\n    A = get_A(N)\n    A_sp_matrix = sparse.BCOO.fromdense(A)\n    return A_sp_matrix\n\n\n\nWe rely on the get_A function that we saw earlier simply because the .fromdense method included in sparse.BCOO can take a numpy array and convert it to a sparce one.\n\nA_sp_matrix = heat_equation.get_sparse_A(N)\nprint(A_sp_matrix)\n\nBCOO(float32[10201, 10201], nse=50601)\nBCOO(float32[10, 10], nse=100)\n\n\nBCOO stands for “Batched coordinate.” The object that we create is created specifically to work with sparse matrix functions. Note that printing out the sparse matrix of type BCOO does not display the numbers of the matrix.\nThe other important thing to note is that we use the function jnp.ravel instead of np.flatten. I’m not sure why there isn’t a method called flatten for jax, but a simple Google search cleared this all up:\nhttps://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ravel.html\n\nu0 = u_test.copy()\nvisualize_plot(heat_equations.advance_time_spmatvec, u0,A_sp_matrix, direct_manip = False)\n\nTime to execute: 1.7815180409816094 seconds\n\n\n\n\n\n\n\n\n\n1.8 seconds. Much faster! Our timing is now only based on doing a number of multiplications proportional to \\(N^2 = 10201\\) rather than \\(N^4 = N^2\\cdot N^2=102^4 = 108243216\\) calculations."
  },
  {
    "objectID": "posts/Homework 4/index.html#attempt-1-naive",
    "href": "posts/Homework 4/index.html#attempt-1-naive",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "Attempt 1 (Naive)",
    "text": "Attempt 1 (Naive)\nMy first time around, I tried manually doing this entry-by-entry using for loops and arrived at the following result:\n\ndef advance_time_numpy_naive(u, epsilon):\n    N = u.shape[0]\n    u_expanded = np.hstack((np.zeros(N+2).reshape([N+2,1]),np.hstack((np.vstack((np.vstack((np.zeros(N).reshape([1,N]),u)),np.zeros(N).reshape([1,N]))),np.zeros(N+2).reshape([N+2,1])))))\n    #Way too long\n    for i in range(u_expanded.shape[0]):\n        for j in range(u_expanded.shape[1]):\n            if ((i in range(1,N+1)) and (j in range(1,N+1))):\n                u_expanded[i,j] = u_expanded[i,j]+epsilon*(u_expanded[i-1,j]+u_expanded[i+1,j]+u_expanded[i,j-1]+u_expanded[i,j+1]-4*u_expanded[i,j])\n    #Cut off, but the above manually copies the approximation`            \n    return u_expanded[1:N+1,1:N+1]\n\n\nu0 = u_test.copy()\nvisualize_plot(advance_time_roll_naive, u0,A)\n\nTime to execute: 23.30165791699983 seconds\n\n\n\n\n\n\n\n\n\nSo, 23 seconds is better than the original matrix multiplication method, but this is still far too slow! Also, what on earth is going on with my code? Well, here’s an outline of what I wanted to do:\n\nPass in a matrix and surround it with zeroes (u_expanded) so that we can let the heat escape\nFor each entry present in the original matrix (hence the conditional statement in the nested for loop), update it so that we use the discrete approximation of the differential equation’s solution.\nReturn the subset of the expanded matrix that corresponds to the original matrix.\n\nThis, however, is quite slow since we go through every single entry. Also, the code for making the matrix surrounded by zeroes is really messy as well. After a while, though, I figured out that I could make major improvements:\n\nThere is a function in numpy called pad that can be used to surround a two-dimensional numpy array with rows/columns of a specified input (seriously, all I had to do was Google “numpy surround matrix” and find this StackOverflow post: https://stackoverflow.com/questions/35751306/python-how-to-pad-numpy-array-with-zeros)\nWe can use numpy’s vectorized matrix arithmetic functionalities to make things a lot smoother."
  },
  {
    "objectID": "posts/Homework 4/index.html#attempt-2-final",
    "href": "posts/Homework 4/index.html#attempt-2-final",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "Attempt 2 (Final)",
    "text": "Attempt 2 (Final)\nI’ll go over the syntax for pad when we look at my actual function, but the part that conceptually took the most work was figuring out how to update each entry without having to use a for loop.\nRecall from PIC16A that if we use an arithmetic operation between two numpy arrays with the same dimensions, we essentially apply the operation element-wise:\n\nA = np.array([[1,2],[3,4]])\nB = np.array([[7,8],[9,10]])\nprint(A+B)\nprint(A*B) #This is the Hadamard (element-wise) product, NOT matrix multiplication!\n\n[[ 8 10]\n [12 14]]\n[[ 7 16]\n [27 40]]\n\n\nThese operations go by quick, so it would be very nice if we could apply this onto u in our function. The problem, though, is that these operations only work on each involved matrix’s corresponding element–if we add four different matrices, for example, we add each matrix’s [0,0], [0,1], … and so on’s elements. But our discrete approximation references neighboring elements of a matrix. So, then, what are we going to do?"
  },
  {
    "objectID": "posts/Homework 4/index.html#vectorized-manipulation",
    "href": "posts/Homework 4/index.html#vectorized-manipulation",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "Vectorized Manipulation",
    "text": "Vectorized Manipulation\nRecall that we essentially need to implement the following step: \\(u_{i,j}^{k+1} \\approx u_{i,j}^{k} + \\epsilon\\left(u_{i-1,j}^{k}+u_{i+1,j}^{k}+u_{i,j-1}^{k}+u_{i,j+1}^{k} - 4u_{i,j}^{k}\\right)\\)\nSo we need data from five different points: the point \\((i,j)\\), and the four points immediately above and below it. If we want to take advantage of vectorized operations, we need to make other matrices with the same dimensions but with the elements slightly shifted. That is, if we want to change u[1,1] by adding epsilon times u[0,1], for example, we need to make a new matrix u_shift such that u_shift[1,1] = u_[0,1]. But, how are we going to do this? I’ll show an example.\nConsider a much more simplified version of our problem, where we only have heat values in a 3-by-3 matrix:\n\nWhen we apply our border of zeroes onto it, it will look like this:\n\nNow, let’s say we want the simulation to continue by one step. Let’s focus on the entry [1,1] indexing from 0, that is 5. In addition to this point itself, we need 4 other points: the ones directly above/below and next to the point:\n\nOur plan, then, will be to make a 3-by-3 matrix that has each of these entries (4,6, 2, and 8) out of the expanded matrix such that the entry [1,1] corresponds to the number we want. For instance, let’s start with 4. We need to make this the center of a 3 by 3 matrix, so imagine drawing a 3-by-3 grid around the point and crossing everything else out:\n\nThis is equivalent to taking the expanded matrix, removing the first and last row, and removing the last two columns. In other words, we would have 4 equal to u_expanded[1:-1, :-2] (remember subsetting form PIC16A?). Let’s see if this logic holds for another point. What if instead we wanted to do this for the number 7, which is [0,2] in our original matrix? We would need the 0 on the left to be the entry [0,2] of a 3x3 matrix–that is, the bottom left:\n\nWe do the exact same thing! Remove the top and bottom rows, and remove the left two columns. It looks like this logic will work for the entirety of our matrix. Let’s apply the same analogy to the point to the right of our point of interest, so going back to [1,1], we need to make a 3x3 subset of the expanded matrix where the 6 is in the center:\n\nWhen it comes to the rows we get rid of, we do the same thing (omit first and last), but this time, we get rid of the first two columns from the left, so we should ideally have u_expanded[1:-1, 2:]. Below you can see the logic that applies for the entries above and below our point of interest:\n\nSo, for getting the entry above, we should eliminate the first and final column as well as the bottom two rows. For the entry below, we also eliminate the first/final column but eliminate the first two rows. All in all, we have the following pattern:\n\nNow we’re ready to show the actual code:\n\nprint(inspect.getsource(heat_equation.advance_time_numpy))\n\ndef advance_time_numpy(u, epsilon):\n    \"\"\"Advances the simulation by one timestep by directly manipulating the values of the heat grid.\n    Args:\n        u: N x N grid state at timestep k\n        epsilon: stability constant\n\n    Returns:\n        N x N Grid state at instant k+1\n    \"\"\"\n    u_expanded = np.pad(u, ((1,1),(1,1)), mode = 'constant', constant_values = 0)\n    u +=  epsilon * (u_expanded[:-2, 1:-1] + u_expanded[2:, 1:-1] + u_expanded[1:-1, :-2] + u_expanded[1:-1, 2:] - 4 * u)\n    \n    return u"
  },
  {
    "objectID": "posts/Homework 4/index.html#np.pad",
    "href": "posts/Homework 4/index.html#np.pad",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "np.pad",
    "text": "np.pad\nLet’s start with np.pad. This will, as the name suggests, pad an existing numpy array with new rows/columns. The first argument is the array itself, and the second argument is a tuple of tuples. Each tuple is the number of rows/columns that we want to wrap around, either “before” (i.e., above/to the left) the array or “after” (i.e., below/to the right) the array. Since we just want all zeroes to be surrounding the matrix, we just have to fill in ones for the entries. We set mode equal to constant to make sure that we’re inputting the same value for all of the new entries, and we make that value 0 with the constant_values argument."
  },
  {
    "objectID": "posts/Homework 4/index.html#the-calculation",
    "href": "posts/Homework 4/index.html#the-calculation",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "The calculation",
    "text": "The calculation\nThe explanation I gave previously was simply a set-up to make this part much easier to understand. First of all, recall from our discretization that we add the term multiplied by \\(\\epsilon\\) to the current entry \\(u_{i,j}^k\\), which is why we use the += operator. We use this on u and not u_expanded becuase we don’t care about the “extra” entries that are only found in the latter but not in the former for the purposes of the return value. Of course, the values present in u_expanded are extremely important for the right-hand side of this operator. As shown in my diagram, the four subsets corresponding to u_expanded are, in the order shown in my function, the entries above, below, to the left, and to the right, of any given entry in u. Since, like we said earlier, matrix arithmetic operations are vectorized, this will apply to every possible entry in the matrix nearly at once! Finally, we subtract off 4*u as that represents the heat flowing out of the point. Then, we return the grid u.\nLet’s see how fast our function is.\n\nu0 = u_test.copy()\nvisualize_plot(heat_equations.advance_time_roll, u0, direct_manip = True)\n\nTime to execute: 0.11722458299482241 seconds"
  },
  {
    "objectID": "posts/Homework 4/index.html#matrix-multiplication-methods",
    "href": "posts/Homework 4/index.html#matrix-multiplication-methods",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "Matrix Multiplication Methods",
    "text": "Matrix Multiplication Methods\n\nNaive (numpy)\nWe started this post using the most naive method possible–manually multiplying a gigantic matrix on our system over and over again. While mathematically beautiful to understand a differential equation as a series of matrix-vector products, it was computationally incredibly expensive, taking nearly 40 seconds.\nWhile it wasn’t difficult for me to implement, it would’ve been difficult to create the get_A function from scratch.\n\n\nImproved (sparse)\nWe were able to speed up this process so that it took less than two seconds by taking advantage of sparse matrices from jax. This was able to speed up our process a lot by eliminating many unnecessary calculations. It is important to recognize, however, that we were only able to leverage this method because the matrix A mostly had zeroes. Nonetheless, the increased efficiency was very helpful, and learning how to use jax for this purpose was very easy."
  },
  {
    "objectID": "posts/Homework 4/index.html#direct-manipulation-methods",
    "href": "posts/Homework 4/index.html#direct-manipulation-methods",
    "title": "How fast we can we discretize a second-order differential equation?",
    "section": "Direct Manipulation Methods",
    "text": "Direct Manipulation Methods\n\nNaive (numpy)\nTaking advantage of vectorization to improve our process was very helpful and also less mathematically intensive in the sense that it was easy to wrap my head around the idea of subtracting heat from the center and adding that of the surroundings. However, figuring out the exact logic of what entries to add where did take some time–as can be seen in the fact that I had to draw some diagrams–. However, this method proved to be very quick, so I think it was definitely worth it. Furthermore, even though figuring out the correct commands in the function’s body was difficult, the code ended up being quite simple! I’m glad we did it this way, and its great that it’s a lot faster than even the method that uses sparse matrices. Anything involving multiplications takes up a lot of computing power, so it’s great that we were able to avoid most of them here.\n\n\nImproved (jit)\nAnd, if that wasn’t good enough, slightly changing our function and taking advantage of just-in-time compilation to make Python run even faster made our function execute lightning fast–nearly 800 times faster than our very first one. Even though I’ve been only given a very simple introduction to jax as a whole, I was able to use it to help increase my code’s efficiency, so it wasn’t very difficult at all!"
  },
  {
    "objectID": "posts/Homework 2/index.html",
    "href": "posts/Homework 2/index.html",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "",
    "text": "It’s natural to be interested in works created by certain artists, and movies are no exception–if there’s an actor or actress that you’re a fan of, there’s a good chance that you’ll like the other films that they’ve appeared in. There are websites, like https://www.themoviedb.org, that conveniently arrange all of that information, but it can be tedious for a human being to have to go through it page by page.\nThe internet, though, is made up of computers, and as such, has a structure to it that we can manipulate using computers. In particular, webpages are often designed with HTML–the ML of which means “markup language.” This means that websites are usually sorted into organized pieces of information that contain different markers, called tags, that signal what they’re used for. HTML tags are of the form &lt;tag&gt; ... &lt;/tag&gt;, where the information within the dots could be text, link, other tags, or much more things.\nBy using a technique called web scraping, we can sift through web pages to search for specific tags and grab whatever information we want. To do this, we’re going to use a framework called scrapy that’s written in Python, but we won’t really be using a jupyter notebook as we have been previous. Rather, we’ll be editing files directly that use Python syntax but have a lot of machinery under the hood. The purpose of this post is not to meticulously detail every aspect of scrapy–I would have no idea how to do that–but rather to introduce some of its web scraping capabilities so that you can carry out something like this yourself.\nWe are going to be scraping the aforementioned website https://www.themoviedb.org, but before we get to that, let’s go over how to set up scrapy."
  },
  {
    "objectID": "posts/Homework 2/index.html#creating-the-project",
    "href": "posts/Homework 2/index.html#creating-the-project",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Creating the Project",
    "text": "Creating the Project\nEvery scrapy project starts as some sort of folder on your system, but we have the choice of what directory we’d like to place that folder in. Because I’m putting my project together with my Quarto blog, I navigated to the folder to this post to create my project. To do so, use the following syntax:\ncd \"folder name\"\n...\ncd \"final folder name\"\nThat is, each time you call cd and specify a folder name, you will change your current directory to that folder. If you want to place your project into a folder within a folder, you must call cd multiple times, each time specifying the name of a folder you’d like to navigate to.\nThen, to generate a (blank) scrapy project, enter the following commands:\nconda activate \"environment name\"\nscrapy startproject \"project name\"\nReplace environment name with whatever Anaconda environment has scrapy installed. This is necessary for us to run the command after, which will create the project, giving it whatever name you specify where I have written project name. In my case, the commands looked like this:\nconda activate PIC16B-24W\nscrapy startproject TMDB_scraper\nThis will create a folder with the name TMDB_scraper which contains everything we need for the web scraper. It is absolutely essential that you then change your working directory to the project folder, otherwise, your scraper won’t work!\ncd TMDB_scraper"
  },
  {
    "objectID": "posts/Homework 2/index.html#creating-the-scraper",
    "href": "posts/Homework 2/index.html#creating-the-scraper",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Creating the Scraper",
    "text": "Creating the Scraper\nNow, within Finder (or Explorer for Windows), open the newly created TMDB_scraper folder. You should see the following: 1. A file called scrapy.cfg 2. Another folder also with the name TMDB_scraper\nWe’ll return to scrapy.cfg in a bit–it contains some more general configurations for how scrapy works on our system. For now, though, open TMDB_scraper.\nInside of this folder, you should see something like the following:\n\n\n\nImage 1\n\n\nWe will be returning to settings.py later. The other .py files are not going to be explored for the purposes of this exercise, but more advanced web-scraping projects will require you to edit them.\nFor now, open the folder called spiders. This is where we will manipulate our spider–which we can think of a little “device” that we set up to crawl through our website. It’s what does the scraping for us, and it’s where we’ll have to do most of the coding.\nYou will be greeted by a file called __init__.py. Open it using the program of your choice. I always use Xcode.\nThis should file should be blank besides a comment that is generated by default. Please paste the following into the file (generated by Professor Ko). Below, I’ll explain what the code means, and how we’re going to use it:\n# to run \n# scrapy crawl tmdb_spider -o movies.csv -a subdir=137113-edge-of-tomorrow\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir=None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n        \ndef parse(self, response):\n    yield None\n\ndef parse_full_credits(self, response):\n    yield None\n\ndef parse_actor_page(self, response):\n    yield None\nparse, parse_full_credits, and parse_actor_page yield None as a placeholder. We’ll add to the bodies of these functions later.\nSince our environment already has scrapy, we should be able to import it without any issue.\nFirst of all, note that we have to declare a class in order for our spider to exist. The name of the class can be whatever you want, but here, I name it TmdbSpider. The class only takes one input, namely, scrapy.Spider. In the body of the class, we define a class variable called name, which will determine the name of the spider which we will use when calling it in the command line, so make sure to give it a name that makes sense! Here, I use tmdb_spider.\nThen, for the initialization, besides the mandatory parameter self (recall from PIC16A), we include the argument subdir which by default, we set to None and also allow for additional arguments. We will pass arguments into the spider via the command line. Within the initialization, we have to declare an instance variable called start_urls which is a list of one string, in our case. The element of this list determines what site our spider will start on. I say start on, because by crawling through a website, it can effectively “click” on other links on the page. Here, we declare an f string that takes us to the page of a given movie on themoviedb website. After /movie/, each film has its own unique extension which is precisely what the argument subdir determines. My favorite movie is the action film Edge of Tomorrow starring Tom Cruise, so we’ll be using that for the purposes of this blog. You can pick whatever movie you’d like, but make sure to use the appropriate subdir.\nA method specifically named parse is required in order for our spider to work. Before we get to that, though, it’s important to understand on a conceptual level what we want our spider to do:"
  },
  {
    "objectID": "posts/Homework 2/index.html#website-navigation",
    "href": "posts/Homework 2/index.html#website-navigation",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Website Navigation",
    "text": "Website Navigation\nRecall that our overall goal is to grab the names of all movies that the actors in a given movie appear in. Let’s take a look at our movie’s page to see how this works:\n\n\n\nImage 2\n\n\nThe text circled in red, “Full Cast & Crew”, is actually a link that will take us to a page that lists all of the people credited in the movie with the following URL:\nhttps://www.themoviedb.org/movie/137113-edge-of-tomorrow/cast\nNotice that this is nearly identical to the previous link except with /cast added to the end. Here’s what the page looks like:\n\n\n\nImage 3\n\n\nThen, if we click on an actor or actress’s name, we’re taken to a page with a somewhat different directory. In the case of Tom Cruise, we have:\nhttps://www.themoviedb.org/person/500-tom-cruise\nIf we scroll down on the page, we see that we have a list of movies that Cruise has been in:\n\n\n\nImage 4\n\n\nClicking on the name of the movie will in fact take you to the corresponding page about the movie, but we’re not interested in that. Once we get to this page, we just want to grab the names of the movies, and then repeat this process for the other actors."
  },
  {
    "objectID": "posts/Homework 2/index.html#specifications",
    "href": "posts/Homework 2/index.html#specifications",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Specifications",
    "text": "Specifications\nBefore giving an outline of what we want our spider to do, we should clarify what we want the final output to be. It would be convenient for us to return a .csv file containing each actor’s name and the name of the movie they’ve been in for each entry. The part where we generate the .csv will actually be done in the command line for us–all we need to do with our spider is to generate a dictionary that corresponds to the entries, where the keys are the name of the columns. Hence, we’ll want one for the actor’s name, and one for the name of the movie.\nOkay, let’s inspect the HTML source code of our pages. Note that we’ll start with the “cast” page since all we need to do to get here from the movie page is just by adding cast to the URL. Thus, we won’t actually need to have the spider interact with the movie page itself.\n\n\n\nImage 5\n\n\nOkay, what’s going on here? Remember, tags in HTML are always enclosed in something like &lt;tag&gt;...&lt;/tag&gt;, and a webpage will usually nest lots of tags inside of each other.\nWe see that there is an &lt;ol&gt; tag–which stands for ordered list–of class people credits. &lt;ol&gt; is a tag built into HTML, whereas people credits is just one class of ol among others that exist on the website. If you see on the left-hand side, we can access this with ol.people.credits. Now, just letting you know ahead of time, we can essentially make our spider retrieve content that exists under certain headings, which is why we want to keep track of what sort of tags and classes are relevant for us.\nThen, within the &lt;ol&gt;, we see that we have a bunch of &lt;li&gt; tags, which stands for list item. Let’s look in more detail at the first one (which is opened up in the above image).\n\n\n\nImage 6\n\n\nEverything between the underlined &lt;li&gt; and &lt;/li&gt; is contained within the list element. We see that there are several tags within. One type is called &lt;div&gt;, which stands for division. This divides &lt;li&gt; into different sections. There is another tag called &lt;a&gt;, which contains a link. See how there’s two different ones? The one at the top, which has a &lt;div&gt; tag within it, is a link embedded into the photo of Tom Cruise. The other one is a link embedded into the text that shows his name (highlighted on the left-hand side of the screen).\nWe can see what page the link takes us to by looking at the href attribute of the &lt;a&gt; tag. You can imagine this to be like a sort of “class variable” for &lt;a&gt;. href actually shows what page the link will take us to. In our case, we see that, in both cases, we get taken to the same page: /person/500-tom-cruise/. Note that this is a subdirectory that we’d have to append onto our URL.\nThough both links take us to the same page, we’ll be using the second instance for two reasons. One, it contains less information within it (no images), and two, we can also grab the name of the actor. See how after the href link we simply see the text Tom Cruise? We can grab the actor name from there.\nNow, let’s see what we need from the actor page:\n\n\n\nImage 7\n\n\nSo, what do we see here? There’s a &lt;section&gt; of class full_wrapper_credits. This will contain the different movies that the actor has been cast in. Within full_wrapper_credits is a &lt;div&gt; of class credits_list. On each actor’s page, though, it doesn’t just show what movie’s they’ve acted in, but also, what movie’s they’ve produced or have been a non-acting crew member in. We see this in that within div.credits.list are different &lt;tables&gt; of class card credits. The one at the very top is unlabeled and contains the movies they’ve acted in, but if we see below, there are others with an &lt;h3&gt; (header 3) text entry right before, “Production” and “Crew”.\nThat means that we have to be careful! We don’t want to select a movie if our favorite actor didn’t actually play in it. Let’s look inside of the table now:\n\n\n\nImage 8\n\n\nSimilarly to the &lt;ol&gt;, the table is filled with instances of &lt;tr&gt;–table rows. Each &lt;tr&gt; contains, among other things, another table of class credit_group, if we open this even further, we get:\n\n\n\nImage 9\n\n\n&lt;td&gt; (table data) tags, which contains an &lt;a&gt; (remember, a link) to the movie in question, and within the link is the text that has the movie name which is inside of a &lt;bdi&gt;, which stands for bidirectional isolate. I’m not really sure what that means, but it stores text, and this is the text that we’re interested in order to get the movie name."
  },
  {
    "objectID": "posts/Homework 2/index.html#pseudocode",
    "href": "posts/Homework 2/index.html#pseudocode",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "“Pseudocode”",
    "text": "“Pseudocode”\nSo, let’s recap what we need our spider to do:\n\nGo to the cast page of our movie.\nIterate through the &lt;li&gt;’s of an &lt;ol&gt;.\nFor each &lt;li&gt;:\n\nStore the actor’s name from the &lt;a&gt; in each &lt;li&gt;.\nFollow the link to the actor’s page\nIsolate the correct &lt;table&gt; of class card credits\nIterate through the &lt;tr&gt;’s of the &lt;table&gt;\nFor each &lt;table&gt;\n\nOpen up the &lt;a&gt; in the credit group table.\nGrab the text stored in &lt;bdi&gt;\nReturn a dictionary containing the actor’s name and the movie name\n\n\n\nThat’s a lot of steps, but we’ll take them one at a time. Another rule of thumb (whose justification I’ll explain) is that, every time we follow a new link, we need to create a new method for our spider. So, we start on the movie page, go to the cast page, and then go to the actor’s page. Hence, we’ll need 3 different methods which correspond to the (currently blank) ones that you should have in your tmdb_spider.py file."
  },
  {
    "objectID": "posts/Homework 2/index.html#additional-tweaks",
    "href": "posts/Homework 2/index.html#additional-tweaks",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Additional Tweaks",
    "text": "Additional Tweaks\nThere are a few essential things that we should go over before sstarting to play around with our spider, even though we could technically get started right now.\n\nscrapy shell\nAs it stands, the only way we’re able to test our spider is by imputting commands into our terminal. This makes debugging quite difficult. In order to help with this, it’s recommended that you use the scrapy shell:\nhttps://docs.scrapy.org/en/latest/topics/shell.html\nThis should be easy to set up if you are working in an envrionment with IPython installed. All you need to do is go to the scrapy.cfg file (which is in the highest level directory within your project folder) and enter the following:\n[settings]\nshell = ipython\nWhen you start of the command prompt, this will actually let you simultaneously run scrapy commands while also using it as a Python console, meaning that you can write code.\n\n\nUser Agent\nWebsites don’t necessarily like bots on their pages collecting their data all of the time, especially because it can cause a lot of traffic to be directed to a site at once. To mitigate this, many websites have measures that attempt to remove spiders. For example, there is a document called robots.txt that contains information about what webscrapers can and can’t access. By default, there is a line in the settings.py file that displays the following:\nROBOTSTXT_OBEY = True\nIt’s important to obey robots.txt, so we’ll be keeping this as-is. However, when I first tried to run my scraper, it didn’t work because I kept getting 403 (access forbidden) errors because the site wasn’t happy about my spider. In order to get around this, we need to mimic a human being on the website. To do so, open settings.py and paste the following:\nUSER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.71 Safari/537.36'\nI’m not an expert on this, but essentially, the information in USER_AGENT is sent to the website when accessing it, and if we use the USER_AGENT argument to mimic a web browser (I found the above information online), then a website is much more likely to let us through. In my case (and with this website), this solves the issue. Stay in settings.py for the next section!\n\n\nPage Limit\nWhile debugging, we don’t want to run extremely long requests since they take time. It’s generally good practice to limit the number of pages that your spider can scrape while you’re still testing it out. In settings.py, paste the following line:\nCLOSESPIDER_PAGECOUNT = 20\nThis instructs the spider to terminate its scraping after having visited 20 pages, which is a decent number to start out with, even if (according to the implementation of our spider) we should be visiting more.\nOnce you’re ready to test your final version, make sure to remove this line!"
  },
  {
    "objectID": "posts/Homework 2/index.html#method-1-parse",
    "href": "posts/Homework 2/index.html#method-1-parse",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Method 1: parse",
    "text": "Method 1: parse\n\nStep 1: Go to the cast page of our movie.\nThe header of our function parse looks like the following:\nparse(self, response)\nThe response parameter means “whatever it is that our spider is looking at”, i.e., the current page. Thus, the completed version of parse should then be:\n\ndef parse(self, response):\n    \"\"\"\n    Initial parse method for class ```TMdbSpider``\n    Assumes valid ``subdir`` argument from initialization.\n    Sends spider to crawl ``cast_url`` (movie's cast page).\n    Yields `scrapy.Request` using `parse_full_credits\n    \"\"\"\n    cast_url = self.start_urls[0] + \"cast\"\n    yield scrapy.Request(cast_url, callback = self.parse_full_credits)\nThe string cast_url simply refers to the step of adding the cast to the end of our movie page’s URL in order to access the cast page. The next line is more complicated. Recall from PIC16A that, for generators, yield acts as a sort of partial return statement that lets the function continue running. When it comes to a scrapy spider, we use yield when we want to jump to another page. We do this by calling the function scrapy.Request, which, for our purposes, has two crucial arguments. One is the url (which we set to cast_url), and the other is called callback. Since yield effectively lets us ‘exit’ the function, we need to specify what method we’re using to crawl through the linked url, which will be the next method, parse_full_credits\nUpon completing this yield, our spider will then go to the cast page for the actor and access its HTML source code, just like we did when inspecting the page. This is why we need 3 different methods: each page has a different source code, so when we Request something from a new page, we’re looking at something new!"
  },
  {
    "objectID": "posts/Homework 2/index.html#method-2-parse_full_credits",
    "href": "posts/Homework 2/index.html#method-2-parse_full_credits",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Method 2: parse_full_credits",
    "text": "Method 2: parse_full_credits\n\nStep 2: Iterate through the &lt;li&gt;’s of an &lt;ol&gt;.\nOkay, so now that our spider is on the page cast_url, we need to tell it what to do on that page, and because of our callback argument, we’re going to need to write code for the parse_full_credits method.\nSo, remember that the first thing that we wanted our spider to do was to inspect the &lt;ol&gt; of class people credits. If you go to the page in question, we will see that these are properly kept as ol.people.credits. There’s one other caveat. We have to make sure that we’re only grabbing actors and not any other crew members. Luckily, the credits for crew members are stored in a class called ol.people.credits.crew, but if we were to just specify ol.people.credits in our code, scrapy will only check whether or not that tag/class is contained in the class that it’s inspecting, not if they’re identical. So, to navigate to just hte normal ol.people.credits, we need our spider to go to ol.people.credits:not(.crew). Furthermore, we don’t want our spider to grab every single piece of information within the ol, but rather, each of the lis. To specify that we’re looking for a tag witihn a tag, we put a space after our query and type the name of the tag we’re interested in. Hence, we want our spider to go to ol.people.credits:not(.crew) li, for each li. Finally, remember how there were 2 different links inside of each li, but we only wanted one of them? The one that wasn’t stored with the actor’s photo was in a &lt;div&gt; of class info. So, our final “query” is going to be ol.people.credits:not(.crew) li div.info\nAlright, how do we actually do this? This involves using the response parameter from earlier. Recall that response is the HTML source code. By using the .css method, we can essentially sort through the page’s CSS code to convert what’s on the page into something that tmdb_spider.py can process. Hence, for now, write the following:\ndef parse_full_credits(self, response): \n    actor_list = response.css(\"ol.people.credits:not(.crew) li div.info\")\nOne very nice thing about the way .css works is that our spider will automatically grab every instance of li that matches the above criteria. We don’t have to worry about any sort of iteration at this point. In fact, the return value, which we store as actor_list is an iterator that we can use with a for loop! Note that anything inside of .css must be a string.\n\n\nStep 3: For each &lt;li&gt;\n\nStore the actor’s name from the &lt;a&gt; in each &lt;li&gt;.\nRecall that the both the actor’s name and the link to their page is stored within each &lt;li&gt;. Thus, we can make our for loop as such:\ndef parse_full_credits(self, response): \n    actor_list = response.css(\"ol.people.credits:not(.crew) li div.info\")\n    for actor_li in actor_list:\n        actor_name = actor_li.css(\"a::text\").get()\nSince actor_list is an iterator, we can call its individual elements actor_li, which is still a piece of HTML source code that we’ve placed locally into our program. So, once again, we inspect the &lt;li&gt;’s CSS code by using .css. In particular, recall that links are stored in a tag called &lt;a&gt;, but &lt;a&gt;’s contain multiple parts that aren’t necessarily subtags. In these cases, we use the double colon operator :: to grab specific parts of the tag. The name of the actor, if you go back to the screenshot above, was not kept in any sort of specific attribute. Rather, it was just placed in the body of &lt;a&gt;. We can access it by calling a::text. To convert that text into a Python string, however, we need to use the .get() method, which we then assign to actor_name.\nOne tricky bit here, by the way, is that while response.css grabs every instance of the specified tag, .get() will only place the first instance of what’s found into a Python object. This doesn’t matter in this specific instance, but it’s important to know. You would have to use .getall() otherwise.\n\n\nFollow the link to the actor’s page\nThe completed for loop below is longer than you might be expecting:\ndef parse_full_credits(self, response): \n    \"\"\"\n    Second parse method for class ``TMdbSpider``\n    Called from `parse`. Assumes valid ``cast_url``\n    Fetches links to all actors' pages from cast page:\n    An ordered list `ol` of class `people credits` contains\n    `li` list elements with `div` elements of class `info`.\n    Each `li` stores the link to the actor's page and the\n    actor's name in an `a` tag.\n    Stores name of each actor in ``actor_name``.\n    Yields `scrapy.Request` using `self.parse_actor_page`. Passes ``actor_name``\n    \"\"\"\n    actor_list = response.css(\"ol.people.credits:not(.crew) li div.info\")\n    for actor_li in actor_list:\n        actor_link = actor_li.css(\"a::attr(href)\").get()\n        actor_name = actor_li.css(\"a::text\").get()\n        if actor_link:\n            actor_link = response.urljoin(actor_link)\n            yield scrapy.Request(actor_link, callback=self.parse_actor_page, meta = {\"actor\": actor_name})\nIn addition to grabbing the actor’s name, as we just did, we also grab the link and store it as a string in actor_link, once again using .get(). This time, href is stored as an attribute of &lt;a&gt; (note that it’s in pink when we inspect the page), so we use a::attr(href). If you remember from earlier, though, what’s stored inside of href is not the full URL, it’s just a subdirectory on https://www.themoviedb.org. We then have a conditional statement if actor_link. Recall that actor_link is a string. We want to make sure to stop here if we’ve gotten to the last actor on the page, so if there’s no one left, actor_link will simply be set to None. If we do have a link present, however, we can easily convert it to the full URL, however, by using the .urljoin method of response. This takes the root of the response webpage and simply adds whatever subdirectory is specified.\nSince we’re going to be navigating to another page, we’re going to have to call scrapy.Request again. The next URL is actor_link, our last method will be parse_actor_page, but we also pass in a dictionary called meta with key actor and value actor_name. Parse methods in scrapy have the ability to allow for optional arguments (**kwargs) which we need if we want to pass the actor’s name from this method to the next one. To find out about this, I read this page. Search for “meta”:\nhttps://docs.scrapy.org/en/latest/topics/request-response.html\nNote that we can’t just declare a new instance variable called actor_name or something like that. This ends up breaking the code."
  },
  {
    "objectID": "posts/Homework 2/index.html#method-3-parse_actor_page",
    "href": "posts/Homework 2/index.html#method-3-parse_actor_page",
    "title": "Getting Your Favorite Movies: Using Scrapy to Crawl through the Web",
    "section": "Method 3: parse_actor_page",
    "text": "Method 3: parse_actor_page\nNow we’re onto the last page, and the last method!\n\nStep 4: Isolate the correct &lt;table&gt; of class card credits\nJust like we did with the &lt;ol&gt; in the previous method, we’re going to use response.css to grab the card credits table that has the acting appearances and store it as an iterator that we will extract individual movies from. But remember what we said earlier? We only want to grab the first table of this type, so we’re going to be using a single colon with :first-of-type (thank you Professor Ko for helping me with this!). and grab all of the tooltip classes of &lt;a&gt;, which are precisely the actor names (see images from the previous part:\ndef parse_actor_page(self, response):\n    movie_list = response.css(\"div.credits_list table.card.credits:first-of-type a.tooltip\")\n…Except this doesn’t completely work. The problem (which I discovered when testing my submission for this assignment) is that there are some actors for whom the table that shows the movies in which they’ve acted is not the first. This seems to happen when someone is primarily known as a crew member but has also made appearances in movies–the works in which they’ve been a crew member appear at the top.\nThere is, however, a fix. Look at the image below:\n\n\n\nImage 10\n\n\nEach of the tables is preceded by a tag &lt;h3&gt; (which is a sort of header) that acts as a label. In our case, we want the table that follows the &lt;h3&gt; tag with text “Acting.” How do we do this? I wasn’t very sure because up until now, all we’ve had to do is filter within certain tags–I had to figure out how to access items based on surrounding tags.\nI’m not sure if this is possible using the .css method, but I did find that through xpath, this is possible via this StackOverflow post:\nhttps://stackoverflow.com/questions/65271036/python-scrapy-selector-for-text-elements-between-tags\nxpath is a language designed to parse through xml documents, which is similar to HTML. I’m not exactly how XML differs from HTML, nor do I know about how compatible they are. What I do know is that we can use it with Scrapy, and there is documentation about this on scrapy’s selector page:\nhttps://docs.scrapy.org/en/latest/topics/selectors.html#scrapy.selector.Selector\nI was able to get a grasp on how to use it from both the StackOverflow post, the site below, and the blog post below:\nhttps://www.w3schools.com/xml/xpath_syntax.asp https://www.guru99.com/using-contains-sbiling-ancestor-to-find-element-in-selenium.html\nThe command we need to input is the following:\nacting_table = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[@class='card credits'][1]\")\n movie_list = acting_table.css(\"a.tooltip\")\nresponse.xpath is used when we want to use XPath instead of HTML. The double slash // is similar to typing the name of a tag with .css. What follows is the name of the tag that we’re interested in, so in our case, we use h3. The brackets are used to place some sort of function inside. We use one called contains which is a sort of filtering function which, for our purposes, takes two arguments. The first lets us know what kind of item we’re looking for, in our case, text(), and the second is the thing itself we want, namely, Acting. So, once we wrap this up, we get all &lt;h3&gt;s such that they contain the word Acting.\nBut, remember, that’s not what we want. We want the table of class card credits that follows the table. This is where we really need xpath, because we get to use something called following-sibling. First, though, we have to put a single slash /. This is what’s used to select from a prior node (whereas the double slash is used to select everything corresponding to a specific tag). following-sibling can be used to select items that are siblings “on the same level,” in a certain sense. Notice how the &lt;table&gt;‘s are interlaced by the &lt;h3&gt;’s–one isn’t a subtag of the other, so they’re siblings! According to the syntax, we have to specify what kind of tag we want with the double colon operator, so ::table, and then we specify the class within brackets with @class = 'card credits'. We put a [1] at the end (xpath indexing sadly starts from 1) to specify that we only want the first of such tables (since otherwise we’d be grabbing all of them that following ’Acting’.\nThen I return back to CSS just to be more comfortable. We grab all of the tooltips in &lt;a&gt; to get the movie names.\n\nGrab the text stored in &lt;bdi&gt;\nNow, like we did earlier, we need to create a for loop. Recall that the name of the movie that the actor has starred in is stored in a &lt;bdi&gt;, so we once again need to use the double colon operator with text in order to select the name. Finally, we use the .get() method to store it as a string.\ndef parse_actor_page(self, response):\nacting_table = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[@class='card credits'][1]\") \n        movie_list = acting_table.css(\"a.tooltip\")\n        for movie in movie_list:\n            movie_or_TV_name = movie.css(\"bdi::text\").get()\n\n\nReturn a dictionary containing the actor’s name and the movie name\nThis is the last step! We’ll actually do this in the body of a yield statement, which will signal to scrapy that this is the end (no more requests after this). Of course, by virtue of the fact that we’ve been using yields (and not returns) this whole time allows us to continue iterating in our for loop (we do, after all, technically have a nested for loop split across two methods).\nSee the code below:\n\ndef parse_actor_page(self, response):\n    \"\"\"\n    Final parse method for class ``TMdbSpider``\n    Called from `parse_full_credits`. Assumes valid ``actor_link`` and ``actor_name``\n    Fetches the name of each movie that an actor has been in from their page. Each\n    movie's name is in the text component of a `bdi` element on the page. The `bdi`\n    element is in a link (`a`) element of class `tooltip`.\n    Yields the actor's name and each movie/TV show's name as a dictionary with keys\n    \"actor\" and \"movie_or_TV_name\"\n    Actor name  takenfrom ``actor`` key in dict ``meta``, extracted from\n    `parse_full credits`.\n    \"\"\"\n    acting_table = response.xpath(\"//h3[contains(text(), 'Acting')]/following-sibling::table[@class='card credits'][1]\") #Grabs the first table following 'Acting'\n        movie_list = acting_table.css(\"a.tooltip\")\n        #Get only the films/shows they've appeared in, and grab their link tooltips\n        for movie in movie_list:\n            movie_or_TV_name = movie.css(\"bdi::text\").get() #Get the name in &lt;bdi&gt; from each movie\n            yield {\n            \"actor\" : response.meta.get(\"actor\"), #Actor name passed into function\n            \"movie_or_TV_name\" : movie_or_TV_name #From above result\n                }              \nmovie_or_TV_name was stored for us in the last step, but recall that we passed the actor name into parse_actor_page using kwargs**. Per the documentation, we have to use the method .meta.get() on the dictionary key in order to get its corresponding value (recall that we had meta = {\"actor\" : actor_name}).\nAnd then we’re done! Well, with making the spider, anyway."
  },
  {
    "objectID": "posts/Homework 0/index.html",
    "href": "posts/Homework 0/index.html",
    "title": "Homework 0: Penguins",
    "section": "",
    "text": "Introduction\nHello! In this post, I’ll show you how we can make a nice visualization of the famous “Palmer Penguins” dataset by using the Pandas, Matplotlib, and Seaborn packages in Python. First we need to start by importing the packages that we need and obtaining the data, which we luckily have access to in a .csv file provided by professor Ko.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\n\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\npenguins.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 344 entries, 0 to 343\nData columns (total 17 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   studyName            344 non-null    object \n 1   Sample Number        344 non-null    int64  \n 2   Species              344 non-null    object \n 3   Region               344 non-null    object \n 4   Island               344 non-null    object \n 5   Stage                344 non-null    object \n 6   Individual ID        344 non-null    object \n 7   Clutch Completion    344 non-null    object \n 8   Date Egg             344 non-null    object \n 9   Culmen Length (mm)   342 non-null    float64\n 10  Culmen Depth (mm)    342 non-null    float64\n 11  Flipper Length (mm)  342 non-null    float64\n 12  Body Mass (g)        342 non-null    float64\n 13  Sex                  334 non-null    object \n 14  Delta 15 N (o/oo)    330 non-null    float64\n 15  Delta 13 C (o/oo)    331 non-null    float64\n 16  Comments             26 non-null     object \ndtypes: float64(6), int64(1), object(10)\nmemory usage: 45.8+ KB\n\n\nWe see that we have 15 different variables, some of them being quantiative and others categorical. The 16th column of the data frame is called Comments and simply houses additional information about the specific penguins.\nWe also see that not all entries are present for every single penguin. Let’s see why this is:\n\npenguins[penguins.iloc[:,:-1].isnull().any(axis=1)].head() #See next markdown block for explanation.\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n8\nPAL0708\n9\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN5A1\nYes\n11/9/07\n34.1\n18.1\n193.0\n3475.0\nNaN\nNaN\nNaN\nNo blood sample obtained.\n\n\n9\nPAL0708\n10\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN5A2\nYes\n11/9/07\n42.0\n20.2\n190.0\n4250.0\nNaN\n9.13362\n-25.09368\nNo blood sample obtained for sexing.\n\n\n10\nPAL0708\n11\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN6A1\nYes\n11/9/07\n37.8\n17.1\n186.0\n3300.0\nNaN\n8.63243\n-25.21315\nNo blood sample obtained for sexing.\n\n\n\n\n\n\n\n\npenguins[penguins.iloc[:,:-1].isnull().any(axis=1)]['Comments']\n\n0                         Not enough blood for isotopes.\n3                                     Adult not sampled.\n8                              No blood sample obtained.\n9                   No blood sample obtained for sexing.\n10                  No blood sample obtained for sexing.\n11                             No blood sample obtained.\n12                        Not enough blood for isotopes.\n13                        Not enough blood for isotopes.\n15                        Not enough blood for isotopes.\n39     Nest never observed with full clutch. Not enou...\n41                        Not enough blood for isotopes.\n46                        Not enough blood for isotopes.\n47     Sexing primers did not amplify. Not enough blo...\n212                                                  NaN\n246                                                  NaN\n250                                                  NaN\n286                                                  NaN\n324                                                  NaN\n339                                                  NaN\nName: Comments, dtype: object\n\n\nIn the above code, we first select all entries using iloc besides the last column (because this has comments). Then we run the .isnull() method which checks if a given entry is NaN (True or False). The .any(axis=1) method will simply change this to True if any of the entries in one row True as a result of the .isnull() method, and we use axis =1 to specify that we mean across columns for a given row.\nWe see that for many of these samples, not enough blood was drawn to obtain proper measurements. Let’s get rid of those entries that contain any null measurements.\n\npenguins2 = penguins[-penguins.iloc[:,:-1].isnull().any(axis=1)] #The minus sign negates the logical operation--that is, we only include entries where there are no null entries.\n\n\npenguins2[penguins2.iloc[:,:-1].isnull().any(axis=1)] #None left\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n\n\n\n\n\n\npenguins2.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n5\nPAL0708\n6\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A2\nYes\n11/16/07\n39.3\n20.6\n190.0\n3650.0\nMALE\n8.66496\n-25.29805\nNaN\n\n\n6\nPAL0708\n7\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN4A1\nNo\n11/15/07\n38.9\n17.8\n181.0\n3625.0\nFEMALE\n9.18718\n-25.21799\nNest never observed with full clutch.\n\n\n\n\n\n\n\nSo, what to do now? Many species exhibit what’s known as sexual dimorphism, wherein members of one sex have markedly distinct features from those of another. It wouldn’t necessarily be surprising to see, for example, females and males from one species to have a different average mass.\nHowever, it would be interesting to know if bodily proportions differ between sexes. For instance, males of one species might be larger and heavier than females, but if their bodies are of similar shape, then one would simply appear to be a smaller version of the other.\nTo this end, let’s compare the length of each penguin’s flipper to the length of it’s culmen. If you’re wondering what a culmen is, by the way…\n\n\n\nCredit to Allison Horst and Dr. Kristen Gorman on Twitter\n\n\nWe’ll start by eliminating any columns that we don’t want. For our purposes, we just want the Species (since proportions probably differ from one to the next), Stage (since at different points in their lifespan, penguins might have different shapes) Culmen Length (mm), Flipper Length (mm), and Sex.\n\npenguins3 = penguins2[['Species', 'Stage', 'Culmen Length (mm)', 'Flipper Length (mm)', 'Sex']] #Select the columns we want\npenguins3.head()\n\n\n\n\n\n\n\n\nSpecies\nStage\nCulmen Length (mm)\nFlipper Length (mm)\nSex\n\n\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\nAdult, 1 Egg Stage\n39.5\n186.0\nFEMALE\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\nAdult, 1 Egg Stage\n40.3\n195.0\nFEMALE\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\nAdult, 1 Egg Stage\n36.7\n193.0\nFEMALE\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\nAdult, 1 Egg Stage\n39.3\n190.0\nMALE\n\n\n6\nAdelie Penguin (Pygoscelis adeliae)\nAdult, 1 Egg Stage\n38.9\n181.0\nFEMALE\n\n\n\n\n\n\n\n\nset(penguins3['Stage']) #Coercing something into a sit displays its unique values.\n\n{'Adult, 1 Egg Stage'}\n\n\n\nset(penguins3['Species'])\n\n{'Adelie Penguin (Pygoscelis adeliae)',\n 'Chinstrap penguin (Pygoscelis antarctica)',\n 'Gentoo penguin (Pygoscelis papua)'}\n\n\nWe see that we have three distinct species of penguin, but only one stage (the 1 egg stage of adulthood), so we can eliminate this column.\n\npenguins3.columns\n\nIndex(['Species', 'Stage', 'Culmen Length (mm)', 'Flipper Length (mm)', 'Sex'], dtype='object')\n\n\n\npenguins3 = penguins3.drop('Stage', axis = 1)\n\nOkay, now we need to create a new column that contains the ratio of the flipper length to the culmen length.\n\npenguins3['Flipper-Culmen Ratio'] =  penguins3['Flipper Length (mm)'] / penguins3['Culmen Length (mm)']\n\n\npenguins3.head()\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nFlipper Length (mm)\nSex\nFlipper-Culmen Ratio\n\n\n\n\n1\nAdelie Penguin (Pygoscelis adeliae)\n39.5\n186.0\nFEMALE\n4.708861\n\n\n2\nAdelie Penguin (Pygoscelis adeliae)\n40.3\n195.0\nFEMALE\n4.838710\n\n\n4\nAdelie Penguin (Pygoscelis adeliae)\n36.7\n193.0\nFEMALE\n5.258856\n\n\n5\nAdelie Penguin (Pygoscelis adeliae)\n39.3\n190.0\nMALE\n4.834606\n\n\n6\nAdelie Penguin (Pygoscelis adeliae)\n38.9\n181.0\nFEMALE\n4.652956\n\n\n\n\n\n\n\n\n\nVisualization\nNow that we have all of our column names in order, let us visualize our data properly. We’ll start by looking at Adelie Penguins.\n\npenguinsAdelie = penguins3[penguins3['Species'] == 'Adelie Penguin (Pygoscelis adeliae)']\n\n\nadelie_male = penguinsAdelie[penguinsAdelie['Sex'] == 'MALE']\nadelie_female = penguinsAdelie[penguinsAdelie['Sex'] == 'FEMALE']\n\n\nadelie_male.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 68 entries, 5 to 151\nData columns (total 5 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   Species               68 non-null     object \n 1   Culmen Length (mm)    68 non-null     float64\n 2   Flipper Length (mm)   68 non-null     float64\n 3   Sex                   68 non-null     object \n 4   Flipper-Culmen Ratio  68 non-null     float64\ndtypes: float64(3), object(2)\nmemory usage: 3.2+ KB\n\n\n\nadelie_female.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 71 entries, 1 to 150\nData columns (total 5 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   Species               71 non-null     object \n 1   Culmen Length (mm)    71 non-null     float64\n 2   Flipper Length (mm)   71 non-null     float64\n 3   Sex                   71 non-null     object \n 4   Flipper-Culmen Ratio  71 non-null     float64\ndtypes: float64(3), object(2)\nmemory usage: 3.3+ KB\n\n\nFor this species, there is nearly an indetical number of members of either sex, which is important for our visualization.\nRecall from PIC16A that we can make an array of plots by using the subplots function from matplotlib. We’re going to first make an array of two empty plots, and then we’ll fill them with nearly identical histograms except for the fact that one contains the male data and the other the female data.\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) #Because we have a 1-d grid of plots, axes will have a 1-d index\naxes[0].hist(adelie_male['Flipper-Culmen Ratio'], bins=10, edgecolor='black', color='red') #10 bins is probably fine if we have ~70 observations\naxes[0].set_title('Male Flipper-Culmen Ratio') #axes[0] refers to the first (left-most) graph\naxes[0].set_xlabel('Ratio')\naxes[0].set_ylabel('Frequency')\n\n\naxes[1].hist(adelie_female['Flipper-Culmen Ratio'], bins=10, edgecolor='black', color='green')\naxes[1].set_title('Female Flipper-Culmen Ratio')\naxes[1].set_xlabel('Ratio')\naxes[1].set_ylabel('Frequency')\n\nText(0, 0.5, 'Frequency')\n\n\n\n\n\n\n\n\n\nWe see that there may be somewhat of a difference–on average, the ratio for male penguins is lower than that of female ones. We can compare them side by side by using a box-plot. Matplotlib is convenient because it lets us put box-plots side by side by creating lists of data and labels, respectively.\n\nplt.figure()\nboxplot = plt.boxplot([adelie_male['Flipper-Culmen Ratio'],adelie_female['Flipper-Culmen Ratio']], labels = ['Male','Female'])\nplt.xlabel('Sex')\nplt.ylabel('Flipper-Cumen Ratio')\n\nText(0, 0.5, 'Flipper-Cumen Ratio')\n\n\n\n\n\n\n\n\n\nIt indeed looks like that, for a given culmen length, females have longer flippers! We can look at the numbers more in detail by returning to our table and using the describe() method alongside groupby (which allows us to apply functions individually to certain groups, sex in our case).\n\nround(penguinsAdelie.groupby('Sex')['Flipper-Culmen Ratio'].describe()[['min','25%','50%','75%','max']],2)\n\n\n\n\n\n\n\n\nmin\n25%\n50%\n75%\nmax\n\n\nSex\n\n\n\n\n\n\n\n\n\nFEMALE\n4.27\n4.86\n5.09\n5.29\n5.86\n\n\nMALE\n4.19\n4.61\n4.75\n4.94\n5.72\n\n\n\n\n\n\n\nAlright, now let’s apply what we’ve learned to all of the species included in the data set. We’ll actually start with the table of summary statistics above, except this time, we can group by both species and sex to create 6 total combinations. We’ll also include count since we don’t know if we have roughly similar numbers of each type of penguin:\n\nround(penguins3.groupby(['Species','Sex'])['Flipper-Culmen Ratio'].describe()[['count','min','25%','50%','75%','max']],2)\n\n\n\n\n\n\n\n\n\ncount\nmin\n25%\n50%\n75%\nmax\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n71.0\n4.27\n4.86\n5.09\n5.29\n5.86\n\n\nMALE\n68.0\n4.19\n4.61\n4.75\n4.94\n5.72\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n34.0\n3.12\n4.04\n4.14\n4.26\n4.64\n\n\nMALE\n33.0\n3.63\n3.81\n3.91\n3.98\n4.33\n\n\nGentoo penguin (Pygoscelis papua)\n.\n1.0\n4.88\n4.88\n4.88\n4.88\n4.88\n\n\nFEMALE\n58.0\n4.19\n4.54\n4.66\n4.80\n5.23\n\n\nMALE\n60.0\n3.86\n4.37\n4.48\n4.60\n4.93\n\n\n\n\n\n\n\nWe see there’s one member of the ‘Chinstrap penguin’ species whose sex is unspecified. Let’s remove that individual.\n\npenguins3[(penguins3['Flipper-Culmen Ratio'] &gt; 4.875) & (penguins3['Flipper-Culmen Ratio'] &lt; 4.885)]\n\n\n\n\n\n\n\n\nSpecies\nCulmen Length (mm)\nFlipper Length (mm)\nSex\nFlipper-Culmen Ratio\n\n\n\n\n89\nAdelie Penguin (Pygoscelis adeliae)\n38.9\n190.0\nFEMALE\n4.884319\n\n\n123\nAdelie Penguin (Pygoscelis adeliae)\n41.4\n202.0\nMALE\n4.879227\n\n\n251\nGentoo penguin (Pygoscelis papua)\n42.8\n209.0\nFEMALE\n4.883178\n\n\n336\nGentoo penguin (Pygoscelis papua)\n44.5\n217.0\n.\n4.876404\n\n\n\n\n\n\n\n\npenguins3 = penguins3.drop(336)\n\n\npenguin_summary = round(penguins3.groupby(['Species','Sex'])['Flipper-Culmen Ratio'].describe()[['count','min','25%','50%','75%','max']],2)\npenguin_summary\n\n\n\n\n\n\n\n\n\ncount\nmin\n25%\n50%\n75%\nmax\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n71.0\n4.27\n4.86\n5.09\n5.29\n5.86\n\n\nMALE\n68.0\n4.19\n4.61\n4.75\n4.94\n5.72\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n34.0\n3.12\n4.04\n4.14\n4.26\n4.64\n\n\nMALE\n33.0\n3.63\n3.81\n3.91\n3.98\n4.33\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n58.0\n4.19\n4.54\n4.66\n4.80\n5.23\n\n\nMALE\n60.0\n3.86\n4.37\n4.48\n4.60\n4.93\n\n\n\n\n\n\n\nWe see that we have fewer Chinstrap penguins in comparison to the other two species. At a first glance, it seems like the ratio is higher for females than males in all penguin species. Let’s try making box plots!\n\nplt.boxplot(penguins3.groupby([['','Sex']])['Flipper-Culmen Ratio'])\n\nValueError: Grouper and axis must be same length\n\n\nUh oh! This doesn’t work. We’re going to have to import the seaborn package, which interacts very well with matplotlib. (Thanks professor for demonstrating sns with lineplots in class!). For the documentation for boxplots, see the link below, but the syntax is quite simple as we just need to specify different groups (x), our variable of interest (y), and one more differentiation in the color (hue, which we set equal to sex):\nhttps://seaborn.pydata.org/generated/seaborn.boxplot.html\n\nimport seaborn as sns\n\n\nplt.figure()\nsns.boxplot(x='Species', y='Flipper-Culmen Ratio', hue='Sex', data=penguins3)\n\nplt.xticks(rotation=45, ha='center', fontsize = 7) #Needed because otherwise the names go on top of each other.\nplt.title('Ratio of Flipper to Culmen Length of Male and Female Penguins')\nplt.show()\n\n\n\n\n\n\n\n\nIndeed, for all of the species, it seems as if, on average, females have a greater flipper length to culmen length ratio. Wouldn’t it be great, though, if we could place some of those summary statistics that we saw earlier onto the plot itself?\n\npenguin_summary\n\n\n\n\n\n\n\n\n\ncount\nmin\n25%\n50%\n75%\nmax\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n71.0\n4.27\n4.86\n5.09\n5.29\n5.86\n\n\nMALE\n68.0\n4.19\n4.61\n4.75\n4.94\n5.72\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n34.0\n3.12\n4.04\n4.14\n4.26\n4.64\n\n\nMALE\n33.0\n3.63\n3.81\n3.91\n3.98\n4.33\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n58.0\n4.19\n4.54\n4.66\n4.80\n5.23\n\n\nMALE\n60.0\n3.86\n4.37\n4.48\n4.60\n4.93\n\n\n\n\n\n\n\nWe can use the plt.text() function to help us. I didn’t learn about this in PIC16A, but the syntax is very simple (see below for documentation). We need an x coordinate, a y coordinate, the text itself, and a color. The y coordinate is easily visible on our graph, but the x isn’t necessarily. In our case, the boxplots are centered around 0, 1, and 2. Below, for instance, is a plot where we show the median value of the ratio for female Chinstrap penguins.\n\nplt.figure()\nsns.boxplot(x='Species', y='Flipper-Culmen Ratio', hue='Sex', data=penguins3)\nplt.text(.7,4.14,'4.14', color = 'red') #x coordinate, y coordinate, text, color\nplt.xticks(rotation=45, ha='center', fontsize = 7) #Needed because otherwise the names go on top of each other.\nplt.title('Ratio of Flipper to Culmen Length of Male and Female Penguins')\nplt.show()\n\n\n\n\n\n\n\n\nWe can thus use a loop to do this for every single summary statistic for every single box plot. Let’s start by making a modified version of the summary table without the count variable:\n\npenguin_summary_mod = penguin_summary.drop(\"count\", axis=1)\npenguin_summary_mod\n\n\n\n\n\n\n\n\n\nmin\n25%\n50%\n75%\nmax\n\n\nSpecies\nSex\n\n\n\n\n\n\n\n\n\nAdelie Penguin (Pygoscelis adeliae)\nFEMALE\n4.27\n4.86\n5.09\n5.29\n5.86\n\n\nMALE\n4.19\n4.61\n4.75\n4.94\n5.72\n\n\nChinstrap penguin (Pygoscelis antarctica)\nFEMALE\n3.12\n4.04\n4.14\n4.26\n4.64\n\n\nMALE\n3.63\n3.81\n3.91\n3.98\n4.33\n\n\nGentoo penguin (Pygoscelis papua)\nFEMALE\n4.19\n4.54\n4.66\n4.80\n5.23\n\n\nMALE\n3.86\n4.37\n4.48\n4.60\n4.93\n\n\n\n\n\n\n\nTo use a loop, we need to make use of the .iloc method provided to Pandas data frames, which allows us to select the at a specific point in a table. We see above that penguin_summary_mod has 6 rows (6 species-sex combinations) and 5 columns (5 summary statistics). The first coordinate in iloc is the row index and the second is the column index.\nWe need to more explicitly set x coordinates due to the positioning of the box plots on the graph. Both the text and the y coordinates should correspond to the value of the summary statistic.\n\n\nplt.figure() #This delimits the end of the last figure and the beginning of this one\n\nbox = sns.boxplot(x='Species', y='Flipper-Culmen Ratio', hue='Sex',width = .7, data=penguins3 ) #I set width to .7 to make the boxes slightly skinnier\nx_coords = [-.25,0.1,.75,1.1,1.75,2.1] #By experimenting, I was able to see that an offset of -.25 and +.1, respectively, centered the text.\nfor i in range(6): #Iterates through the 6 x coordinates\n    for j in range(5): #Iterates through the 5 columns of penguin_summary_mod\n        plt.text(x_coords[i],penguin_summary_mod.iloc[i,j]+.02,penguin_summary_mod.iloc[i,j], color = 'lightgreen', fontsize = 8) #I add .02 so that the text does not intersect the whiskers/edges.\nbox.set_facecolor('gray')\nplt.xticks(rotation=45, ha='center', fontsize = 7) #Needed because otherwise the names go on top of each other.\nplt.title('Ratio of Flipper to Culmen Length of Male and Female Penguins')\nplt.show()\n\n\n\n\n\n\n\n\nAfter playing around with the functions and methods for a bit, I was able to come up with this chart, which I think looks pretty nice.\n\n\nThanks for looking at my post :3\n\n\n\nPusheen in the Tub"
  },
  {
    "objectID": "posts/Homework 6/index.html",
    "href": "posts/Homework 6/index.html",
    "title": "Fake News Detector",
    "section": "",
    "text": "It can be very difficult to detect whether or not a news article is truthful in terms of the information that it provides. However, just like with everything else, there are often patterns (in this case, in language) that we cannot detect with the naked eye.\nLuckily, we can rely once again on the power of machine learning to try to find them for us. Just as we did with images, though, we’re going to have to do some preprocessing steps in order to make some data that our model can even interpret. After all, news articles have lots of words. How do you turn that into numbers? Let’s find out."
  },
  {
    "objectID": "posts/Homework 6/index.html#splitting-into-training-and-validation-data",
    "href": "posts/Homework 6/index.html#splitting-into-training-and-validation-data",
    "title": "Fake News Detector",
    "section": "Splitting into Training and Validation Data",
    "text": "Splitting into Training and Validation Data\nNow we just have to split the data into a training and validation set (test data will come later!).\n\ntrain_size = int(0.8*len(data))\nval_size   = int(0.2*len(data))\n\ntrain = data.take(train_size)\nval   = data.skip(train_size).take(val_size)\n\nNote that we can use special methods made for Dataset objects that allow us to easily do a train-test-split operation. The .take() method allows us to take a subset from data, and by using the .skip() method, we can then select a (hitherto unseen) val_size worth of data."
  },
  {
    "objectID": "posts/Homework 6/index.html#base-rate",
    "href": "posts/Homework 6/index.html#base-rate",
    "title": "Fake News Detector",
    "section": "Base Rate",
    "text": "Base Rate\nWith every classification model, there’s a certain score to beat: the base rate. This refers to finding whichever proportion of the data (among fake and not fake news articles) is higher, and then creating a dummy model that only picks that possibility. Anything worse than that is useless, and hopefully we’ll get much higher.\nAs demonstrated above, we can’t directly print out the items inside of data, but we can iterate over them! We’ll make a dictionary that counts the number of truthful and fake news articles and update its entries as we loop through:\n\nclass_dict = {\"not fake\":0, \"fake\":0}\nfor inputs, label in train:\n    if label['fake'][0] == 0:\n        class_dict[\"not fake\"] += 1\n    else:\n        class_dict[\"fake\"] += 1\n    \n\nNote that in the above for loop, we iterate over two distinct parts (inputs and label), but we’re only interested in label which is what contains the values for fake.\nOkay, let’s round up the total:\n\nclass_dict\n\n{'not fake': 8586, 'fake': 9373}\n\n\n\nclass_dict[\"fake\"]/(class_dict[\"not fake\"]+class_dict[\"fake\"])\n\n0.521911019544518\n\n\nThere are ever so slightly more fake news articles than real ones–52.19%, in fact. Let’s make sure our model does better than this!"
  },
  {
    "objectID": "posts/Homework 6/index.html#text-vectorization",
    "href": "posts/Homework 6/index.html#text-vectorization",
    "title": "Fake News Detector",
    "section": "Text Vectorization",
    "text": "Text Vectorization\nOkay, so before we can even make a model, there’s one last thng that we have to talk about. Namely, how are we going to get our text strings into numbers that we can plug into the model?\nWe’re going to have to do something called text vectorization, which is a way of encoding numbers into words. Here’s a tutorial from the tensorflow documentation that goes into detail: https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization\nThe basics though, are the following: 1. Make an ordered list of words that you’re interested in with some size n 2. Create an n dimensional integer vector for each text sample 3. Increment the corresponding entry of the vector every time a word from the list is found.\nSo, if my list of words was\ndog, cat, apple, house,...\nand my text was\n\"There was a dog and a cat in the house. The dog...\"\nWe would have the vector\n[2,1,0,1...]\nBecause the word “dog” appears twice, “cat” once, and so on.\nIn the case of a text vectorization implementation for our case, the list of words is going to be generated based on the most common ones that appear in our text.\nThe following code was provided by Professor Ko (with some edits on my part):\n\nimport keras\nprint(keras.__version__) #Making sure to use Keras 3\nfrom keras import layers\nfrom keras.layers import TextVectorization\nimport re\nimport string\nsize_vocabulary = 2000\n\ndef standardization(input_data):\n    no_punctuation = tf.strings.regex_replace(input_data,\n                                  '[%s]' % re.escape(string.punctuation),'')\n    return no_punctuation \n\ntitle_vectorize_layer = TextVectorization(\n    standardize=standardization,\n    max_tokens=size_vocabulary, \n    output_mode='int',\n    output_sequence_length=500) \n\ntitle_vectorize_layer.adapt(train.map(lambda x, y: x[\"title\"]))\n\ntext_vectorize_layer = title_vectorize_layer\ntext_vectorize_layer.adapt(train.map(lambda x, y: x[\"text\"]))\n\n3.1.1\n\n\nThere’s a lot to unpack here. Firstly, the function standardization simply removes all punctuation from any strings inside by making use of regular expressions (i.e., imagine like a search function that tries to find punctuation marks inside of a string). This function then returns a punctuation-less version of the data.\nThis function, though, isn’t actually called on its own. It’s an input to the function TextVectorization which is included in keras.layers. The function has an argument called standardize that essentially allows us to do some data cleaning beforehand. Above, we set size_vocabulary equal to 2000 which then becomes our value for the max_tokens argument–this ensures that we will only consider the 2000 most common words that appear in our headlines, which will make training the model easier. Note that there will be an item in the vector for words that fall out of the 2000 most common, so all gaps will be closed!\noutput_mode simply ensures that our vector entries are integers, and output_sequence_length relates to how many words in each string we’ll consider. In this case, only 500, which is probably enough for a news article.\nNote that the output of TextVectorization is a function that we can turn into a layer for our model. This is why we use the adapt method on title_vectorize_layer to specifically just grab the title out of a set of inputs (x) and outputs (y) from our model.\nOne thing that I added was also creating a text_vectorize_layer which is essentially identical but just does the same thing for text rather than title."
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "Here’s some text that I’ve placed underneath heading 1. Isn’t that great?\n\n\\[\\sin(x) = \\sum_{n=0}^{\\infty} \\frac{x^{2n+1}}{(2n+1)!}\\]\n\n\n\n\nTo do:\n\nEat dinner\nShower\n\n\n\n\nfor i in range(10):\n    print(i)"
  },
  {
    "objectID": "posts/bruin/index.html#heading-2",
    "href": "posts/bruin/index.html#heading-2",
    "title": "Creating posts",
    "section": "",
    "text": "for i in range(10):\n    print(i)"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Homework 1/index.html",
    "href": "posts/Homework 1/index.html",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "",
    "text": "In this post, I’m going to create some interactive tools that you can play with that really help you understand the degree to which global warming has affected temperatures across the Earth. And, best of all, I’ll teach you how to do it as well, assuming you have some basic Python knowledge. We’re going to be using a package called plotly that helps us easily make these graphics, but we’re also going to employ pandas and sqlite3.\nAcknowledgement: I refer to the Jupyter notebooks provided by Professor Seyoon Ko for PIC16B as they give clear instructions on how to create SQL databases from pandas data frames. I do my best to present these ideas in an original manner, but the setup is not my own work!"
  },
  {
    "objectID": "posts/Homework 1/index.html#part-1a-creating-the-database",
    "href": "posts/Homework 1/index.html#part-1a-creating-the-database",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Part 1a: Creating the Database",
    "text": "Part 1a: Creating the Database\nFor our purposes, databases exist as files in our working directory with the extension .db. Before we do anything, we can create a blank database file where we’ll be inputting our tables. Just think of the follwing command as a blank canvas:\n\nconn = sqlite3.connect(\"files/climate.db\")\n\nWhy is it called connect? Well, connect actually looks for an existing file called climate.db in this case, but if it doesn’t exist, it will create one, so we also use the command to make new databases.\nBut, regardless of whether or not the file already exists, connect also, as the name suggests, connects to the file name in its argument (climate.db) in our case. This is what allows us to establish a link between our Python notebook and our database, which is essential. There are many other programs that can allow you to write and execute SQL queries, but sqlite is special in that we can just use our existing Python IDE as an interface to use SQL, but we need to connect to some sort of datbase in order to do that.\nAlso, you might be wondering why we stored the output of this function as conn. Think of it as like a variable name for our database so that we can more easily use it in later function calls."
  },
  {
    "objectID": "posts/Homework 1/index.html#part-1b-preparing-the-tables",
    "href": "posts/Homework 1/index.html#part-1b-preparing-the-tables",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Part 1b: Preparing the Tables",
    "text": "Part 1b: Preparing the Tables\nOkay, so now we actually need to populate our database with tables. To do this, we’re going to employ a method that comes with pandas called .to_sql() that allows us to turn a data frame into a SQL table within a database of our choice. First, though, we actually need to get our hands on our data. There are 3 .csv files that we’re interested in: one called temps that contains temperature data, one called station-metadata.csv which contains information about the weather stations where the aforementioned data was collected, and finally, one which we will call countries, which will allow us to identify the country (or region) in which each station is located. The first two files come from NOAA but were provided to my PIC16B class by my professor, Seyoon Ko. I would link these files, but they are behind our BruinLearn page. The last file was also provided by Professor Ko but was taken from the URL that appears in the code block below. I’m not sure who to attribute it to.\n(Note that, by the way, although we’re doing this because we want to avoid using too many memory-intensive operations, we’re going to have to load these files at least once into our notebook so that we can turn them into SQL tables in the first place).\n\ntemperatures = pd.read_csv('files/temps.csv')\nstations = pd.read_csv('files/station-metadata.csv')\ncountries = pd.read_csv(\"https://raw.githubusercontent.com/mysociety/gaze/master/data/fips-10-4-to-iso-country-codes.csv\")\n#This may take a bit to run.\n\nWhile it’s tempting to immediately try to convert these 3 data frames into SQL tables, we first want to clean the data up a little bit. Let’s start with temps: (The following steps are taken exactly from Prof. Ko’s notebook)\n\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\nThere are a few major problems at the moment with this table, which you might’ve not known without prior information. Firstly, it’s unclear what the columns VALUE1 through VALUE12 refer to on their own, though you may be able to guess that they have to do with months. Indeed, they represent the average temperature taken at a given station in a given year during some month (VALUE1 being January and VALUE12 being December).\nThe units of the temperature, however, are actually in hundreths of a degree Celsius. This is not a unit that we’re used to working with.\nAnd finally, on a more qualitative side of things, it’s not the best idea to keep VALUE1 through VALUE12 as different columns. Why? Because, although they are presented as separate variables, they actually measure the same thing (namely, temperature in hundreths of a degree). In general, we should have it so that each column represents a distinct variable.\nSo, it seems that there are three problems that we need to solve:\n\nColumn names are ambiguous.\nUnits for data are not the best.\nData frame needs to be restructured.\n\nWe’ll start with the third item, since we shouldn’t worry about renaming things our changing units if we don’t even have the structure down in the first place. But how do we do this? In my previous blog post (HW 0), I briefly introduced the idea of stacking in pandas, and we’re going to be using that again this time.\nIf you don’t recall, here’s the basic idea of stacking: * Specify some column(s) that will be “preserved” * The .stack() method will combine all other columns into one big column Let’s give this a go:\n\ntemperatures = temperatures.set_index(keys = [\"ID\",\"Year\"])\ntemperatures = temperatures.stack()\ntemperatures.head()\n\nID           Year        \nACW00011604  1961  VALUE1     -89.0\n                   VALUE2     236.0\n                   VALUE3     472.0\n                   VALUE4     773.0\n                   VALUE5    1128.0\ndtype: float64\n\n\nOkay, so what did we do? First, we used the .set_index() method which allows us to turn a set of columns into the index of the data frame. This is important because these columns will not be stacked. In our case, we want to preserve the station ID and the year in which the measurement was taken.\nThen, the remaining columns are stacked (combine) into one “column” once we call .stack(). This actually turns the object into a pandas series. Recall from PIC16A that a series is like a generalization of a list that we can imagine as one column of a data frame. A series also has an index, which in our case, contains three different items for each numerical value (this technically makes it a MultiIndex): the ID and Year (which we already “preserved” when we called .set_index(), and any one of the 12 possible strings VALUE1 through VALUE12. This final item allows us to remember which column that got eliminated the observation originated from.\nBut, if we look at the index explicitly, we’ll notice an interesting detail:\n\ntemperatures.index\n\nMultiIndex([('ACW00011604', 1961,  'VALUE1'),\n            ('ACW00011604', 1961,  'VALUE2'),\n            ('ACW00011604', 1961,  'VALUE3'),\n            ('ACW00011604', 1961,  'VALUE4'),\n            ('ACW00011604', 1961,  'VALUE5'),\n            ('ACW00011604', 1961,  'VALUE6'),\n            ('ACW00011604', 1961,  'VALUE7'),\n            ('ACW00011604', 1961,  'VALUE8'),\n            ('ACW00011604', 1961,  'VALUE9'),\n            ('ACW00011604', 1961, 'VALUE10'),\n            ...\n            ('ZIXLT622116', 1970,  'VALUE3'),\n            ('ZIXLT622116', 1970,  'VALUE4'),\n            ('ZIXLT622116', 1970,  'VALUE5'),\n            ('ZIXLT622116', 1970,  'VALUE6'),\n            ('ZIXLT622116', 1970,  'VALUE7'),\n            ('ZIXLT622116', 1970,  'VALUE8'),\n            ('ZIXLT622116', 1970,  'VALUE9'),\n            ('ZIXLT622116', 1970, 'VALUE10'),\n            ('ZIXLT622116', 1970, 'VALUE11'),\n            ('ZIXLT622116', 1970, 'VALUE12')],\n           names=['ID', 'Year', None], length=13992662)\n\n\nWe see that each observation is explicitly linked to the three aforementioned pieces of information (ID, year, VALUE column). It’s as if that our series was really a data frame in disguise, where the first 3 columns are the items in the index (ID, year, and VALUE column name) while the last column is the actual “data” of the series (the temperature recordings).\nWell, we can actually force this data frame to resurge by calling the .reset_index() method on temperatures, which eliminates the index that we manually specified above and replaces it with the default one (just numbers), but this would turn the current MultiIndex into “data columns” which is not possible in a Series, so temperatures will revert to being a data frame. The nicest and most important part, however, is because the VALUE column name is now in our index, it will be added as a new column within our data frame.\n\ntemperatures = temperatures.reset_index()\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nlevel_2\n0\n\n\n\n\n0\nACW00011604\n1961\nVALUE1\n-89.0\n\n\n1\nACW00011604\n1961\nVALUE2\n236.0\n\n\n2\nACW00011604\n1961\nVALUE3\n472.0\n\n\n3\nACW00011604\n1961\nVALUE4\n773.0\n\n\n4\nACW00011604\n1961\nVALUE5\n1128.0\n\n\n\n\n\n\n\nThe names of the columns ID and Year are preserved since they were in the index of our series. Our new VALUE name column is given a default name level_2, and the temperature column is called 0 since our series data didn’t have any name associated with it. From here, though, we can fix the other two problems we had earlier: the naming of our columns and the units of our recordings!\nFirst, let’s rename (using the .rename() method) our columns to something that we can interpret more easily. Note that the string that we have for VALUE precisely corresponds to the month in which the observation was made, so we name this column Month.\nNote that the columns argument of .rename() is a dictionary where the keys are the existing column names and the values are the names that we wish to give to the columns.\n\ntemperatures = temperatures.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"}) #Input as a dictionary.\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\nVALUE1\n-89.0\n\n\n1\nACW00011604\n1961\nVALUE2\n236.0\n\n\n2\nACW00011604\n1961\nVALUE3\n472.0\n\n\n3\nACW00011604\n1961\nVALUE4\n773.0\n\n\n4\nACW00011604\n1961\nVALUE5\n1128.0\n\n\n\n\n\n\n\nNow, we don’t want the values of the Month column to be have the VALUE string beforehand. It would make a lot more sense to just store the number at the very end. We also need to convert it to an integer (since right now it’s merely a string).\nWe’ll also divide the Temp column by 100.\n\ntemperatures[\"Month\"] = temperatures[\"Month\"].str[5:].astype(int) \ntemperatures[\"Temp\"]  = temperatures[\"Temp\"] / 100 \n\n\ntemperatures.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nACW00011604\n1961\n1\n-0.89\n\n\n1\nACW00011604\n1961\n2\n2.36\n\n\n2\nACW00011604\n1961\n3\n4.72\n\n\n3\nACW00011604\n1961\n4\n7.73\n\n\n4\nACW00011604\n1961\n5\n11.28\n\n\n\n\n\n\n\nLooks pretty good to me! Let’s look at our other tables.\n\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\n\ncountries.head()\n\n\n\n\n\n\n\n\nFIPS 10-4\nISO 3166\nName\n\n\n\n\n0\nAF\nAF\nAfghanistan\n\n\n1\nAX\n-\nAkrotiri\n\n\n2\nAL\nAL\nAlbania\n\n\n3\nAG\nDZ\nAlgeria\n\n\n4\nAQ\nAS\nAmerican Samoa\n\n\n\n\n\n\n\n\nprint(\"Number of regions in `stations`:\", len(set(stations['ID'].str[0:2])))\nprint(\"Number of regions in `countries`:\", len(countries['FIPS 10-4']))\n\nNumber of regions in `stations`: 237\nNumber of regions in `countries`: 279\n\n\nI will not be deleting the extra rows in countries because it may be possible for some of the countries that do not appear in stations to be represented in updated versions of the data, and we would not be able to access the information that we wanted correctly if, for instance, we were basing our countries table based on what was included in January of 2024."
  },
  {
    "objectID": "posts/Homework 1/index.html#c-conversion-to-sql-tables",
    "href": "posts/Homework 1/index.html#c-conversion-to-sql-tables",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "1c: Conversion to SQL Tables",
    "text": "1c: Conversion to SQL Tables\nWe’re almost ready to place our data into our empty climate.db file that we created earlier. But, before we do that, there’s one more thing we need to keep in mind: memory usage. Recall that we will be using the .to_sql() method to turn our data frames into tables. This essentially requires a large transfer of data which might be dangerous or annoying if we have very big files. We can measure the memory used by our current frames with the memory_usage() method (note that the results are an approximation).\nSee the documentation in the link below for more details. The argument deep = True more correctly considers the memory taken up by string. By default, the function outputs results by column, so .sum() will tell us the total memory usage in bytes.\nDocumentation: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.memory_usage.html).\n\nprint(\"Memory used by `countries`: \", round(countries.memory_usage(deep = True).sum()/1024,2), \"kilobytes\")\nprint(\"Memory used by `stations`: \", round(stations.memory_usage(deep = True).sum()/1024,2), \"kilobytes\")\nprint(\"Memory used by `temperatures`: \", round(temperatures.memory_usage(deep = True).sum()/1024,2), \"kilobytes\")\n\nMemory used by `countries`:  50.62 kilobytes\nMemory used by `stations`:  4340.02 kilobytes\nMemory used by `temps`:  1257153.36 kilobytes\n\n\ncountries is relatively small, only taking up about 50.6 kilobytes. stations, however, seems to take up more than 4 megabytes. temperatures, though is massive!\n\nprint(\"Memory used by `temperatures`: \", round(temperatures.memory_usage(deep = True).sum()/(1024**2),2), \"megabytes\")\n\nMemory used by `temps`:  1227.69 megabytes\n\n\nThis is more than a gigabyte of data! This is really big, and it would probably be unwise to directly turn it into a SQL table. Let’s, then, start with countries and stations first, and we’ll save the biggest for last.\n\ncursor = conn.cursor()\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\n27585\n\n\nAssuming everything’s gone well, climate.db now contains our table!\nLet’s break down the above command. The first argument specifies the name we want to give to our table, which will still be stations. The second argument is the database that we want to place it in (remember what I said earlier about conn representing climate.db?). The argument if_exists tells us what to do if we happen to find a table in our database that’s already called stations. Since our database is empty, we won’t find any such tables, but by setting the argument to replace, we will overwrite any files with the same name. index = False indicates that we do not want to transfer over the index of our data frame into the table.\nThe numerical output of our method call, by the way, is the number of rows in our table\nLet’s do the same thing with countries.\n\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n\n279\n\n\nNow, let’s move onto temperatures. How are we going to transfer more than a gigabyte of data? The answer is by doing so bit by bit. Specifically, we can iteratively transfer pieces of our data frame called chunks into our database. In the original Python notebook from class, our professor created a for loop that took advantage of an argument inside of read_csv() called chunksize that created an iterator out of our original data divided into a set number of pieces.\nIn that for loop, our professor used a function he created that would clean the data frame as we did above before creating the iterator. So, even though we’ve already cleaned our data above, let’s walk through the steps one more time so that we can implement this for our iterator:\n\nUse .set_index() and .stack() to place all temperature measurements in one column (series).\nConvert this series back into a data frame with .reset_index()\nEdit and rename the Month and Temp columns.\n\nLet’s make a function that does this for a data frame. The idea will then be to apply this function onto different chunks of the original data before finally combining them into a table in our database.\n\n#This function is taken directly from Professor Ko's notebook.\n#I rename it to `prepare_temps` to make its usage explicit:\n\ndef prepare_temps(df):\n    \"\"\"\n    Cleans a chunk ``df`` of the `temps.csv` file based on the criteria elaborated\n    earlier. Note that this function implicitly assumes that its argument comes\n    from `temps.csv`. It will otherwise likely throw an error since it relies\n    on the existence of certain column names.\n    \"\"\"\n    df = df.set_index(keys=[\"ID\", \"Year\"]) #\"Row names\"\n    df = df.stack() #Everything else (temperature names) gets stacked\n    df = df.reset_index() #Converts MultiIndex into columns\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"}) #Renames new columns\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int) #Converts `Month` to integer\n    df[\"Temp\"]  = df[\"Temp\"] / 100 #Convert to degrees Celsius\n    return(df)\n\nNow, let’s make our iterator. We’ll be using a chunksize of 100000 just like Professor Ko, which splits our data frame into pieces of about 19 MB.\n\ntemps_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\n\nOur general idea is to go through each chunk of the data and then clean it. Then, once the chunk is cleaned, we’ll add it to the database in a table called temps. Note that because of this iterative method, the if_exists arugment of .to_sql() becomes relevant, since after we input the first chunk, a table called temps already exists. We’ll make sure to use the other possible value of if_exists, 'append', to add onto our table. The code chunk below is also taken from Professor Ko.\n\nfor i, temps_chunk in enumerate(temps_iter):\n    df = prepare_temps(temps_chunk)\n    df.to_sql(\"temperatures\", conn,\n              if_exists = \"replace\" if i == 0 else \"append\", index = False)\n\nFirstly, to properly iterate through temps_iter, we should be using the enumerate() function. If you recall from PIC16A, this essentially creates the idea of an index and a value (almost like a dictionary) for our iterator. The “value”, in this case, is a pandas DataFrame object that has (initially) 100000 rows. This is why we’re able to then input temps_chunk into the prepare_temps() function we wrote earlier.\nAs for if_exists, we will only replace an existing table called temperatures (which should not exist) during the first iteration. For all other chunks, we’ll append to the table that is created during the first iteration.\nAs good practice, we should close the connection that we made between this notebook and climate.db, since now that our file has all the information we need, we only need to access it while we’re running queries or making edits!\n\nconn.close()"
  },
  {
    "objectID": "posts/Homework 1/index.html#summary-of-part-1",
    "href": "posts/Homework 1/index.html#summary-of-part-1",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Summary of Part 1:",
    "text": "Summary of Part 1:\nWe’ve now created a SQL database called climate.db that contains 3 different tables: temperatures, stations, and countries. In order to do this, we had to use sqlite to link said database with our notebook, and we then had to use the .to_sql() method in order to convert the data that we had loaded as pandas data frames into tables.\nTo make countries, we used a for loop with an iterator that allowed us to add chunks of the data to our table so as to not have to transfer a large amount of data at once."
  },
  {
    "objectID": "posts/Homework 1/index.html#part-3a-explanation-of-data-frame-filtering",
    "href": "posts/Homework 1/index.html#part-3a-explanation-of-data-frame-filtering",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Part 3a: Explanation of Data Frame Filtering",
    "text": "Part 3a: Explanation of Data Frame Filtering\nIgnore the **kwargs in the header of our function–we’ll use that later to make edits to our plot. The first thing that we do in our function’s body is call the query_climate_database function according to the inputs given by the user. Then, using this data frame, we make a new column to our resulting data frame queried_data called Name_Count which records the number of times each station appears in the query. We do this by combining the .groupby() and .transform() methods. I explained .groupby() in my previous post, but .transform() is new. It allows us to apply a function as specified in its input (in this case, count) to each group. Thus, .transform('count') counts the number of observation in each NAME group. What’s special about the function .transform, however, is that its results will be placed as a new column within our data frame. For instance, if the station PBO_ANANTAPUR appears 15 different times in query_climate_database, each row pertaining to that station will have a value of 15 in Name_Count–even though the number of groups is much smaller than the total number of rows, .transform() allows us to preserve our data frame’s structure while still adding group-level information!\nThe next line creates a new data frame called filtered_data that is essentially identical to queried_data, except we drop all rows where Name_Count is less than min_obs. Notice that because we were able to get count displayed for every row, the rows whose NAME appears precisely fewer than min_obs times will be deleted. We also drop the ‘Name_Count’ column since it will no longer be useful for us."
  },
  {
    "objectID": "posts/Homework 1/index.html#part-3b-explanation-of-regression-coefficient-collection",
    "href": "posts/Homework 1/index.html#part-3b-explanation-of-regression-coefficient-collection",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Part 3b: Explanation of Regression Coefficient Collection",
    "text": "Part 3b: Explanation of Regression Coefficient Collection\nThe next step is by far the most complicated. Essentially, we are aiming to group each station by name and run a linear regression of Temp on Year and extract the slope of the line. To do this, we are going to use another method that allows us to use a function on part of a data frame. This time, though, we’re going to instead use a method called .apply() because .transform() only works with one column at a time, whereas .apply() let us work with multiple columns (something we need for a regression!). See this StackOverflow post for more details: https://stackoverflow.com/questions/27517425/whether-to-use-apply-vs-transform-on-a-group-object-to-subtract-two-columns-and.\nThere is no function (at least that I know of) that returns the slope coefficient of a linear regression automatically, so we have to write one ourselves. This is why there is a function called coef above my function definition, which I took from Professor Ko with a few slight modifications. For a given data_group (the chunks we get out of .groupby()), we store the Year column in a data frame x and the Temps column in a series y. The reason x is a data frame is because, if you recall from PIC16A, data matrices are automatically assumed to be data frames since they can have multiple dimensions. We then create a new regression called LR, fit it on x and y, and return the slope of the line rounded to 4 digits (this should also be review from PIC16A).\nWhen we use apply on our data frame, the output of the function will, in this case, be a pandas Series where the index is the NAME of the station and the values are the coefficient correlation. Note that unlike with .transform, there will only be one entry per group, so we cannot simply append the column. Instead, we first convert the series into a data frame by converting the index into its own column with reset_index(), and then we give the column containing our coefficients an appropriate name (Estimated Yearly Increase (°C)).\nThen, we use pd.merge to combine our filtered query data with the coefficient data. The column that we’re looking for in common is NAME, and then we execute a left join, which allows us to keep rows in our filtered query data even if, somehow, it wasn’t possible to run coef. pd.merge checks each row of filtered_data, looks at its entry in NAME, finds the corresponding entry in coef_frame, and appends the values of coef_frame onto the row of filtered_data. This allows us to add this information onto filtered_data even though coef_frame has much fewer rows. We call the result merged_frame."
  },
  {
    "objectID": "posts/Homework 1/index.html#part-3c-explanation-of-graphing",
    "href": "posts/Homework 1/index.html#part-3c-explanation-of-graphing",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Part 3c: Explanation of Graphing",
    "text": "Part 3c: Explanation of Graphing\nNow that we have our data prepared, we can actually create our plot, which will rely on express from plotly (which we imported as px). express allows us to create many different types of plots very easily, akin to ggplot2 from R if you have any experience with that. Before actually making the plot, though, I’ve prepared its title beforehand. It is once again an f-string which allows us to specifically state the country, beginning year, and ending year for which the data was collected. Since the column Month is a number, however, we create a dictionary called month_dict that maps each numerical value of Month with its word equivalent, which we then place into the title.\nThe actual command to create the plot is px.scatter_mapbox. scatter means that we’re making a scatterplot, and mapbox means that, instead of placing our points on, say, an x-y grid, we’re instead placing them on a map of the Earth (which is still an x-y grid but with latitude and longitude). The first argument is the name of the data frame that we’re interested in using (merged_frame), and the following arguments rely on the columns of merged_frame: lat and lon are the latitude and longitude of our points, respectively, which we were able to take from stations. Because plotly graphs are interactive, we’re allowed to see information when we hover our cursor over individual points, which is why we set hover_name = 'NAME', which will let us know the station name, and the most important part, the color, is controlled by 'Estimated Yearly Increase (°C)'.\nThose are actually all the necessary arguments of px.scatter_mapbox to get our plot working, but we can include more. color_continuous_scale lets us pick what sort of color scheme we want for our points. zoom controls how far zoomed in we are on the map when we call our function, color_continuous_midpoint allows us to set a ‘neutral’ value for our color scale at a certain value (0 in this case, since temperatures can either be going up or down over time), opacity controls how transparent the points are, and finally, title controls, of course, the title. Most of these arguments are of the form kwargs.get(value, default). This is related to the **kwargs in the function header: when the function is called, any arguments in addition to those required will be placed into a dictionary called kwargs whose keys are the argument name and values are the value inputted for that variable. kwargs.get allows us to access that dictionary and input the value for that variable within our function. The second value, however, is simply the default that will be used if no additional input was specified by the user. For example, we have opacity = kwargs.get('opacity',0.2), which will set the opacity value to whatever the user specified, and otherwise, will set it to 0.2.\nFinally, we run two more commands once we actually create our graph that alter it (via update_layout): one changes something called mapbox_style, and the other simply changes the dimensions of the plot. As for the first command, when we create our plot, you can almost think of the map as just a picture that we lay points on top of depending on some coordinates. As such, we can change the type of map in the background to whatever we want, provided that the coordinates are aligned correctly. By default, we will use one called open-street-map.\nHere’s a demonstration for India between 1980 and 2020 for the month of January:\n\ncolor_map = px.colors.diverging.RdGy_r # choose a colormap\n\nfig = temperature_coefficient_plot('climate.db',\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nYou can zoom into the map and see for yourself how temperatures have changed over the years. By hovering over the individual points, you can also see the name of each station, the latitude, longitude, and the estimated yearly temperature increase. Note that while we only specified hover_name = \"NAME\", by default, information about the x and y coordinates (i.e., the latitude and longitude) as well as the variable used in color will also be displayed.\nIf you come across this notebook, feel free to play around with the inputs of the function. You will find that, all across the world, temperatures have been increasing at an alarming rate. Here’s another example of Mexico in September:\n\ntemperature_coefficient_plot('climate.db', 'Mexico', 1980, 2020, 9, min_obs = 10)"
  },
  {
    "objectID": "posts/Homework 1/index.html#part-4a-changes-in-temperature-ranges",
    "href": "posts/Homework 1/index.html#part-4a-changes-in-temperature-ranges",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Part 4a: Changes in Temperature Ranges",
    "text": "Part 4a: Changes in Temperature Ranges\nOne of the consequences of global warming that people sometimes forget is that it doesn’t necessarily make the temperature of every single part of the earth hotter at the same time, even if average temperatures are increasing. Rather, climate change can also increase the impact of extreme weather events.\nAs such, it would be interesting to see how the yearly range in temperature has changed over time; perhaps temperatures used to be pretty constantly mild throughout the entire year, but now, they jump to highs in the summer and lows in the winter.\nTo find out how these have changed over the time, we’re going to create a new SQL querying function called min_max_query and incorporate it into a function called min_max_plot. min_max_plot will be a series of histograms that will display the range of temperature for all stations within a given country. First, I’ll show a screenshot of a sample plot, and then we’ll dive deeper into the code:\n\n\n\nMin Max Image\n\n\nI’ve zoomed the picture out so that we can see the whole thing, but there are a few things that we’d like to keep in mind:\n\nThe user should be able to specify the range of time during which we track the changes.\nThe user should also be able to specify how many graphs they would like to see.\nAdditional information (such as the number of observations and the median range) should also be displayed for ease of viewing.\n\nTherefore, our SQL query should include sufficient information about our country of interest, the date range, and the temperatures. While this wasn’t covered in class, SQL also has MAX() and MIN() functions (which I learned about for my internship, so we can get this part of the work cut out for us in the SQL query. Below I show the code for min_max_query:\n\nfrom climate_database import min_max_query\nimport inspect\nprint(inspect.getsource(min_max_query))\n\ndef min_max_query(db_file, country, year_begin, year_end):\n    \"\"\"\n    This function uses `sqlite3` to query a .db database ``db_file`` for temperature data in ``country`` between\n    the years of ``year_begin`` and ``year_end`` (inclusive).\n    The output is a `pandas` data frame ``df`` which contains the following columns:\n    ``NAME``: The name of the weather station where the temperature was measured.\n     ``Year``: The year during which the measurement was taken, will be between ``year_begin`` and ``year_end``, inclusive.\n     ``temp_max``: The maximum temperature during a specific year at a given station.\n     ``temp_min``: The minimum temperature during a specific year at a given station.\n    \"\"\"\n    with sqlite3.connect(f\"{db_file}\") as conn:\n\n        cmd = \\\n        f\"\"\"\n        SELECT t2.NAME, t2.Year, MAX(Temp) AS temp_max, MIN(Temp) AS temp_min FROM\n        (SELECT S.NAME, t1.Name AS Country, t1.Year, t1.Temp\n        FROM stations S\n        INNER JOIN\n        (SELECT T.ID, T.Year, T.Temp, C.Name\n        FROM temperatures T\n        LEFT JOIN countries C ON SUBSTRING(T.ID,1,2) = C.\"FIPS 10-4\"\n        WHERE C.Name = \"{country}\" AND T.Year BETWEEN {year_begin} AND {year_end})\n        AS t1 ON S.ID = t1.ID) as t2\n        GROUP BY NAME, Year\n        \"\"\"\n\n        df = pd.read_sql_query(cmd, conn)\n\n        return df\n\n\n\nThe basic structure is identical to that of query_climate_database, so let’s just focus on what exactly is going on in the query itself. We see once again that we use subquerying, but here, we have two subqueries. The innermost one joins the temperatures and countries tables, almost identically to the subqery of query_climate_database, with less restrictive WHERE conditions since we don’t need to restrict to a certain month (that would defeat the purpose of getting a yearly range). The output of this first query is called t1. The second subquery then joins t1 with stations, giving us the station name, and joining where the IDs are equivalent (just as in query_climate_database). This second result is saved as t2.\nFinally, from t2 we grab NAME, Year, as well as MAX(TEMP) and MIN(TEMP) (which have slightly prettier names). This does not grab the maximum and minimum temperatures for all of t2, however. Just like with pandas data frames, we can use GROUP BY statements in SQL, but they go at the very end of a query. Here, we group by NAME, Year, which means that we group by every possible name-year combination (i.e., grouping two-dimensionally). This allows us to get the maximum and minimum temperatures for a specific station in a specific year, separate from all other max/min temperatures for that same station in a different year.\nNow we need to make the actual plotting function. Skip to the code block below the function body to see the explanation. Once again, I define some helper functions that we will with transform as well.\n\ndef min_max_plot(db_file, country, year_begin, year_end, graph_num):\n    \"\"\"\n    This function creates a series of ``graph_num`` stacked histograms displaying\n    the range of temperatures measured at stations in ``country`` between\n    ``year_begin`` and ``year_end``, inclusive. It obtains the data using\n    `sqlite3` via the `min_max_query` function to query a `.db` database file \n    ``db_file`` to obtain the maximum and minimum temperatures at weather stations.\n    The function manipulates the resulting data frame to calculate the temperature\n    range at all stations for all valid years. It then calculates the years for\n    which graphs will be shown via linear interpolation between ``year_begin`` and\n    ``year_end``, with non-integer years being rounded to the nearest integer.\n    The plot, made using `plotly express` displays the histograms along with a\n    dotted line showing the median temperature range and with annotations\n    indicating the median value as well as the number of observations in the given year.\n    \"\"\"\n    queried_data = min_max_query(db_file, country, year_begin, year_end) #Run the query\n    \n    queried_data[\"Hi-Low Difference\"] = queried_data[\"temp_max\"] - queried_data[\"temp_min\"] #Calculate the differnece\n    \n    slope = (year_end-year_begin)/(graph_num -1)\n    intercept = year_begin - slope\n    year_list = []\n    for i in range(graph_num):\n        year_list.append(round(intercept+slope*(i+1),0))\n        #Obtain the years required for the graphs\n\n    \n    filtered_query = queried_data[queried_data['Year'].isin(year_list)].sort_values('Year')\n    #Filter to specified years\n    \n    title_string = f\"Distribution of Yearly Temperature Ranges (°C) for Stations in {country} between {year_begin} and {year_end} (Median in Red)\"\n    \n    fig = px.histogram(filtered_query, x='Hi-Low Difference', opacity = 0.5, nbins = 25,\n                       facet_row = \"Year\", title = title_string) #Create graph\n    fig.update_yaxes(title_text='Occurrences') #Change facet titles\n    \n    median_values = filtered_query.groupby('Year')['Hi-Low Difference'].median()[::-1]\n    counts = list(filtered_query.groupby('Year').count()['NAME'])[::-1]\n    y_cord = np.median(counts) #Collect medians and observation numbers\n\n    \n    for row_num, median_value in enumerate(median_values): #(Over all facets)\n        fig.add_vline(x=median_value, line_dash='dash', line_color='red',\n                      col=1, row=row_num+1) #Add line signaling median\n        fig.add_annotation(xref='paper', yref='paper', x=0.5, y=y_cord/8, \n                       text=f\"(n = {counts[row_num]})\", showarrow=False,\n                       col=1, row=row_num+1) #Add number of observations\n        fig.add_annotation(xref='paper', yref='paper', x=0.5, y=y_cord/4, \n                       text=f\"Median: {round(median_value,2)}\", showarrow=False,\n                       col=1, row=row_num+1, font = {'color':'red'}) #Add median value\n    fig.update_layout(height = 1000)\n    fig.update_traces(marker_line_width=1, marker_line_color='black') #Add border to graph\n    fig.show()\n    return filtered_query #Also return data frame\n\nThere’s a lot of code, so let’s digest it bit by bit.\n\nPart 4a(i): Data Frame Manipulation\nOur first order of business is to create a new column, which we name Hi-Low Difference that contains the difference between the highest and lowest temperatures recorded in a given year. We then create variables called slope and intercept which will help us figure out what years we want to include in our final plot; remember, we want to include graph_num graphs between year_begin and year_end, so we only need to keep data corresponding to so many years.\nI decided to make this in the form of a linear function to make it more easily visualized. Suppose, for instance, that year_begin = 1980, year_end = 2014, and graph_num = 6. If we can imagine graph 1 (input) corresponding to 1980 (output) and graph 6 to 2014, the slope of the line connecting \\((1,1980)\\) to \\((6,2014)\\) is precisely \\(\\frac{2014-1980}{6-1} = \\frac{{end}-{begin}}{num-1}\\). So, if graph 1 corresponds to 1980, we need that the output for 1 is 1980, so we have \\(1980 = slope\\cdot(1)+intercept \\Rightarrow intercept = 1980-slope \\Rightarrow intercept = begin - slope\\)\nThen, we have a list called year_list which will contain the list of years that we need. We create a for loop that iterates over the number of necessary graphs and obtains the output of the linear equation above for different numbered graphs. We then round the output to the nearest whole number. This is because there is no guarantee that we will be able to fit a whole number of graphs between year_begin and year_end. The result of this, of course, is that the graphs will not be completely evenly spaced, but in that case, we would simply have no choice.\nWe then make a new data frame called filtered_query that selects only the rows such that the year is included within year_list. For this, we use the .isin() method, and furthermore, I sort my data frame using sort_values by year because, otherwise, the faceted graphs do not end up in chronological order.\n\n\nPart 4a(ii): Plot Creation\nWe once again create a title string and then call a px function, this time histogram, which creates a bar graph that shows the frequencies of a certain variable. The variable in question, Hi-Low Differnece is placed along the horizontal axis, which is why we place it into the argument x (if you use y, you can have a histogram that sticks out horizontally!). I specify that the number of bins should be 25 (that looked good while testing). Then, I introduce facet_row = year. Recall that a facet in this context can be thought of like a subplot from matplotlib. Essentially, we are duplicating a graph with the same characteristics, except for the fact that it should be limited to a certain year. Note that by specifying the argument as facet_row and not facet_column, the histograms are stacked vertically.\n\n\nPart 4a(iii): Plot Customization\nI had to look up the documentation for a lot of the functions and methods we use in this part since we didn’t look too deeply into facets and histograms in lecture. The first thing we do is call a method called update_yaxes to change the name of the y-axis label for the histogram. The default label is “count”. We then create a list of the medians and the occurrences for each histogram by taking filtered_query, grouping by year, and then using the .median() and .count() (should be review from PIC16A). We also create a variable called y_cord and set it to the median of counts, (see below).\nThen we use a for loop with enumerate on median_values to iterate over all facets with an index i. From here, we use two different methods on fig: add_vline and add_annotation, similar to functions from ggplot2()in R. .add_vline places a vertical line in a graph of a specified shape (dashed in our case) and color (red) at a specific x coordiante, which I place exactly at the median, since the point of the dotted line is to see where the median is. Notice that arguments row and col are also included, which correspond to the “coordinates” of the facet. This indexing starts from 1, not 0. Also, even though there is only one column of graphs, the code will not compile unless a row is specified.\nadd_annotation is used for placing text on top of the plot. The first two arguments, xref and yref control the text’s position. By setting it to paper, the subsequent inputs for x and y will be based on their positioning within the space of the graph . See the documentation here for more information: https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html\nThe x values of both pieces of text are set to 0.5 to keep it towards the left-hand side. The y values, however, depend on y_cord (the median of counts). This is because the relative position of the text to the height of the histogram bars depends on the number of per-country observations, so this always keeps the text around the same place. text contains the raw text itself–the number of observations and the median, respectively. In both cases, showArrow is set to False. Were this to be instead True, there would be a small black line on the plot pointing to the text. We once again specify row and column to align with our facets, and finally, we set the color of our median text to be red. This has to be done through an argument called font, which is a dictionary. I didn’t look up what all the other possible keys are, but I imagine that the color is just one of them.\nThen, I change the height using update_layout (so they are not squished), and finally, .update_traces allows us to put borders on the edges of our histogram bars. The program finishes by showing the plot and also by returning filtered_query as a data frame, in case the user wants to inspect the source of the data further.\nSo, how how has the yearly temperature range changed with time? Here are a few examples from a few different countries:\n\nmin_max_1 = min_max_plot(\"climate.db\", \"India\", 1960, 2020, 6)\n\n\n\n\n\nmin_max_2 = min_max_plot(\"climate.db\", \"United States\", 1960, 2020, 6)\n\n\n\n\n\nmin_max_3 = min_max_plot(\"climate.db\", \"Russia\", 1960, 2020, 6)\n\n\n\n\n\nmin_max_4 = min_max_plot(\"climate.db\", \"Canada\", 1960, 2020, 6)\n\n\n\n\nFrom these graphs alone, it’s hard to say whether or not there has been a significant shift in the median difference between high temperatures and low temperatures in a year in the above countries, but what all of them have in common is that the measurements in the past couple of decades seem to have lower medians than the earlier ones, which might actually suggest that temperatures are converging rather than diverging!"
  },
  {
    "objectID": "posts/Homework 1/index.html#part-4b-altitude-and-temperature-changes",
    "href": "posts/Homework 1/index.html#part-4b-altitude-and-temperature-changes",
    "title": "Global Warming: Spread, Change, and Resistance",
    "section": "Part 4b: Altitude and Temperature Changes",
    "text": "Part 4b: Altitude and Temperature Changes\nOne of the variables that we have access to in the stations table that we haven’t used up until now is STNELEV: the elevation (in feet, I compared with some of the actual places) of each weather station. It is important to understand what makes certain communities more susceptible or resistant to global warming, and one factor that may be at play is elevation. From what we can guess, it would make sense that higher areas have lower temperatures, so let’s ask a more interesting question: does the year-to-year volatility in temperatures depend on elevation? How would temperatures change from one year to the next at sea level? At 1000 feet? At 2000?\nThat’s what I want to find out, and to do this, we’re going to make one more function that allows us to visualize a plot, in particular, a scatter plot, and in order to be able to gain a conclusion from a scatter plot, we also want to see a regression output. Here’s what a final version should look like:\n\n\n\nElevation Graph\n\n\nOnce again, we’re going to have to write a new query function, which we’ll call elevation_query, especially because we’re using a variable that we haven’t used before. Then, we’ll write a function called elevation_temp_plotter which calls the query function. Here are the broad steps that we’re going to need:\n\nWrite and call the query function based on user input specifying a certain low and high height at which to compare temperature data.\nManipulate the data frame so that we see the absolute difference between subsequent years’ temperatures.\nStore the data for the regression between the temperature differences and elevation deviation so that we can add it onto our plot.\nPlot a faceted scatterplot using plotly express\nAdd the regression line.\nAdd any other necessary features.\n\nLet’s start with the query function:\n\nfrom climate_database import elevation_query\nimport inspect\nprint(inspect.getsource(elevation_query))\n\ndef elevation_query(db_file, month, year_begin, year_end, elev_below_center, elev_above_center, radius):\n    \"\"\"\n    This function uses `sqlite3` to query a `.db` database file ``db_file`` and returns a data frame ``df``\n    containing the following columns:\n    ``NAME``: The name of valid weather stations. \n    ``STNELEV``: Each station's elevation in feet. Must be within ``radius`` feet of ``elev_below_center`` or ``elev_above_center``\n    ``LATITUDE``: Each station's latitude.\n    ``LONGITUDE``: Each station's longitude.\n    ``Country``: The name of the country where the recoridng was taken.\n    ``Year``: The year when the recording was taken, between ``year_begin`` and ``year_end``, inclusive.\n    ``Month``: The month when the recording was taken, equal to ``month``.\n    ``Temp``: The temperature recording at that station at that point in time.\n    \"\"\"\n    with sqlite3.connect(f\"{db_file}\") as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.STNELEV, S.LATITUDE, S.LONGITUDE, t1.Name AS Country, t1.Year, t1.Month, t1.Temp\n        FROM stations S\n        INNER JOIN\n        (SELECT T.ID, T.Year, T.Month, T.Temp, C.Name\n        FROM temperatures T\n        LEFT JOIN countries C ON SUBSTRING(T.ID,1,2) = C.\"FIPS 10-4\"\n        WHERE T.Year BETWEEN {year_begin} AND {year_end} AND T.Month = {month})\n        AS t1 ON S.ID = t1.ID\n        WHERE ((S.STNELEV BETWEEN {elev_above_center-radius} AND {elev_above_center+radius}) OR (S.STNELEV BETWEEN {elev_below_center-radius} AND {elev_below_center+radius})) AND S.STNELEV != 9999\n        \"\"\"\n        df = pd.read_sql_query(cmd, conn)\n        \n        return df\n\n\n\nBy now, you should be getting the hang of SQL queries. Our inner subquery joins temps onto countries in accordance to the range year_begin and year_end, the result of which we call t1. Then, we join stations onto t1 using the ID column as we had before, but this time, we have a very long WHERE condition that takes advantage of f-strings once again: the elevation must be within radius feet of either elev_above_center or elev_below_center. These two user-inputted parameters determine the elevation at which we center our scatterplot. Of course, we also want to be able to control how far from this center our observations can be–we don’t want any overlap! That’s why we use the radius variable.\nOkay, now let’s move onto elevation_temp_plotter itself. This time, we’re going to be relying on 3 different helper functions, so make sure to scroll down far enough!\n\ndef altitude_check(group, elev_below_center, radius):\n    \"\"\"\n    This is a helper function to be used with `elevation_temp_plotter` and simply\n    categorizes stations as being either 'Low' or 'High' depending on their\n    closeness to ``elev_below_center`` or ``elev_above_center``\n    \"\"\"\n    if (np.mean(group) &gt;= elev_below_center - radius) and (np.mean(group) &lt;= elev_below_center + radius):\n        return 'Low'\n    else:\n        return 'High'\n\n\ndef relative_height_difference(x, elev_below_center, elev_above_center):\n    \"\"\"\n    This is a helper function to be used with `elevation_temp_plotter` and\n    calculates the distance between each station's\n    elevation ``STNELEV`` and either `elev_below_center` or `elev_above_center`\n    \"\"\"\n    if x['Relative Height'] == 'Low':\n        return x['STNELEV'] - elev_below_center\n    else:\n        return x['STNELEV'] - elev_above_center\n\n\ndef coef_elev(data_group):\n    \"\"\"\n    This is a helper function to be used with `elevation_temp_plotter` and returns\n    a tuple containing `numpy` arrays that will be used for plotting regression\n    lines and also the regression intercept/coefficients for printing onto plots.\n    \"\"\"\n    x = data_group[[\"Distance from Center\"]] # 2 brackets because X should be a df no matter what\n    y = data_group[\"Diff from Last\"]   # 1 bracket because y should be a series\n    LR = LinearRegression()\n    LR.fit(x, y)\n    reg_info = np.array([LR.intercept_, LR.coef_[0]])\n    x_vals = np.linspace(data_group['Distance from Center'].min(),\n                         data_group['Distance from Center'].max(),100)\n    y_vals = LR.predict(x_vals.reshape(-1,1))\n    return (np.column_stack((x_vals, y_vals)),reg_info)\n\n\ndef elevation_temp_plotter(db_file,month,year_begin,year_end,elev_below_center,elev_above_center, radius):\n    \"\"\"\n     This function creates two scatterplots in ``plotly express`` displaying the\n     yearly change in temperatures during ``month`` at all weather stations whose\n     elevations are within ``radius`` feet of ``elev_below_center`` and \n     ``elev_above_center`` in ``country`` between ``year_begin`` and ``year_end``,\n     inclusive. It obtains the data using `sqlite3` via the ``elevation_query``\n     function to query a `.db` database file ``db_file`` to obtain the relevant\n     temperature recordings. The function manipulates the resulting data frame to\n     calculate the absolute value change in temperatures from one year to the next.\n    The function creates faceted scatterplots, each displaying data centered around\n    ``elev_below_center`` and ``elev_above_center`` The horizontal axis does not\n    display the altitude, but rather, distance from ``elev_below_center`` and\n    ``elev_above_center``, respectively. The OLS regression line for each\n    scatterplot is also displayed with the equation printed as an annotation for\n    each one. The function returns the data frame used for the plot for reference.\n\n    This function relies on functions ``altitude_check``, ``relative_height_difference``,\n    and ``coef_elev`` to manipulate the data frame and obtain regression\n    information. It requires the `numpy` package to be installed.\n    \"\"\"\n    \n    \n    queried_data = elevation_query(db_file,month, year_begin-1, year_end,\n                                   elev_below_center, elev_above_center, radius)\n    queried_data['Diff from Last'] = queried_data.groupby(['NAME', 'LATITUDE'])['Temp'].diff().dropna().abs()\n    #Query and difference data.\n    \n    queried_data['Relative Height'] = queried_data.groupby(['NAME','LATITUDE'])[['STNELEV']].transform(altitude_check,\n                                                                               elev_below_center, radius)\n    queried_data['Distance from Center'] = queried_data.apply(relative_height_difference,\n                          args = (elev_below_center, elev_above_center), axis = 1)\n    #Calculate distance from elev_below_center or elev_above_center  \n    coef_info = queried_data.dropna().groupby('Relative Height').apply(coef_elev) #Create necessary arrays for regression and obtain coefficient info.\n\n    month_dict = {1:'January', 2:'February', 3:'March', 4:'April', 5:'May', 6:'June', 7:'July', 8:'August', 9:'September', 10:'October', 11:'November', 12:'December'} #For month names\n    title_string = f\"Absolute Difference From Prev. Year’s {month_dict[month]} Temperatures (°C) for Stations at Elevations Within {elev_below_center}±{radius} and {elev_above_center}±{radius} Feet\"\n    #Create string for title\n    \n    fig = px.scatter(queried_data, x= 'Distance from Center', y = 'Diff from Last',\n                     hover_name = 'NAME', hover_data = ['Country','LATITUDE','LONGITUDE'],\n                     color_discrete_sequence=['red'], facet_row = 'Relative Height', \n                     title = title_string) #Create faceted scatterplot.\n\n    altitude_dict = {2: ['Low',elev_below_center], 1:['High',elev_above_center]}\n    #For labels in each facet\n    fig.update_layout(height = 1000)\n    fig.layout.annotations[1]['text'] = f\"Center = {altitude_dict[2][1]} (Low)\" \n    #Change the label that indicates the facet.\n    fig.layout.annotations[0]['text'] = f\"Center = {altitude_dict[1][1]} (High)\"\n\n    for key, value in altitude_dict.items(): #Add regression line and text to each plot.\n        fig.add_trace(px.line(x=coef_info[value[0]][0][:,0],\n                              y = coef_info[value[0]][0][:,1]).data[0],\n                      row = key, col = 1)\n        fig.add_annotation(xref='paper', yref='paper', x=-radius/2,\n           y=1.1*(queried_data[queried_data['Relative Height'] == value[0]])['Diff from Last'].max(),\n           text=f\"y={round(coef_info[value[0]][1][0],4)}{'+' if (coef_info[value[0]][1][1] &gt; 0) else ' '}{round(coef_info[value[0]][1][1],4)}*x\", showarrow=False,\n               col=1,row = key)\n        \n    fig.show()\n    return queried_data #Returns the data frame used as well.\n\nThere’s a lot going on here. Let’s take it step by step.\n\nPart 4b(i): Data Frame Manipulation\nThe first thing we do is to run our query using elevation_query using the variables entered through the function call, with the notable exception being the initial year is year_begin -1. This is because we will be differencing our data, and as such, the earliest year’s observation for each station will become an NaN value since it has nothing to be compared against, so we go one year back in order to obtain a valid value.\nThen, we make a column called Diff From Last which uses the .diff() method from pandas to calculate the difference in temperature between one year and the next at a given station. We take the absolute value using .abs() since we’re interested in overall volatility, and we make sure to drop any NaNs to eliminate the recordings for the year year_begin -1. Note that in this case, we group by both NAME and LATITUDE because we are not filtering our data by country. In fact, there are several stations that happen to have the same name but in different countries! Furthermore, we can’t group by country either since there are some stations that aren’t located in any country (such as in the ocean) and therefore, their country label is called None (there was some trial and error in figuring this out :)).\nNext, we create a column called Relative Height which employs .transform() to apply a function called altitude_check onto the STNELEV column, grouped by station. Note that because altitude_check has multiple arguments, we can actually pass these into .transform() itself as additional arguments. altitude_check inspects station group and checks whether or not the mean of the group’s height (this should be the same as each individual STNELEV value since the station isn’t moving) is within radius feet of elev_below_center. If it is, it sets the value of Relative Height to 'Low', and otherwise, the station must be within radius feet of elev_above_center, so we assign these rows 'High'.\nWe then use Relative Height via .apply() to create a column called Distance from Center. Once again, we use .apply() here and not .transform() because we need information from multiple columns. Here we once again do a simple logical check: if Relative Height is 'Low', then we simply take the station’s elevation and subtract elev_below_center, which will give us the signed difference in elevation between the station and the center. If all works well, Distance form Center should be within -radius and radius.\n\n\nPart 4b(ii): Regression Information\nWe then create an object (a tuple, actually) called coef_info that is not stored in queried_data, but rather, is kept separate. We use .apply again on a function called coef_elev so that we can have a customizable output, where our two groups are the 'Low' and 'High' values of Relative Height. We do it this way so that we can make a different regression for each group, which will translate nicely onto our faceted plot.\ncoef_elev is the most complex helper function that we’ve made for this post, but at its core, it’s quite similar to the coef function that we used for the first data visualization that we made. We’re going to want to be comparing the Distance from Center column to Diff from Last so that we can see how a one unit change in elevation (centered at either elev_below_center or elev_above_center) is related to the volatility in temperatures. We create a linear regression object LR and fit it to our data matrices, and in order to actually make something that we can graph, recall how we can use np.linspace() and .predict() to create x and y values that we can plot.\nThe function returns a single numpy array containing both the x and y values as well as the intercept/coefficient information of the regression, which we’ll use when created annotations on our graph to write our equation.\n\n\nPart 4b(iii): Plotting and Initial Changes\nNow comes the actual plotting. First we create a dictionary month_dict which, as it did in our original plot, will allow us to display month names in title_string, which is an adequately labeled f-string.\nThen we call px.scatter where we use queried_data as our source and our relevant x and y values. We also make it so that the user can hover over individual observations and see, in large text, the name of the station (hover_name = 'NAME') and also see secondary information about the country, latitude, and longitude of the station (hover_data = ['Country', 'LATITUDE', 'LONGITUDE']). We make the points all red using color_discrete_sequence (which allows us to select a discrete set of colors rather than using a spectrum) because the regression line is going to be, by default, blue. We make the facet rows Relative Height, which will separate the points into a 'Low' and 'High' scatterplot, and finally, we add our title.\nThen, once the graph is created, we first increase the height, and then we create a dictionary called altitude_dict whose keys are 2 and 1, and whose values are a list containing the words 'Low' or 'High' and elev_below_center or elev_above_center. It will become apparent why we do this when we apply facet specific changes. But first, we call fig.layout.annotations which allows us to change the label on the right-hand side that indicates what detemrines our facets. The 0th annotation will be for the High graph (which is on the bottom), and we clarify where the center referenced in ‘Distance from Center’ (i.e., elev_below_center or elev_above_center) is, and likewise, the 1st will be for the Low graph (which is on top). The reason I do not do this in the body of the for loop below is because editing layout.annotations seems to affect all annotations on the graph, so I ordered it in this particular way because, otherwise, the equations that we print on our graph would also be affected by this.\n\n\nPart 4b(iv): Facet-Specific Changes\nThe last large step is to make a for loop which will help us add the regression line and equation information onto the graph. Once again, I had to look up the documentation for this as while I had done something similar in R’s ggplot2, I had no idea how to do this for plotly. See this link and search for add_trace for a similar example: https://plotly.com/python/creating-and-updating-figures/\nI create a for loop over the dictionary that we created earlier which will allow us to simultaneously keep an index for the facet rows (key) as well as pertinent information (value). Recall that altitude_dict is the following:\naltitude_dict = {2: ['Low',elev_below_center], 1:['High',elev_above_center]}\nAlright. We start by using .add_trace on our figure, which essentially allows us to print other objects on top of our existing graph (geoms in ggplot2 if you’re familiar), and the trace that we want to add is a line, which I found out that you can create using px.line (https://plotly.com/python-api-reference/generated/plotly.express.line). The syntax is pretty simple (see row above), though. For the x values, we look in coef_info. It’s worth inspecting the output of coef_info first to see why this actually works:\n\ndb_file = 'climate.db'\nmonth = 3\nyear_begin = 2015\nyear_end = 2020\nelev_below_center = 0\nelev_above_center = 2000\nradius = 200\nqueried_data = elevation_query(db_file,month, year_begin, year_end,\n                               elev_below_center, elev_above_center, radius)\nqueried_data['Diff from Last'] = queried_data.groupby(['NAME', 'LATITUDE'])['Temp'].diff().dropna().abs() #Query and difference data.  \nqueried_data['Relative Height'] = queried_data.groupby(['NAME','LATITUDE'])[['STNELEV']].transform(altitude_check, elev_below_center, radius)\nqueried_data['Distance from Center'] = queried_data.apply(relative_height_difference,\n  args = (elev_below_center, elev_above_center), axis = 1)  #Calculate distance from elev_below_center or elev_above_center  \ncoef_info = queried_data.dropna().groupby('Relative Height').apply(coef_elev)\nprint(coef_info, coef_info['High'][1])\n\nRelative Height\nHigh    ([[-198.5999999999999, 1.8781317057391695], [-...\nLow     ([[-59.1, 1.4440271474376656], [-56.4828282828...\ndtype: object [ 1.87803968e+00 -4.63365829e-07]\n\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n\n\nWe see that the output of coef_info is sorted into the two groups High and Low, which is why we’re able to acccess the information regarding individual regressions using value[0] as it returns either of those exact keywords. Furthermore, because of the structure of our numpy array stored in coef_info, we’re capable of extracting the x and y coordinates using the standard subsetting operator. This is all we need for px.line, but before being able to add it as a trace, we need to append .data[0] to the end of px.line because otherwise, we’re actually going to be trying to grab the entirety of the line object and pass it to add_trace, when we only need the values contained within it. Finally, we can specify our row with key and column as 1 (recall that both are required even for one-dimensional faceting). Note that we use values 2 and 1 for key because we start by plotting the Low values on top (row 2) and then the High values below (row 1). This indexing does not start at zero! (This also took a lot of trial and error :( ).\nOnce we’ve added the line, though, all that’s left is the equation as text. We once again set xref and yref to paper as in the case of the histsograms, and we set x equal to -radius/2, which should ideally keep it at the 25th percentile of the chart. The height will have to depend on the distribution of the points–if some are extremely high up, there’s a chance that they might block the text. As such, we find the maximum value of Diff from Last for our particular group and add an extra 10 percent for good measure.\nThe text for the annotation looks complicated, but it’s mostly syntax that’s getting in the way. Our equation will be of the form \\(y = b_1+b_2x\\). \\(b_1\\) is simply the intercept term which we can find within the second item (1st index) of coef_info for our particular group as the item in the 0th index (hence the [1][0]), which is then rounded to 4 decimal places for ease of viewing. Adding the slope is a bit tricky because we don’t want to put a plus sign if the slope is a negative number since the minus sign will be printed by default, so we use Python’s ternary operator to print a plus sign if and only if the slope term (the item with the 1st index in the coefficient information part of coef_info), otherwise, we just leave it blank and let the minus sign of the coefficient take care of it. After this, we add the coefficient itself multiplied by x. Once again, we set showArrow = False to avoid creating any distractions for our labels, and we make sure to specify the right column and row using key.\nAnd then we’re done! We show the figure and also return queried_data if the user is interested in seeing the data up close.\nSo, what can we conclude from this? Let’s look at data from 1980 till 2020 for a few different months, and looking at 200 feet within sea level and 2000 feet:\n\nplot_1 = elevation_temp_plotter('climate.db', 4, 1980, 2020, 0, 2000, 200)\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n\n\n\n\n\n\nplot_2 = elevation_temp_plotter('climate.db', 8, 1980, 2020, 0, 2000, 200)\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n\n\n\n\n\n\nplot_3 = elevation_temp_plotter('climate.db', 8, 1980, 2020, 0, 2000, 200)\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n/Users/ziongassner/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning:\n\nX does not have valid feature names, but LinearRegression was fitted with feature names\n\n\n\n\n\n\nIn all three cases, the interesting part is that the y-intercept term increase between the Low and High altitudes, but the slope decreases. This suggests that elevation might actually have a positive relationship with temperature volatility (since the intercept term for the second graph encapsulates the effect of the first ~1800 feet and because the slope starts out positive), but the effect of higher altitudes on volatility may diminish, as evidenced by the lower slopes in the second example."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "haiiiiiiiii :3",
    "section": "",
    "text": "Hello! If you’re reading this page, that means that you’ve ended up at my Quarto Blog hosted on Github Pages for PIC16B with Professor Seyoon Ko!\n\nThis is not my dog (but I wish it was)."
  },
  {
    "objectID": "posts/Homework 5/index.html",
    "href": "posts/Homework 5/index.html",
    "title": "(put a cool title here)",
    "section": "",
    "text": "pip install keras\n\nRequirement already satisfied: keras in /Users/ziongassner/anaconda3/envs/PIC16B-24W/lib/python3.11/site-packages (2.15.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport os\nimport keras\nimport tensorflow_datasets as tfds\nfrom tensorflow import data as tf_data\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\n/Users/ziongassner/anaconda3/envs/PIC16B-24W/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\ntrain_ds, validation_ds, test_ds= tfds.load(\n    \"cats_vs_dogs\",\n    # 40% for training, 10% for validation, and 10% for test (the rest unused)\n    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n    as_supervised=True,  # Include labels\n)\n\nprint(f\"Number of training samples: {train_ds.cardinality()}\")\nprint(f\"Number of validation samples: {validation_ds.cardinality()}\")\nprint(f\"Number of test samples: {test_ds.cardinality()}\")\n\nNumber of training samples: 9305\nNumber of validation samples: 2326\nNumber of test samples: 2326\n\n\n\nresize_fn = keras.layers.Resizing(150, 150)\ntrain_ds = train_ds.map(lambda x, y: (resize_fn(x), y))\nvalidation_ds = validation_ds.map(lambda x, y: (resize_fn(x), y))\ntest_ds = test_ds.map(lambda x, y: (resize_fn(x), y))\nbatch_size = 64\ntrain_ds = train_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\nvalidation_ds = validation_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\ntest_ds = test_ds.batch(batch_size).prefetch(tf_data.AUTOTUNE).cache()\n\n\ntake_ran = np.random.randint(1, 291)\nplt.figure(figsize=(10, 10))\nim_num = 0\ncounter = 0\nfor images, labels in train_ds.take(take_ran):\n    counter +=1\n    if im_num &lt; 3:\n        if labels[counter].numpy() == 0:\n            ax = plt.subplot(3, 3, im_num + 1)\n            plt.imshow(images[counter].numpy().astype(\"uint8\"))\n            plt.title(\"cat\")\n            im_num +=1\n        plt.axis(\"off\")\n    elif im_num &lt; 6:\n        if labels[counter].numpy() == 1:\n            ax = plt.subplot(3, 3, im_num + 1)\n            plt.imshow(images[counter].numpy().astype(\"uint8\"))\n            plt.title(\"dog\")\n            im_num +=1\n        plt.axis(\"off\")\n    else:\n        break\n\n2024-03-02 19:54:42.040204: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n\n\n\n\n\n\n\n\n\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\nclassification_dict = {\"cat\":0, \"dog\":0}\nfor i in labels_iterator:\n    if i == 0:\n        classification_dict[\"cat\"] +=1\n    else:\n        classification_dict[\"dog\"] +=1\n\nprint(classification_dict)\nprint(classification_dict[\"dog\"]/(classification_dict[\"dog\"]+classification_dict[\"cat\"]))\n\n{'cat': 4637, 'dog': 4668}\n0.5016657710908113\n\n\n\n#class_names = train_ds.class_names\ntake_ran = np.random.randint(1, 291)\nplt.figure(figsize=(10, 10))\nim_num = 0\ncounter = 0\nfor images, labels in train_ds.take(take_ran):\n    counter +=1\n    if im_num &lt; 3:\n        if labels[counter].numpy() == 0:\n            ax = plt.subplot(3, 3, im_num + 1)\n            plt.imshow(images[counter].numpy().astype(\"uint8\"))\n            plt.title(\"cat\")\n            im_num +=1\n    elif im_num &lt; 6:\n        if labels[counter].numpy() == 1:\n            ax = plt.subplot(3, 3, im_num + 1)\n            plt.imshow(images[counter].numpy().astype(\"uint8\"))\n            plt.title(\"dog\")\n            im_num +=1\n        plt.axis(\"off\")\n    else:\n        break\n\n\n\n\n\n\n\n\n\nlabels_iterator= train_ds.unbatch().map(lambda image, label: label).as_numpy_iterator()\nclassification_dict = {\"cat\":0, \"dog\":0}\nfor i in labels_iterator:\n    if i == 0:\n        classification_dict[\"cat\"] +=1\n    else:\n        classification_dict[\"dog\"] +=1\n\nprint(classification_dict)\nprint(classification_dict[\"dog\"]/(classification_dict[\"dog\"]+classification_dict[\"cat\"]))\n\n{'cat': 4637, 'dog': 4668}\n0.5016657710908113\n\n\nThe baseline machine learning model is the model that always guesses the most frequent label. Since there are slightly more dogs than cats in our training set, if we were always to guess that the image is of a dog, then we would get the prediction right:\n\nprint(classification_dict[\"dog\"]/(classification_dict[\"dog\"]+classification_dict[\"cat\"]))\n\n0.5016657710908113\n\n\nslightly over 50% of the time. We want our model to do a lot better than this if possible!\n\ntrain_ds\n\n&lt;CacheDataset element_spec=(TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))&gt;\n\n\n\nmodel1 = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.25),\n    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nmodel2 = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.25),\n    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Conv2D(32, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nmodel3 = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.25),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nmodel1.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model1.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 66s 451ms/step - loss: 18.5739 - accuracy: 0.5179 - val_loss: 0.6939 - val_accuracy: 0.5013\nEpoch 2/20\n146/146 [==============================] - 65s 447ms/step - loss: 0.6914 - accuracy: 0.5316 - val_loss: 0.6945 - val_accuracy: 0.5047\nEpoch 3/20\n146/146 [==============================] - 64s 436ms/step - loss: 0.6860 - accuracy: 0.5487 - val_loss: 0.6953 - val_accuracy: 0.5052\nEpoch 4/20\n146/146 [==============================] - 63s 433ms/step - loss: 0.6769 - accuracy: 0.5698 - val_loss: 0.6970 - val_accuracy: 0.5064\nEpoch 5/20\n146/146 [==============================] - 63s 431ms/step - loss: 0.6627 - accuracy: 0.5825 - val_loss: 0.6930 - val_accuracy: 0.5413\nEpoch 6/20\n146/146 [==============================] - 70s 482ms/step - loss: 0.6426 - accuracy: 0.6082 - val_loss: 0.6872 - val_accuracy: 0.5537\nEpoch 7/20\n146/146 [==============================] - 72s 494ms/step - loss: 0.6152 - accuracy: 0.6390 - val_loss: 0.7054 - val_accuracy: 0.5645\nEpoch 8/20\n146/146 [==============================] - 66s 451ms/step - loss: 0.5796 - accuracy: 0.6631 - val_loss: 0.7162 - val_accuracy: 0.5752\nEpoch 9/20\n146/146 [==============================] - 66s 450ms/step - loss: 0.5420 - accuracy: 0.6977 - val_loss: 0.7937 - val_accuracy: 0.5770\nEpoch 10/20\n146/146 [==============================] - 67s 461ms/step - loss: 0.4948 - accuracy: 0.7222 - val_loss: 0.8172 - val_accuracy: 0.5774\nEpoch 11/20\n146/146 [==============================] - 356s 2s/step - loss: 0.4614 - accuracy: 0.7522 - val_loss: 0.8495 - val_accuracy: 0.5653\nEpoch 12/20\n146/146 [==============================] - 717s 5s/step - loss: 0.4347 - accuracy: 0.7682 - val_loss: 0.9020 - val_accuracy: 0.5658\nEpoch 13/20\n146/146 [==============================] - 62s 425ms/step - loss: 0.3984 - accuracy: 0.7986 - val_loss: 0.9860 - val_accuracy: 0.5778\nEpoch 14/20\n146/146 [==============================] - 62s 424ms/step - loss: 0.3829 - accuracy: 0.8082 - val_loss: 1.1108 - val_accuracy: 0.5825\nEpoch 15/20\n146/146 [==============================] - 65s 443ms/step - loss: 0.3762 - accuracy: 0.8216 - val_loss: 1.1310 - val_accuracy: 0.5787\nEpoch 16/20\n146/146 [==============================] - 62s 426ms/step - loss: 0.3301 - accuracy: 0.8403 - val_loss: 1.1563 - val_accuracy: 0.5731\nEpoch 17/20\n146/146 [==============================] - 62s 421ms/step - loss: 0.2920 - accuracy: 0.8614 - val_loss: 1.3111 - val_accuracy: 0.5748\nEpoch 18/20\n146/146 [==============================] - 63s 430ms/step - loss: 0.2523 - accuracy: 0.8870 - val_loss: 1.4469 - val_accuracy: 0.5684\nEpoch 19/20\n146/146 [==============================] - 62s 423ms/step - loss: 0.2515 - accuracy: 0.8833 - val_loss: 1.3675 - val_accuracy: 0.5752\nEpoch 20/20\n146/146 [==============================] - 62s 421ms/step - loss: 0.2276 - accuracy: 0.8990 - val_loss: 1.4461 - val_accuracy: 0.5825\n\n\n\nmodel2.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model2.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 40s 269ms/step - loss: 1.7543 - accuracy: 0.4989 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 2/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 3/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.6932 - accuracy: 0.4963 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 4/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 5/20\n146/146 [==============================] - 36s 246ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 6/20\n146/146 [==============================] - 36s 248ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 7/20\n146/146 [==============================] - 151s 1s/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 8/20\n146/146 [==============================] - 36s 244ms/step - loss: 0.6932 - accuracy: 0.4967 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 9/20\n146/146 [==============================] - 36s 246ms/step - loss: 0.6932 - accuracy: 0.4978 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 10/20\n146/146 [==============================] - 36s 248ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 11/20\n146/146 [==============================] - 36s 249ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 12/20\n146/146 [==============================] - 54s 372ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 13/20\n146/146 [==============================] - 36s 248ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 14/20\n146/146 [==============================] - 37s 252ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 15/20\n146/146 [==============================] - 38s 258ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 16/20\n146/146 [==============================] - 37s 250ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 17/20\n146/146 [==============================] - 38s 262ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 18/20\n146/146 [==============================] - 37s 252ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 19/20\n146/146 [==============================] - 37s 252ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\nEpoch 20/20\n146/146 [==============================] - 37s 252ms/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6933 - val_accuracy: 0.4948\n\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model3.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 81s 549ms/step - loss: 46.3093 - accuracy: 0.5420 - val_loss: 0.6904 - val_accuracy: 0.5176\nEpoch 2/20\n146/146 [==============================] - 81s 550ms/step - loss: 0.6659 - accuracy: 0.5842 - val_loss: 0.6896 - val_accuracy: 0.5279\nEpoch 3/20\n146/146 [==============================] - 81s 554ms/step - loss: 0.6462 - accuracy: 0.6082 - val_loss: 0.6918 - val_accuracy: 0.5464\nEpoch 4/20\n146/146 [==============================] - 84s 577ms/step - loss: 0.6051 - accuracy: 0.6484 - val_loss: 0.7015 - val_accuracy: 0.5572\nEpoch 5/20\n146/146 [==============================] - 87s 597ms/step - loss: 0.5479 - accuracy: 0.6956 - val_loss: 0.7324 - val_accuracy: 0.5653\nEpoch 6/20\n146/146 [==============================] - 87s 592ms/step - loss: 0.5124 - accuracy: 0.7219 - val_loss: 0.8320 - val_accuracy: 0.5602\nEpoch 7/20\n146/146 [==============================] - 86s 589ms/step - loss: 0.4504 - accuracy: 0.7608 - val_loss: 0.9073 - val_accuracy: 0.5834\nEpoch 8/20\n146/146 [==============================] - 96s 660ms/step - loss: 0.4091 - accuracy: 0.7944 - val_loss: 0.9157 - val_accuracy: 0.5864\nEpoch 9/20\n146/146 [==============================] - 86s 591ms/step - loss: 0.3614 - accuracy: 0.8257 - val_loss: 1.1317 - val_accuracy: 0.6053\nEpoch 10/20\n146/146 [==============================] - 97s 662ms/step - loss: 0.3137 - accuracy: 0.8521 - val_loss: 1.2630 - val_accuracy: 0.5873\nEpoch 11/20\n146/146 [==============================] - 94s 640ms/step - loss: 0.2939 - accuracy: 0.8658 - val_loss: 1.5064 - val_accuracy: 0.6075\nEpoch 12/20\n146/146 [==============================] - 89s 605ms/step - loss: 0.2354 - accuracy: 0.8973 - val_loss: 1.7173 - val_accuracy: 0.6015\nEpoch 13/20\n146/146 [==============================] - 88s 602ms/step - loss: 0.2223 - accuracy: 0.9095 - val_loss: 1.9325 - val_accuracy: 0.5933\nEpoch 14/20\n146/146 [==============================] - 180s 1s/step - loss: 0.2122 - accuracy: 0.9194 - val_loss: 2.0122 - val_accuracy: 0.5847\nEpoch 15/20\n146/146 [==============================] - 99s 675ms/step - loss: 0.1694 - accuracy: 0.9300 - val_loss: 2.3729 - val_accuracy: 0.6092\nEpoch 16/20\n146/146 [==============================] - 86s 591ms/step - loss: 0.1625 - accuracy: 0.9357 - val_loss: 2.1516 - val_accuracy: 0.6053\nEpoch 17/20\n146/146 [==============================] - 82s 562ms/step - loss: 0.1351 - accuracy: 0.9473 - val_loss: 2.2941 - val_accuracy: 0.6075\nEpoch 18/20\n146/146 [==============================] - 84s 573ms/step - loss: 0.1329 - accuracy: 0.9510 - val_loss: 2.3376 - val_accuracy: 0.6019\nEpoch 19/20\n146/146 [==============================] - 86s 588ms/step - loss: 0.1078 - accuracy: 0.9606 - val_loss: 2.5800 - val_accuracy: 0.6139\nEpoch 20/20\n146/146 [==============================] - 88s 600ms/step - loss: 0.0972 - accuracy: 0.9653 - val_loss: 2.9061 - val_accuracy: 0.5993\n\n\n\nmodel3 = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.5),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(64, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nmodel3.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model3.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 87s 597ms/step - loss: 91.5381 - accuracy: 0.4949 - val_loss: 0.6932 - val_accuracy: 0.4948\nEpoch 2/20\n146/146 [==============================] - 103s 705ms/step - loss: 0.6922 - accuracy: 0.5199 - val_loss: 0.6932 - val_accuracy: 0.4996\nEpoch 3/20\n146/146 [==============================] - 96s 658ms/step - loss: 0.6883 - accuracy: 0.5329 - val_loss: 0.6920 - val_accuracy: 0.5043\nEpoch 4/20\n146/146 [==============================] - 99s 677ms/step - loss: 0.6841 - accuracy: 0.5502 - val_loss: 0.6930 - val_accuracy: 0.5026\nEpoch 5/20\n146/146 [==============================] - 94s 641ms/step - loss: 0.6734 - accuracy: 0.5615 - val_loss: 0.6903 - val_accuracy: 0.5206\nEpoch 6/20\n146/146 [==============================] - 97s 663ms/step - loss: 0.6513 - accuracy: 0.5952 - val_loss: 0.6861 - val_accuracy: 0.5370\nEpoch 7/20\n146/146 [==============================] - 93s 638ms/step - loss: 0.6317 - accuracy: 0.6111 - val_loss: 0.6829 - val_accuracy: 0.5486\nEpoch 8/20\n146/146 [==============================] - 114s 779ms/step - loss: 0.6050 - accuracy: 0.6305 - val_loss: 0.6893 - val_accuracy: 0.5602\nEpoch 9/20\n146/146 [==============================] - 99s 677ms/step - loss: 0.5770 - accuracy: 0.6622 - val_loss: 0.6941 - val_accuracy: 0.5688\nEpoch 10/20\n146/146 [==============================] - 106s 724ms/step - loss: 0.5461 - accuracy: 0.6877 - val_loss: 0.7030 - val_accuracy: 0.5679\nEpoch 11/20\n146/146 [==============================] - 104s 709ms/step - loss: 0.5067 - accuracy: 0.7174 - val_loss: 0.7431 - val_accuracy: 0.5658\nEpoch 12/20\n146/146 [==============================] - 111s 759ms/step - loss: 0.4587 - accuracy: 0.7554 - val_loss: 0.8044 - val_accuracy: 0.5830\nEpoch 13/20\n146/146 [==============================] - 108s 742ms/step - loss: 0.4240 - accuracy: 0.7772 - val_loss: 0.9070 - val_accuracy: 0.5795\nEpoch 14/20\n146/146 [==============================] - 102s 698ms/step - loss: 0.3832 - accuracy: 0.8087 - val_loss: 1.0703 - val_accuracy: 0.5959\nEpoch 15/20\n146/146 [==============================] - 104s 711ms/step - loss: 0.3642 - accuracy: 0.8253 - val_loss: 1.0733 - val_accuracy: 0.5778\nEpoch 16/20\n146/146 [==============================] - 105s 720ms/step - loss: 0.3093 - accuracy: 0.8576 - val_loss: 1.1632 - val_accuracy: 0.5808\nEpoch 17/20\n146/146 [==============================] - 104s 711ms/step - loss: 0.2919 - accuracy: 0.8644 - val_loss: 1.1887 - val_accuracy: 0.5873\nEpoch 18/20\n146/146 [==============================] - 86s 589ms/step - loss: 0.2627 - accuracy: 0.8817 - val_loss: 1.2570 - val_accuracy: 0.5886\nEpoch 19/20\n146/146 [==============================] - 90s 613ms/step - loss: 0.2430 - accuracy: 0.8946 - val_loss: 1.4591 - val_accuracy: 0.6002\nEpoch 20/20\n146/146 [==============================] - 163s 1s/step - loss: 0.2420 - accuracy: 0.8990 - val_loss: 1.4847 - val_accuracy: 0.5980\n\n\n\nmodel5 = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.5),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nmodel5.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model5.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 83s 561ms/step - loss: 148.8628 - accuracy: 0.5131 - val_loss: 0.6933 - val_accuracy: 0.4957\nEpoch 2/20\n146/146 [==============================] - 86s 589ms/step - loss: 0.6909 - accuracy: 0.5202 - val_loss: 0.6931 - val_accuracy: 0.5017\nEpoch 3/20\n146/146 [==============================] - 85s 584ms/step - loss: 0.6829 - accuracy: 0.5491 - val_loss: 0.6928 - val_accuracy: 0.5056\nEpoch 4/20\n146/146 [==============================] - 95s 647ms/step - loss: 0.6716 - accuracy: 0.5720 - val_loss: 0.6923 - val_accuracy: 0.5077\nEpoch 5/20\n146/146 [==============================] - 98s 673ms/step - loss: 0.6572 - accuracy: 0.5895 - val_loss: 0.6894 - val_accuracy: 0.5275\nEpoch 6/20\n146/146 [==============================] - 102s 696ms/step - loss: 0.6423 - accuracy: 0.6159 - val_loss: 0.6881 - val_accuracy: 0.5383\nEpoch 7/20\n146/146 [==============================] - 100s 682ms/step - loss: 0.6197 - accuracy: 0.6371 - val_loss: 0.6865 - val_accuracy: 0.5559\nEpoch 8/20\n146/146 [==============================] - 99s 679ms/step - loss: 0.5959 - accuracy: 0.6586 - val_loss: 0.6954 - val_accuracy: 0.5731\nEpoch 9/20\n146/146 [==============================] - 98s 672ms/step - loss: 0.5718 - accuracy: 0.6805 - val_loss: 0.6860 - val_accuracy: 0.5903\nEpoch 10/20\n146/146 [==============================] - 99s 675ms/step - loss: 0.5359 - accuracy: 0.7106 - val_loss: 0.6901 - val_accuracy: 0.6118\nEpoch 11/20\n146/146 [==============================] - 91s 623ms/step - loss: 0.4970 - accuracy: 0.7436 - val_loss: 0.7245 - val_accuracy: 0.6101\nEpoch 12/20\n146/146 [==============================] - 95s 650ms/step - loss: 0.4706 - accuracy: 0.7611 - val_loss: 0.7682 - val_accuracy: 0.5903\nEpoch 13/20\n146/146 [==============================] - 94s 640ms/step - loss: 0.4314 - accuracy: 0.7912 - val_loss: 0.8029 - val_accuracy: 0.6156\nEpoch 14/20\n146/146 [==============================] - 98s 671ms/step - loss: 0.4153 - accuracy: 0.7946 - val_loss: 0.8260 - val_accuracy: 0.5959\nEpoch 15/20\n146/146 [==============================] - 97s 664ms/step - loss: 0.3712 - accuracy: 0.8190 - val_loss: 0.9192 - val_accuracy: 0.6088\nEpoch 16/20\n146/146 [==============================] - 95s 647ms/step - loss: 0.3437 - accuracy: 0.8399 - val_loss: 0.9800 - val_accuracy: 0.6221\nEpoch 17/20\n146/146 [==============================] - 105s 718ms/step - loss: 0.3157 - accuracy: 0.8502 - val_loss: 1.0257 - val_accuracy: 0.6255\nEpoch 18/20\n146/146 [==============================] - 97s 661ms/step - loss: 0.2849 - accuracy: 0.8723 - val_loss: 1.2465 - val_accuracy: 0.6182\nEpoch 19/20\n146/146 [==============================] - 93s 633ms/step - loss: 0.2858 - accuracy: 0.8753 - val_loss: 1.3133 - val_accuracy: 0.6182\nEpoch 20/20\n146/146 [==============================] - 93s 634ms/step - loss: 0.2468 - accuracy: 0.8921 - val_loss: 1.2754 - val_accuracy: 0.6195\n\n\n\nmodel6 = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.5),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nmodel6.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model6.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 91s 622ms/step - loss: 147.4244 - accuracy: 0.5071 - val_loss: 0.6930 - val_accuracy: 0.4974\nEpoch 2/20\n146/146 [==============================] - 97s 662ms/step - loss: 0.6885 - accuracy: 0.5383 - val_loss: 0.6929 - val_accuracy: 0.5021\nEpoch 3/20\n146/146 [==============================] - 96s 658ms/step - loss: 0.6742 - accuracy: 0.5714 - val_loss: 0.6905 - val_accuracy: 0.5189\nEpoch 4/20\n146/146 [==============================] - 98s 672ms/step - loss: 0.6639 - accuracy: 0.5827 - val_loss: 0.6892 - val_accuracy: 0.5249\nEpoch 5/20\n146/146 [==============================] - 94s 646ms/step - loss: 0.6503 - accuracy: 0.6053 - val_loss: 0.6876 - val_accuracy: 0.5305\nEpoch 6/20\n146/146 [==============================] - 100s 684ms/step - loss: 0.6360 - accuracy: 0.6189 - val_loss: 0.6924 - val_accuracy: 0.5391\nEpoch 7/20\n146/146 [==============================] - 99s 677ms/step - loss: 0.6186 - accuracy: 0.6417 - val_loss: 0.6982 - val_accuracy: 0.5391\nEpoch 8/20\n146/146 [==============================] - 118s 811ms/step - loss: 0.6021 - accuracy: 0.6509 - val_loss: 0.7175 - val_accuracy: 0.5353\nEpoch 9/20\n146/146 [==============================] - 98s 671ms/step - loss: 0.5820 - accuracy: 0.6797 - val_loss: 0.7318 - val_accuracy: 0.5391\nEpoch 10/20\n146/146 [==============================] - 98s 673ms/step - loss: 0.5572 - accuracy: 0.6869 - val_loss: 0.7686 - val_accuracy: 0.5512\nEpoch 11/20\n146/146 [==============================] - 100s 683ms/step - loss: 0.5402 - accuracy: 0.7007 - val_loss: 0.7815 - val_accuracy: 0.5701\nEpoch 12/20\n146/146 [==============================] - 103s 702ms/step - loss: 0.5189 - accuracy: 0.7130 - val_loss: 0.8167 - val_accuracy: 0.5718\nEpoch 13/20\n146/146 [==============================] - 99s 675ms/step - loss: 0.4789 - accuracy: 0.7435 - val_loss: 0.8530 - val_accuracy: 0.5752\nEpoch 14/20\n146/146 [==============================] - 102s 698ms/step - loss: 0.4526 - accuracy: 0.7583 - val_loss: 0.8745 - val_accuracy: 0.5847\nEpoch 15/20\n146/146 [==============================] - 463s 3s/step - loss: 0.4165 - accuracy: 0.7914 - val_loss: 0.9287 - val_accuracy: 0.5916\nEpoch 16/20\n146/146 [==============================] - 93s 632ms/step - loss: 0.3971 - accuracy: 0.8033 - val_loss: 0.9476 - val_accuracy: 0.5976\nEpoch 17/20\n146/146 [==============================] - 95s 648ms/step - loss: 0.3548 - accuracy: 0.8274 - val_loss: 1.1110 - val_accuracy: 0.5954\nEpoch 18/20\n146/146 [==============================] - 95s 647ms/step - loss: 0.3224 - accuracy: 0.8409 - val_loss: 1.2639 - val_accuracy: 0.5890\nEpoch 19/20\n146/146 [==============================] - 5394s 37s/step - loss: 0.2871 - accuracy: 0.8694 - val_loss: 1.2749 - val_accuracy: 0.5980\nEpoch 20/20\n146/146 [==============================] - 3660s 25s/step - loss: 0.2686 - accuracy: 0.8780 - val_loss: 1.3559 - val_accuracy: 0.5997\n\n\nModel 5 was our best model:\n\nmodel5 = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.5),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nflip_fn = keras.layers.RandomFlip(\"vertical\")\nflipped_ds = train_ds.map(lambda x, y: (flip_fn(x), y))\n#class_names = train_ds.class_names\ntake_ran = np.random.randint(1, 291)\nplt.figure(figsize=(10, 10))\nim_num = 0\ncounter = 0\nfor images, labels in flipped_ds.take(take_ran):\n    counter +=1\n    if im_num &lt; 3:\n        if labels[counter].numpy() == 0:\n            ax = plt.subplot(3, 3, im_num + 1)\n            plt.imshow(images[counter].numpy().astype(\"uint8\"))\n            plt.title(\"cat\")\n            im_num +=1\n    elif im_num &lt; 6:\n        if labels[counter].numpy() == 1:\n            ax = plt.subplot(3, 3, im_num + 1)\n            plt.imshow(images[counter].numpy().astype(\"uint8\"))\n            plt.title(\"dog\")\n            im_num +=1\n        plt.axis(\"off\")\n    else:\n        break\n\n\n\n\n\n\n\n\n\nflip_fn = keras.layers.RandomFlip(\"vertical_and_horizontal\")\nflipped_ds = train_ds.map(lambda x, y: (flip_fn(x), y))\n\nplt.figure(figsize=(10, 10))\nim_num = 0\nfor images, labels in flipped_ds:\n    if im_num &gt;= 6:\n        break\n    if im_num % 3 == 0:\n        animal_type = \"cat\" if labels[0].numpy() == 0 else \"dog\"\n    if labels[0].numpy() == (im_num // 3):\n        ax = plt.subplot(3, 3, im_num + 1)\n        plt.imshow(images[0].numpy().astype(\"uint8\"))\n        plt.title(animal_type)\n        plt.axis(\"off\")\n        im_num += 1\n\n\n\n\n\n\n\n\n\nflip_fn = keras.layers.RandomFlip(\"horizontal_and_vertical\")\nplt.figure(figsize=(5, 5))\ncounter = 0\nfor images, labels in train_ds.take(6):\n    ax = plt.subplot(3, 4, counter*2 + 1)\n    plt.imshow(images[0].numpy().astype(\"uint8\"))\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    ax = plt.subplot(3, 4, counter*2 + 2)\n    flipped_images = flip_fn(images)\n    plt.imshow(flipped_images[0].numpy().astype(\"uint8\"))\n    plt.title(\"Flipped\")\n    plt.axis(\"off\")\n\n    counter += 1\n\n\n\n\n\n\n\n\nAs we can see, whether or not an image is flipped is random, and it can be flipped both horizontally and vertically.\n\nrotate_fn = keras.layers.RandomRotation(factor=1, fill_mode =\"reflect\", interpolation = \"bilinear\")\nplt.figure(figsize=(5, 5))\ncounter = 0\nfor images, labels in train_ds.take(6):\n    ax = plt.subplot(3, 4, counter*2 + 1)\n    plt.imshow(images[0].numpy().astype(\"uint8\"))\n    plt.title(\"Original\")\n    plt.axis(\"off\")\n\n    ax = plt.subplot(3, 4, counter*2 + 2)\n    rotated_images = rotate_fn(images)\n    plt.imshow(rotated_images[0].numpy().astype(\"uint8\"))\n    plt.title(\"Rotated\")\n    plt.axis(\"off\")\n\n    counter += 1\n\n\n\n\n\n\n\n\n\nmodel_new = keras.models.Sequential([\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(factor=1, fill_mode =\"reflect\", interpolation = \"bilinear\"),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.5),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(128, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n\nmodel_new.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model_new.fit(train_ds, \n                     epochs=20, \n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 101s 685ms/step - loss: 119.2604 - accuracy: 0.5126 - val_loss: 0.6931 - val_accuracy: 0.4996\nEpoch 2/20\n146/146 [==============================] - 101s 689ms/step - loss: 0.6933 - accuracy: 0.5119 - val_loss: 0.6930 - val_accuracy: 0.4979\nEpoch 3/20\n146/146 [==============================] - 105s 715ms/step - loss: 0.6928 - accuracy: 0.5133 - val_loss: 0.6932 - val_accuracy: 0.4987\nEpoch 4/20\n146/146 [==============================] - 111s 758ms/step - loss: 0.6927 - accuracy: 0.5154 - val_loss: 0.6931 - val_accuracy: 0.4987\nEpoch 5/20\n 45/146 [========&gt;.....................] - ETA: 1:19 - loss: 0.6921 - accuracy: 0.5059\n\n\nKeyboardInterrupt: \n\n\n\ni = keras.Input(shape=(150, 150, 3))\n# The pixel values have the range of (0, 255), but many models will work better if rescaled to (-1, 1.)\n# outputs: `(inputs * scale) + offset`\nscale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\nx = scale_layer(i)\npreprocessor = keras.Model(inputs = [i], outputs = [x])\n\n\nmodel_preprocessor = keras.models.Sequential([\n    preprocessor,\n    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n    keras.layers.RandomRotation(factor=1, fill_mode =\"reflect\", interpolation = \"bilinear\"),\n    keras.layers.Input((150, 150, 3)),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.3),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.MaxPooling2D((2, 2)),\n    keras.layers.Dropout(.3),\n    keras.layers.Conv2D(64, (3, 3), activation='relu'),\n    keras.layers.Flatten(),\n    keras.layers.Dense(256, activation='relu'),\n    keras.layers.Dense(2) # number of classes\n])\n\n#74.85: 64, .5,64, .5, 64, 256\n\n\nmodel_preprocessor.compile(optimizer='adam',\n              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model_preprocessor.fit(train_ds,\n                     epochs=20,\n                     validation_data=validation_ds)\n\nEpoch 1/20\n146/146 [==============================] - 119s 810ms/step - loss: 0.7906 - accuracy: 0.5062 - val_loss: 0.6934 - val_accuracy: 0.4948\nEpoch 2/20\n146/146 [==============================] - 113s 775ms/step - loss: 0.6917 - accuracy: 0.5147 - val_loss: 0.6943 - val_accuracy: 0.4948\nEpoch 3/20\n146/146 [==============================] - 119s 817ms/step - loss: 0.6873 - accuracy: 0.5204 - val_loss: 0.6844 - val_accuracy: 0.5679\nEpoch 4/20\n146/146 [==============================] - 124s 847ms/step - loss: 0.6632 - accuracy: 0.5857 - val_loss: 0.6509 - val_accuracy: 0.6036\nEpoch 5/20\n146/146 [==============================] - 3563s 25s/step - loss: 0.6406 - accuracy: 0.6113 - val_loss: 0.6180 - val_accuracy: 0.6690\nEpoch 6/20\n146/146 [==============================] - 1387s 10s/step - loss: 0.6174 - accuracy: 0.6502 - val_loss: 0.5878 - val_accuracy: 0.6883\nEpoch 7/20\n124/146 [========================&gt;.....] - ETA: 19s - loss: 0.6008 - accuracy: 0.6711"
  },
  {
    "objectID": "posts/Homework 3/index.html",
    "href": "posts/Homework 3/index.html",
    "title": "Making Discord, Well, Sort of…",
    "section": "",
    "text": "Up until now, we’ve been creating programs that we run locally on our system, but often times when we interact with something on a computer, we do so thanks to something that someone else has made that we’ve acquired through the internet. Now, by making a website using HTML and CSS, we’ll do the same (in theory, anyway, since we won’t be publicly hosting our website, but you could if you wanted to). We’ll also put our code onto a GitHub repository so that anyone can play with it freely.\nIn this post, I’m going to create a simple website that gives the user the opportunity to send messages under a certain name or handle and will then be able to view such messages in a different page. The skeleton of the website will be designed via HTML templates, but we’ll be using a framework called Flask which allows us to write Python code specifically for our website, allowing us to incorporate more advanced techniques, including a SQL database which will contain users’ stored messages. Finally, we’ll also dabble a bit into CSS, which we can use to customize the look of our website, making it look like the popular messaging app Discord.\n\n\nWhile I provide a tutorial to create everything from scratch, you are free to download my files for the project and experiment with them however you like:\nhttps://github.com/torwar02/hw3\nEverything necessary is in the primary folder called flask-project"
  },
  {
    "objectID": "posts/Homework 3/index.html#github-repository",
    "href": "posts/Homework 3/index.html#github-repository",
    "title": "Making Discord, Well, Sort of…",
    "section": "",
    "text": "While I provide a tutorial to create everything from scratch, you are free to download my files for the project and experiment with them however you like:\nhttps://github.com/torwar02/hw3\nEverything necessary is in the primary folder called flask-project"
  },
  {
    "objectID": "posts/Homework 3/index.html#creating-a-flask-project",
    "href": "posts/Homework 3/index.html#creating-a-flask-project",
    "title": "Making Discord, Well, Sort of…",
    "section": "Creating a Flask Project",
    "text": "Creating a Flask Project\nOnce you have Flask installed, it’s time to actually create a project. We first need to set something up ourselves–make a folder, and give it an appropriate name (mine is simply called flask-project as it is unambiguous inside of my blog files), and using a text editor of your choice, create a new Python file, also giving it a sensible name (app.py). Then, in your command line, change the current directory to your project folder (using\ncd\nand the appropriate folder name) and run the following command:\nexport FLASK_APP=app.py\nThis lets Flask know that app.py is the sort of ‘master file’ that your web app is going to be based on. Otherwise, it wouldn’t know what to do! But we can’t actually run our website yet as we need to set up a few things in our file before we can even view anything. Inside of app.py, you should have the following:\nimport sqlite3\nimport pandas as pd\n\nfrom flask import Flask, g, render_template, request\nfrom flask import redirect, url_for\n\napp = Flask(__name__)\nThe import statements are nothing crazy, although it’s important to note that there are a lot of things that we have to retrieve from Flask itself, including Flask, which is necessary for making our website run, and render_template, without which we would not be able to view HTML files!\nFinally, the line app = Flask(__name__) is what actually creates the file into something that Flask can interact with. This is essentially the skeleton of the website.\nAt this point, we can actually get our website to work with the command\nflask run\nThough we won’t see our website because we haven’t put anything on it yet."
  },
  {
    "objectID": "posts/Homework 3/index.html#project-overview",
    "href": "posts/Homework 3/index.html#project-overview",
    "title": "Making Discord, Well, Sort of…",
    "section": "Project Overview",
    "text": "Project Overview\nFrom this point on, this post will be divided into sections that correspond to different parts of our website that we need to put together, but before we can do that, we need to have an understanding of what we’re going to do.\n\nSubmission\nOur website needs to have a page for the user to submit messages based on a handle of their choice. Below is a screenshot provided from the PIC 16B blog as an example:\n\n\n\nimage.png\n\n\n\n\nDatabase Creation\nWe’re going to eventually want the user to be able to view past messages, but to do so, we’re going to have to store existing ones somehow. We will do this by creating a function within app.py that allows us to create and manipulate a SQL database (which is why earlier we imported sqlite3), which will later be retrieved by the website.\n\n\nMessage Viewing\nFinally, we’re going to want to create an additional page so that users can look at previous messages that have been set. Per the specifications of this assignment, we’re going to want to display five randomly chosen messages out of the existing bank. We’ll have to make not only a new HTML page but also a function that allows us to retrieve the messages from the database.\nWith that overview out of the way, let’s get started!"
  },
  {
    "objectID": "posts/Homework 3/index.html#body-of-submit",
    "href": "posts/Homework 3/index.html#body-of-submit",
    "title": "Making Discord, Well, Sort of…",
    "section": "Body of /submit/",
    "text": "Body of /submit/\nThe following code won’t work because I show a function that we haven’t implemented yet (but will soon):\n@app.route(\"/submit/\", methods=['POST', 'GET'])\ndef submit():\n    if request.method == 'GET':\n        return render_template('submit.html')\n    else:\n        insert_message(request)\n        return render_template('submit.html', sec = True)\nYou can think of request as the current page (remember Webscraping?) and the method as as the “state” of the website. GET is the state of the page when we arrive on the page. render_template is what allows us to take another HTML file and display it on the page, so we’re going to have to make a template called submit.html. If the method is post, we’re going to call a function called insert_message that will allow us to put messages on the message board, so we need to write that function as well. There’s a lot that we have to do to make it work! Let’s start with the Python functions, because otherwise we won’t be able to even display anything on our site."
  },
  {
    "objectID": "posts/Homework 3/index.html#get_message_db",
    "href": "posts/Homework 3/index.html#get_message_db",
    "title": "Making Discord, Well, Sort of…",
    "section": "get_message_db()",
    "text": "get_message_db()\nThe function insert_message will place a message onto the current instance (request) of the website, via a SQL database, so first we need to make a function that simultaneously creates accesses the database (recall from the Week 1 post that .connect in sqlite3 can do both). To do this, we’ll make a function called get_message_db().\nHere’s the function:\ndef get_message_db(): #create/access database\n    try:\n        return g.message_db #gets datbase from `g` object\n    except: #should only go through here once\n        g.message_db = sqlite3.connect(\"messages_db.sqlite\") #makes database\n        cursor = g.message_db.cursor()\n        cmd = \"\"\"\n        CREATE TABLE IF NOT EXISTS messages(\n        handle TEXT,\n        message TEXT\n        );\n        \"\"\" #creates table\n        cursor.execute(cmd)\n        cursor.close()\n        return g.message_db #then returns datbaase\nWe do this with try and except statements because we would get an error if the database doesn’t exist. Most of this should be review from the Week 1 post, including the use of sqlite3.connect to make and then connect to a database, which we call messages_db. Our SQL query creates a table called messages using the CREATE TABLE IF NOT EXISTS command. We specify the name of each column (handle and message) as well as the data type (TEXT). We execute the command, close the cursor, and send back the database.\nThe only thing here that’s completely new is g, which is an attribute of app, that is, the file itself which comes with Flask. I am not well-versed enough in Flask to give an excellent explanation, but see this StackOverflow post for more info:\nhttps://stackoverflow.com/questions/30514749/what-is-the-g-object-in-this-flask-code\nEssentially, it’s a sort of global object in which we can store whatever we want while the app is running. This is important since it sort of lets us make global variables for any of the functions we create, which is why we decide to make our database equivalent to an attribute of g called message_db. Just note that this didn’t work (at least for me) until I imported g from Flask (which I had you do at the beginning)."
  },
  {
    "objectID": "posts/Homework 3/index.html#insert_message",
    "href": "posts/Homework 3/index.html#insert_message",
    "title": "Making Discord, Well, Sort of…",
    "section": "insert_message()",
    "text": "insert_message()\nThis function is a bit hard to explain without the HTML template for /submit, but let’s walk through it:\ndef insert_message(request): #places messages into database\n    handle = request.form['handle'] #extracts `handle`/`message` from form\n    message = request.form['message']\n    db = get_message_db()\n    info = pd.DataFrame([{'handle': handle, 'message' : message}]) #places handle/message into df\n    info.to_sql(\"messages\", db, if_exists = \"append\", index = False) #places into db\n    db.commit()\n    db.close()\n    return handle, message #sends back `handle` and `message` to `submit()`\nI’ll explain this later, but keep in mind that this function is called inside of the submit() function, so we should be thinking that we’re on the submission page. We’ll be grabbing user-inputted variables called message and handle (which will be part of the HTML code) and store them in the function. We then connect to the database using get_message_db(), create a one-row data frame called info containing the handle and the message, and append it onto the messages table in the database (once again, review Homework 1 post for details on the to_sql method from Pandas). Then, we commit the changes, lose the connection, and send back the handle and messages."
  },
  {
    "objectID": "posts/Homework 3/index.html#base.html",
    "href": "posts/Homework 3/index.html#base.html",
    "title": "Making Discord, Well, Sort of…",
    "section": "base.html",
    "text": "base.html\nThe first thing that I wanted to remind you of is the fact that I wanted to make the webpage look like Discord, specifically, a Discord server used by UCLA students, which is why there’s referenced to this throughout the page:\n&lt;!doctype html&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\"&gt;\n&lt;title&gt;{% block title %}{% endblock %} Fake Discord&lt;/title&gt;\n&lt;nav&gt;\n    &lt;h1&gt;Welcome to UCLA Students and Friends! &lt;br&gt;\n        Please see #rules-and-info to get started.\n    &lt;/h1&gt;\n  &lt;ul&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('submit') }}\"&gt;Post Messages&lt;/a&gt;&lt;/li&gt;\n    &lt;li&gt;&lt;a href=\"{{ url_for('view') }}\"&gt;View Messages&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/nav&gt;\n\n&lt;img src = \"https://cdn.discordapp.com/attachments/1186502778600820746/1204873997150453860/ChannelsSidebar.png?ex=65d6515a&is=65c3dc5a&hm=b906d75f0192467f2062c75eb343ba78f629fd8593afa5c781f067bc947f5514&\" height = \"800\" width = \"250\" style=\"position:fixed; top:0px;  left:0px\"&gt;\n    \n&lt;img src = \"https://cdn.discordapp.com/attachments/1186502778600820746/1204873997460971582/UsersSiderbar.png?ex=65d6515a&is=65c3dc5a&hm=f1282537c4f4f000be28a42e583e212f1763294cfde93973c80297b84824a535&\" height = \"800\" width = \"250\" style=\"position:fixed; top:0px;  right:0px\"&gt;\n\n&lt;section class=\"content\"&gt;\n  &lt;header&gt;\n    {% block header %}{% endblock %}\n  &lt;/header&gt;\n  {% block content %}{% endblock %}\n&lt;/section&gt;\nThere’s a lot going on here, and I used the header from a Flask demo that our professor prepared for us as a template while adding my own contributions. We’ll walk through the major parts, but note that I don’t know a lot of the fine details regarding HTML–it’s important to keep in mind that by working with Python, Flask, CSS, and HTML, we’re using many different tools, and it’s unrealistic to be able to master all of them in a short period of time. I have referred to documentation when I was stuck, and I encourage you to do the same.\nFirstly we write &lt;!doctype html&gt; to signal that this is an HTML document. The second statement connects it to the file that we’ll look at later which incorporates our CSS settings.\n\n&lt;title&gt; and &lt;nav&gt;\nFirst we start with a title. This refers to the text that appears on the tab of your web browser. Note that the content of each tag is demarcated by &lt;tag&gt; and &lt;/tag&gt;. A &lt;nav&gt; tag contains links to other parts of the page–we make this so that we can link the submission and viewing pages. First, we put some title text in &lt;h1&gt;, which stands for “heading 1” (bigger text, you’ve seen it on this blog!). Then, we make a &lt;ul&gt; or unordered list that has our links. Each element of a list is an li, and in each li we put the link to our other pages. This should look familiar from the webscraping tutorial–links are &lt;a&gt;. The only thing that’s new for us is the link itself, which is \"{{ url_for('submit') }}\"–we saw this earlier when using redirect–which refers to the URL governed by the submit function. We do the same for view. Then, we close out the list and navegation.\n\n\n&lt;img&gt;\nNow, I want to place images on either side to pretend that we’re on Discord. I took screenshots from a Discord server and uploaded them onto Discord itself since I frequently use it to store image links. I couldn’t get this to work by storing the images locally, otherwise I would have. Each &lt;img&gt; is made of various attributes: src tells us where to get it from (in our case, a URL), whereas height and width control the dimensions. style can be used to control where the image is placed on the page. I want them to look like sidebars, so I set the position to fixed and specify how many pixels from the page’s top, left, or right I want the image. This site was very helpful:\nhttps://www.w3schools.com/html/html_images.asp\n\n\n&lt;section&gt;\nWe use this tag to create concrete sections of a document. For base.html, there really shouldn’t be much here, since all we want to be constant is the navigation and images. Note that we can give it a class type of content , and inside, we use syntax from Jinja, which helps us code certain Python functionalities into HTML pages: we enclose these statements with a curly brace and percent sign, and inside, we put instructions. Here we create header and content blocks. In this case, they’re just markers–when we reference base.html in our other templates, we can specify what we want to put in here. base.html just provides a framework."
  },
  {
    "objectID": "posts/Homework 3/index.html#submit.html",
    "href": "posts/Homework 3/index.html#submit.html",
    "title": "Making Discord, Well, Sort of…",
    "section": "submit.html",
    "text": "submit.html\nWith base.html out of the way, let’s look at what we really wanted to see–the layout of the submit page. It’s actually simpler than you might think, thanks to the fact that submit() in app.py does a lot of the hard work:\n{% extends 'base.html' %}\n\n{% block content %}\n\n&lt;img src =\"https://cdn.discordapp.com/attachments/360940494240350209/1204479028057215056/discordchatting.png?ex=65d4e182&is=65c26c82&hm=922db462e88ad85c31db7b07af6a11855da471687480209a70b96cf82fd57720&\"&gt;\n  &lt;form method=\"post\"&gt;\n      &lt;label for=\"handle\"&gt;What's your handle?&lt;/label&gt;\n      &lt;input name=\"handle\" id=\"handle\"&gt;\n      &lt;label for=\"message\"&gt;Message #general...&lt;/label&gt;\n      &lt;input type = \"text\" name=\"message\" id=\"message\"&gt;\n      &lt;input type=\"submit\" value=\"Send Message\"&gt;\n  &lt;/form&gt;\n  {% if sec == True %}\n  &lt;br&gt;\n      &lt;b&gt;Message sent&lt;/b&gt;\n  {% endif %}\n{% endblock %}"
  },
  {
    "objectID": "posts/Homework 3/index.html#jinja-syntax",
    "href": "posts/Homework 3/index.html#jinja-syntax",
    "title": "Making Discord, Well, Sort of…",
    "section": "Jinja syntax",
    "text": "Jinja syntax\nThe first thing we do is to extend base.html–that is, to essentially place this template’s code within that file. We then specify what we want in the content block which we left empty in base.html.\nThe first thing we do is place an image that simulates a chat in a Disord channel, which we’ve talked about already. Then, however, we input a form with method post"
  },
  {
    "objectID": "posts/Homework 3/index.html#form-and-submit",
    "href": "posts/Homework 3/index.html#form-and-submit",
    "title": "Making Discord, Well, Sort of…",
    "section": "<form> and submit()",
    "text": "&lt;form&gt; and submit()\nThis is where it’s crucial to understand how submit.html interacts with app.py. Essentially, we want to create a tag of type &lt;form&gt; which allows for user input on the site. We specify that it has a method called post–the same post as we saw in the arguments of submit(). The method on the page when we first arrive is get (not listed), but it will be updated to post once the user interacts with the text box, We create labels for the two text boxes (&lt;label&gt;) which we specify are for two variables called handle and message–the two things we wanted to place in messages_db.sqlite. We then create two different tags of type &lt;input&gt; which are the text boxes for the above variables, and finally, a submission button which we get as specifying the type for &lt;input&gt; as submit. Then, we end the form.\n\nrender_template()\nBack in the body for submit() in app.py, we see that we call the function render_template() on submit.html. This causes the template we’ve made above to be displayed and prompts the user for input, setting the method to post. Notice that this submission triggers the body of the else part of the conditional statement, which calls insert_message() on request (recall that request refers to what’s available on the current page). Recall that insert_message() starts as such:\ndef insert_message(request):\n    message = request.form['message']\n    handle = request.form['handle']\nIn other words, we grab the message and handle variables from the form of submit.html. This is how we are able to get Python to interact with the website through Flask in order to store the messages!\nAt the end of the else statement, we render the page once again but with sec set to True. We are allowed to input parameters into our function so long as they correspond with Jinja syntax. Returning to submit.html, we see that we have a statement {% if sec == True %}, which corresponds to such a variable. When the condition is satisfied, we create a line break &lt;br&gt; and display a message saying Message sent, after which we close the conditional statement and the block in general. Here’s what the website looks like when the user is submitting a message!\nThis image shows a user typing a message into the message form:\n\n\n\nImage 3\n\n\nThis one shows the submission message:\n\n\n\nImage 4"
  },
  {
    "objectID": "posts/Homework 3/index.html#random_messages",
    "href": "posts/Homework 3/index.html#random_messages",
    "title": "Making Discord, Well, Sort of…",
    "section": "random_messages()",
    "text": "random_messages()\nAll we essentially need from this function is to return 5 rows from the database in a way that can be read by Python. Look at the following code:\ndef random_messages(n):\n    db = get_message_db()\n    cmd = f\"\"\" SELECT * FROM messages ORDER BY RANDOM() LIMIT {n}; \"\"\"\n    postings = pd.read_sql_query(cmd, db)\n    db.close()\n    return postings\nIn general, we allow for any number of random messages, though I specify 5 in view.html (thus, this can be changed by the user), hence the SQL query we execute will be in the form of an f string. Thanks to the following StackOverflow post provided by Professor Ko, we’re able to easily write the query:\nhttps://stackoverflow.com/questions/2279706/select-random-row-from-a-sqlite-table\nEssentially, * refers to selecting everything, ORDER BY sorts the table, and we do so with a function in SQL called RANDOM(). Finally, LIMIT specifies the number of rows we want back hence, n.\nWe then use the read_sql_query command from pandas alongside the database connection created by using get_message_db, we close the connection, and then we return the rows that we get back. Recall from the post on `sqlite that this will be in the form of a data frame."
  },
  {
    "objectID": "posts/Homework 3/index.html#view-1",
    "href": "posts/Homework 3/index.html#view-1",
    "title": "Making Discord, Well, Sort of…",
    "section": "view()",
    "text": "view()\nNow that we have a function that can fetch items from the database, we’re going to need a function responsible for viewing our page, which we call view in app.py. As we did with submit(), we will need to return a render_template() statement in order to make the changes meaningful on the page. Here’s the code:\n@app.route(\"/view/\")\ndef view(): #For viewing messages\n    postings = random_messages(5) #Get 5 random messages from database\n    length = 5\n    message_tuples = []\n    for i in range(length):\n        message_tuples.append(tuple(postings.iloc[i,:])) #Convert df rows into tuples\n    return render_template('view.html', message_tuples = message_tuples) #display `view`\nFirstly, we call the random_messages() function with n = 5, and then, we create a list that will contain tuples from the postings data frame, which will make it easier to interface with Jinja in view.html. Using a for loop, we add the handle and message as a tuple into the list by using the .iloc method to fetch what’s in each row.\nThen, we return render_template on view.html with an argument called message_tuples set equal to, well, message_tuples, which will be a variable that we place inside of the Jinja syntax."
  },
  {
    "objectID": "posts/Homework 3/index.html#view.html",
    "href": "posts/Homework 3/index.html#view.html",
    "title": "Making Discord, Well, Sort of…",
    "section": "view.html",
    "text": "view.html\nJust like with submit.html, this part is quite simple:\n{% extends 'base.html' %}\n{% block header %}\n  &lt;h1&gt;{% block title %}#general{% endblock %}&lt;/h1&gt;\n{% endblock %}\n\n{% block content %}\n{% for tuple in message_tuples%}\n    &lt;h4&gt;{{tuple[0]}}&lt;/h4&gt;\n &lt;p&gt;{{tuple[1]}}&lt;/p&gt;\n&lt;br&gt;\n{% endfor %}\n{% endblock %}\nWe once again extend base.html to add onto what we already have. In the header block, we place a title with the text #general (as is common on Discord servers).\nThen in the content block, we create a for loop in Jinja in order to print out precisely the messages that we had placed as tuples in message_tuples in view() from app.py. This loop first prints out the handle (which is the element at the 0th index) as an &lt;h4&gt; (header 4) while the actual message is simply kept as a paragraph (~normal text) &lt;p&gt;. This has the effect of highlighting the name in larger text, creating a slight break, and then putting the message in smaller text, which is more reminiscent of Discord’s actual appearance. Then, we place a line break after each message so that they aren’t directly on top of each other, and then we end the for loop and block.\nHere’s an image showing the view page with the message we sent earlier:\n\n\n\nImage 5"
  },
  {
    "objectID": "posts/Homework 3/index.html#css-syntax",
    "href": "posts/Homework 3/index.html#css-syntax",
    "title": "Making Discord, Well, Sort of…",
    "section": "CSS Syntax",
    "text": "CSS Syntax\nThe syntax for CSS, at least in terms of what we’re using it for, is quite simple and follows a general pattern\ntag {\n    aspect_1: value_2;\n    aspect_2: value_2;\n    .\n    .\n    .\n}\nThat is, we specify what specific tags that we want to modify in our document, and then we specify different characteristics (color, margin, etc.), a colon :, and then some sort of appropriate value, ended with a semicolon.\n\nhtml\nThe first aspect of our site for which we will specify anything is simply called html and is related to aspects of the entire html document. Here, we’ll control the font and the background color as such:\nhtml {\n    font-family: Verdana;\n    background: #121b18;\n}\nYou can look up fonts online that are compatible–I went on this site (https://www.w3schools.com/css/css_font.asp) to see which ones were available without any modifications. I picked Verdana because it’s a sans-serif font.\nWe also change the background color via background with a hexadecimal color value. I took a screenshot of the Discord app and used an online color picker (https://imagecolorpicker.com/en) to grab the hex value of the background.\n\n\nText specifications\nThere are a few types of text tags that we use throughout our site. We’re going to change them one by one but largely apply the same ideas.\nhtml {\n    font-family: Verdana;\n    background: #121b18;\n}\n\nbody {\n    color: #81888e;\n    max-width: 900px;\n    margin-left: 350px;\n}\n\nh4 {\n    color: #89cff0;\n    margin: 1rem 0;\n    text-align: left;\n}\n\nh1 {\n    color: #81888e;\n    margin: 1rem 0;\n    text-align: left;\n}\n\np {\n    color: #ffffff;\n    margin: 1rem 0;\n    text-align: left;\n    \n}\n\nlabel {\n    color: #81888e;\n}\nThe only parts that require a lot of explanation above are the margin attributes. These were largely copied from a CSS markup example that was given to us. Please visit the following website and the section on “Margin - Shorthand Property” for a detailed explanation https://www.w3schools.com/css/css_margin.asp – essentially, the first number refers to the amount of space given from the top and the second refers to the space from the right. rem is a unit that takes into account font sizes: https://www.sitepoint.com/understanding-and-using-rem-units-in-css/\nI set some of these settings to what they are because otherwise the page wouldn’t look right–it’s not common for a lot of trial and error to be necessary for this sort of process.\n\n\nOther Specifications\nThere are a few other CSS settings that I’ve tweaked:\na {\n    color: CornflowerBlue;\n}\n\n\nnav ul  {\n    display: flex;\n    list-style: none;\n    margin: 0;\n    padding: 0;\n}\n\nnav ul li a {\n    display: block;\n    padding: 1rem;\n}\nThe template I got this from set the link (that is, a) color to cornflower blue, which I really like. We also specify the settings for the unordered list &lt;ul&gt; and the list item links &lt;li&gt; in base.html. The settings in nav ul I also took from the template that we had, but I like how they look– display: none makes it so that they’re side-by-side, list-style: none removes bullet points, and the margin and padding keep them adequately spaced. I discovered what these things do by seeing what happened to the website when I tweaked the settings.\nSimilarly, for nav ul li a, we use block for display to place some space between the &lt;nav&gt; and the body below, and we also use some padding.\nBelow we can see where each CSS element corresponds to on the webpages:\n\n\n\nimage 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ZionBlogPIC16B",
    "section": "",
    "text": "Get on the Trail! PIC16B Final Project Overview\n\n\n\n\n\n\nproject\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nFake News Detector\n\n\n\n\n\n\nweek 10\n\n\nhomework 6\n\n\n\n\n\n\n\n\n\nMar 12, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\n(put a cool title here)\n\n\n\n\n\n\nweek 8\n\n\nhomework 5\n\n\n\n\n\n\n\n\n\nMar 1, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nHow fast we can we discretize a second-order differential equation?\n\n\n\n\n\n\nweek 6\n\n\nhomework 4\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Discord, Well, Sort of…\n\n\n\n\n\n\nweek 5\n\n\nhomework 3\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Your Favorite Movies: Using Scrapy to Crawl through the Web\n\n\n\n\n\n\nweek 4\n\n\nhomework 2\n\n\n\n\n\n\n\n\n\nJan 28, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal Warming: Spread, Change, and Resistance\n\n\n\n\n\n\nweek 3\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0: Penguins\n\n\n\n\n\n\nweek 1\n\n\nhomework 0\n\n\nhomework\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nhaiiiiiiiii :3\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nZion Gassner\n\n\n\n\n\n\n\n\n\n\n\n\nCreating posts\n\n\n\n\n\n\nweek 0\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 17, 2023\n\n\nZion\n\n\n\n\n\n\nNo matching items"
  }
]